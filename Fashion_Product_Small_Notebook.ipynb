{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion Product Small Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTAd6154aq7gtn8f+MQ1Fg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f9fda824783436d98c05ee8e40aeef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2172c9f50a3540f1adcde8618629f968",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c142c0b983d459f96f87a38001d8bce",
              "IPY_MODEL_48b8cf09f54a4488a4bfe522b1df0555"
            ]
          }
        },
        "2172c9f50a3540f1adcde8618629f968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c142c0b983d459f96f87a38001d8bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2fdd973a7f3a44a9a1afce2f65b91c2f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4422102,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4422102,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e3ff43e9e974229ba61d01c1271ae3e"
          }
        },
        "48b8cf09f54a4488a4bfe522b1df0555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82e3a200660f42c2b7e57c3b9eabca72",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4422656/? [00:03&lt;00:00, 1267169.06it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5ebf0784c75471f9aea842947b8ced4"
          }
        },
        "2fdd973a7f3a44a9a1afce2f65b91c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e3ff43e9e974229ba61d01c1271ae3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82e3a200660f42c2b7e57c3b9eabca72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5ebf0784c75471f9aea842947b8ced4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c334b744dba4716ae41761601e27248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c241800fb4044e6db588d13268a5d4fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5f92d2795e07461895f8c814890b275e",
              "IPY_MODEL_797f68cd94d440bc8fc62c2ef0c8b77f"
            ]
          }
        },
        "c241800fb4044e6db588d13268a5d4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f92d2795e07461895f8c814890b275e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_46bb48a4ba974495bfc54c71461aa401",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5148,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8e9665aa1b54f79ae6eb18873acfbbc"
          }
        },
        "797f68cd94d440bc8fc62c2ef0c8b77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c2240b8a1a84f8792244e86719e35fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6144/? [00:00&lt;00:00, 38709.38it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e61e8d5d5b942a6911ec770b32907bc"
          }
        },
        "46bb48a4ba974495bfc54c71461aa401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8e9665aa1b54f79ae6eb18873acfbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c2240b8a1a84f8792244e86719e35fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e61e8d5d5b942a6911ec770b32907bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neix20/Deep_Learning_May_2021/blob/main/Fashion_Product_Small_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBIr8b5aqwG"
      },
      "source": [
        "## Mount Notebook to Google Colab Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VraP7JTaqBP",
        "outputId": "e0ab0457-1979-4b5a-d4dc-173578175af1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVuKkMuaufv"
      },
      "source": [
        "## Change Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBcBdPdjapwl",
        "outputId": "ac4fa1cb-9409-45ab-baa8-c865de197b0f"
      },
      "source": [
        "cd /content/drive/MyDrive/Deep_Learning_Assignment_May_2021/Deep_Learning_May_2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Deep_Learning_Assignment_May_2021/Deep_Learning_May_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABULw_tbIZx"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a379duUmbME2"
      },
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch, torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdTNzG70VbSF"
      },
      "source": [
        "## Create Dataset Classes\n",
        "This is to transform any dataset we import into a class that can be processed by our model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhw6GdLjb9Xl"
      },
      "source": [
        "### 0. Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCthiKoIcDFk"
      },
      "source": [
        "class FashionMNIST(Dataset):\n",
        "    \n",
        "    def __init__(self, root = \".\", train = True, download=False, transform=None, num_samples = None):\n",
        "        self.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "        self.num_classes = len(self.classes)            \n",
        "\n",
        "        self.__dataset = torchvision.datasets.FashionMNIST(root=root, train=train, download=download, transform=transform)\n",
        "\t\t\t\n",
        "        self.__num_samples = min(num_samples, len(self.__dataset)) if num_samples is not None else len(self.__dataset)\n",
        "                       \n",
        "    def __len__(self):\n",
        "        return self.__num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):            \n",
        "        return self.__dataset[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52sN8iTwo1Oq"
      },
      "source": [
        "## Fetch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EreQJtXwXu6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "9f9fda824783436d98c05ee8e40aeef5",
            "2172c9f50a3540f1adcde8618629f968",
            "6c142c0b983d459f96f87a38001d8bce",
            "48b8cf09f54a4488a4bfe522b1df0555",
            "2fdd973a7f3a44a9a1afce2f65b91c2f",
            "2e3ff43e9e974229ba61d01c1271ae3e",
            "82e3a200660f42c2b7e57c3b9eabca72",
            "a5ebf0784c75471f9aea842947b8ced4",
            "9c334b744dba4716ae41761601e27248",
            "c241800fb4044e6db588d13268a5d4fa",
            "5f92d2795e07461895f8c814890b275e",
            "797f68cd94d440bc8fc62c2ef0c8b77f",
            "46bb48a4ba974495bfc54c71461aa401",
            "c8e9665aa1b54f79ae6eb18873acfbbc",
            "5c2240b8a1a84f8792244e86719e35fa",
            "6e61e8d5d5b942a6911ec770b32907bc"
          ]
        },
        "outputId": "4f87d201-2a97-4f38-a75f-04dac02aedbb"
      },
      "source": [
        "dataset = FashionMNIST(root=\".\", download= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Using downloaded and verified file: ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n",
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Using downloaded and verified file: ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f9fda824783436d98c05ee8e40aeef5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4422102.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c334b744dba4716ae41761601e27248",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5148.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CONb7SLFxboa"
      },
      "source": [
        "## Original Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDAinoxbxgUQ"
      },
      "source": [
        "def show_image(img):\n",
        "    plt.imshow (img, cmap = matplotlib.cm.gray, interpolation = 'nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Ztr9vgdtxjbR",
        "outputId": "b2ccd5e6-45c2-4b2d-8207-e9882933402c"
      },
      "source": [
        "idx = 10\n",
        "img, label = dataset[idx]\n",
        "\n",
        "show_image(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KL0PIYWwcc_"
      },
      "source": [
        "## Transformation Pipeline\n",
        "* Resize image to Standard Image Size (224, 224)\n",
        "* Convert Tensor Pil Image to Tensor\n",
        "* Normalize Image to Tensor Float\n",
        "* _Data Augmentation (Extra): Random Crop_\n",
        "* _Data Augmentation (Extra): Random Horizontal Flip_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OCYrb70wouz"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomCrop(28),                            \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (1.0,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtFeWJEG57LG"
      },
      "source": [
        "fashion_MNIST_transform = transforms.Compose([              \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (1.0,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "6yZStw3OyG0-",
        "outputId": "88b82b26-4634-44de-d4d0-b0364ace76f6"
      },
      "source": [
        "idx = 10\n",
        "img, label = dataset[idx]\n",
        "\n",
        "# tmp = transform(img)\n",
        "tmp = fashion_MNIST_transform(img)\n",
        "\n",
        "print('Shape of img:', tmp.shape)\n",
        "print(f'Range of img: {tmp.min().item():.2f} to {tmp.max().item():.2f}')\n",
        "print('Type of img:', type(tmp))\n",
        "print('Type of items in img:', tmp.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-2280d123366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tmp = transform(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_MNIST_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of img:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \"\"\"\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be PIL Image or ndarray. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_numpy_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmGhPPBw4YSk"
      },
      "source": [
        "dataset = FashionMNIST(root=\".\", download= True, transform = fashion_MNIST_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ftkas_4697"
      },
      "source": [
        "## Split Dataset Into Train Set and Trainning Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8I_lB9W4-_s"
      },
      "source": [
        "trainset, testset = torch.utils.data.random_split(dataset, [50000, 10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdRjpF8V5R3x"
      },
      "source": [
        "## Batch Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWYs3vbK5ZoQ"
      },
      "source": [
        "trainloader = DataLoader(trainset, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "testloader = DataLoader(testset, batch_size = 4, shuffle = True, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQXNJJFJ5cNn"
      },
      "source": [
        "## Train Function\n",
        "\n",
        "#### Train Pipeline:\n",
        "\n",
        "* Optimizer => Used to update parameters after forward-backward Propagation\n",
        "* Loss Function => Used to update the direction of the gradient\n",
        "* The following train Model uses Gradient Descent algorithm. The gradient descent algorithm has the following pipeline.\n",
        "  * Forward Propagation\n",
        "  * Update Gradient to backwards\n",
        "  * Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5GcNXrD8SOI"
      },
      "source": [
        "def train(model, trainloader, num_epochs = 25, loss_iter = 50, lr=0.1, momentum=0.9, verbose=True):\n",
        "    history = {}\n",
        "    \n",
        "    loss_iterations = int(np.ceil(len(trainloader)/loss_iter))\n",
        "    \n",
        "    # Define Device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    \n",
        "    # set to training mode\n",
        "    model.train()\n",
        "\n",
        "    # train the network\n",
        "    for e in range(num_epochs):    \n",
        "\n",
        "        running_loss, running_count = 0.0, 0.0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            # Clear all the gradient to 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # transfer data to GPU\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outs = model(inputs)\n",
        "            loss = F.cross_entropy(outs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # get the loss\n",
        "            running_loss += loss.item()\n",
        "            running_count += 1\n",
        "            train_loss = 0\n",
        "\n",
        "             # display the averaged loss value \n",
        "            if i % loss_iterations == loss_iterations-1 or i == len(trainloader) - 1:                \n",
        "                train_loss = running_loss / running_count\n",
        "                running_loss, running_count = 0.0, 0.0\n",
        "                if verbose:\n",
        "                    print(f'[Epoch {e+1:2d}/{num_epochs:d} Iter {i+1:5d}/{len(trainloader)}]: train_loss = {train_loss:.4f}')       \n",
        "        \n",
        "      history[f'{e+1:2d}'] = train_loss\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PI0kirDHbfm"
      },
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXk44bLkHfjf"
      },
      "source": [
        "def evaluate(model, testloader):\n",
        "    \n",
        "    # set to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define Device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # running_correct\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Repeat for all batch data in the test set\n",
        "    for inputs, targets in testloader:\n",
        "\n",
        "        # transfer to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # # disable gradient computation\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_corrects += (targets == predicted).double().sum()\n",
        "\n",
        "\n",
        "    print('Accuracy = {:.2f}%'.format(100*running_corrects/len(testloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te2EbB7rWxUy"
      },
      "source": [
        "## 2. VGG Net Model\n",
        "\n",
        "<center><b><font>VGG16 Model</font></b></center>\n",
        "\n",
        "Notes: `k`: number of filters, `f`: filter or kernel size, `s`: stride, `p`: padding, `o`: output shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svTfAr2M2KQW"
      },
      "source": [
        "def build_vgg16_network(pretrained = True):\n",
        "  model =  models.vgg16_bn(pretrained = pretrained)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(in_features=4096, out_features=10, bias=True),\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W3y6pk20Ckd"
      },
      "source": [
        "### Pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aBd8Jaw34Jf"
      },
      "source": [
        "model = build_vgg16_network(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0vWJQn_0IIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "182c4431-78d1-4193-bbd9-e2e0aba7a8ca"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGIjPaEd39AQ"
      },
      "source": [
        "# history1 = train(model, trainloader, lr = 0.001, momentum = 0.9, verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-SGOrLX_V0n"
      },
      "source": [
        "# evaluate(model, testloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K0oscT0Wz7E"
      },
      "source": [
        "## 3. Convoluted Network Model I\n",
        "\n",
        "<center><b><font>Convoluted CNN</font></b></center>\n",
        "\n",
        "|Layer|Name|Description|Output Shape|\n",
        "|--- |--- |--- |--- |\n",
        "|1|conv1|Conv2d(k=32,f=5,s=1,p=1)|(?, 32, 28, 28)|\n",
        "|-|ReLU|ReLU(inplace=True)|(?, 32, 28, 28)|\n",
        "|-|Max Pool|max_pool2d(x,k=2,s=1,p=0)|(?, 32, 14, 14)|\n",
        "|2|conv1|Conv2d(k=64,f=2,s=1,p=1)|(?, 64, 14, 14)|\n",
        "|-|ReLU|ReLU(inplace=True)|(?, 64, 14, 14)|\n",
        "|-|Max Pool|max_pool2d(x,k=2,s=2,p=0)|(?, 64, 7, 7)|\n",
        "|3|conv1|Conv2d(k=128,f=3,s=1,p=1)|(?, 128, 7, 7)|\n",
        "|-|ReLU|ReLU(inplace=True)|(?, 128, 7, 7)|\n",
        "|-|Max Pool|max_pool2d(x,k=2,s=2,p=0)|(?, 128, 3, 3)|\n",
        "|4|fc1|Linear(#units=1,152)|(?, 1152)|\n",
        "|-|ReLU|ReLU(inplace=True)|(?, 1152)|\n",
        "|4|fc1|Linear(#units=1,152)|(?, 10)|\n",
        "|-|ReLU|ReLU(inplace=True)|(?, 10)|\n",
        "\n",
        "Notes: `k`: number of filters, `f`: filter or kernel size, `s`: stride, `p`: padding, `o`: output shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grh8KS9fstAM"
      },
      "source": [
        "def build_block_cnn(in_channels, out_channels):\n",
        "  block = nn.Sequential(\n",
        "    nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "    nn.ReLU()\n",
        "  )\n",
        "  return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB7rW31AsjA1"
      },
      "source": [
        "class cnn_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        channel_size = [3, 32, 32]\n",
        "        layer_size = [32 * 7 * 7, 128, 10]\n",
        "        p = 0.25\n",
        "        \n",
        "        # define blocks\n",
        "        self.conv_block = nn.ModuleList(\n",
        "            build_block_cnn(in_channel, out_channel) for(in_channel, out_channel) in zip(channel_size[0:-1], channel_size[1:])\n",
        "        )\n",
        "        \n",
        "        # define fc1\n",
        "        self.fc = nn.ModuleList(\n",
        "            [nn.Linear(in_channel, out_channel) for(in_channel, out_channel) in zip(layer_size[0:-1], layer_size[1:])]\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.ModuleList(\n",
        "            [nn.Dropout(p) for i in range(len(layer_size) - 2)]\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for block in self.conv_block:\n",
        "          x = F.max_pool2d(block(x), kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        for (dropout, fc) in zip(self.dropout, self.fc[:-1]):\n",
        "          x = dropout(torch.relu(fc(x)))\n",
        "        \n",
        "        x = torch.relu(self.fc[1](x))\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxZYVJKyiLHm",
        "outputId": "83ee03f5-15b3-4950-bdf7-1cff4e27325a"
      },
      "source": [
        "model = cnn_model()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnn_model(\n",
            "  (conv_block): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=1568, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            "  (dropout): ModuleList(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymIvdIk9yIWn",
        "outputId": "ccea2c6c-aa9c-4dfb-9309-3b149af7e848"
      },
      "source": [
        "summary(model, (3, 28, 28), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             896\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "            Conv2d-3           [-1, 32, 14, 14]           9,248\n",
            "              ReLU-4           [-1, 32, 14, 14]               0\n",
            "            Linear-5                  [-1, 128]         200,832\n",
            "           Dropout-6                  [-1, 128]               0\n",
            "            Linear-7                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 212,266\n",
            "Trainable params: 212,266\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.48\n",
            "Params size (MB): 0.81\n",
            "Estimated Total Size (MB): 1.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l3FqfB1pqos",
        "outputId": "18a62e1f-2a22-40a0-f912-8ca2c3dc77ff"
      },
      "source": [
        "history1 = train(model, trainloader, num_epochs = 15, loss_iter = 50, lr = 0.001, momentum = 0.9, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/25 Iter   250/12500]: train_loss = 2.2851\n",
            "[Epoch  1/25 Iter   500/12500]: train_loss = 2.0229\n",
            "[Epoch  1/25 Iter   750/12500]: train_loss = 1.4851\n",
            "[Epoch  1/25 Iter  1000/12500]: train_loss = 1.3051\n",
            "[Epoch  1/25 Iter  1250/12500]: train_loss = 1.3338\n",
            "[Epoch  1/25 Iter  1500/12500]: train_loss = 1.1804\n",
            "[Epoch  1/25 Iter  1750/12500]: train_loss = 1.0646\n",
            "[Epoch  1/25 Iter  2000/12500]: train_loss = 1.1032\n",
            "[Epoch  1/25 Iter  2250/12500]: train_loss = 1.0371\n",
            "[Epoch  1/25 Iter  2500/12500]: train_loss = 0.9930\n",
            "[Epoch  1/25 Iter  2750/12500]: train_loss = 1.0096\n",
            "[Epoch  1/25 Iter  3000/12500]: train_loss = 0.9523\n",
            "[Epoch  1/25 Iter  3250/12500]: train_loss = 1.0088\n",
            "[Epoch  1/25 Iter  3500/12500]: train_loss = 0.9626\n",
            "[Epoch  1/25 Iter  3750/12500]: train_loss = 0.8774\n",
            "[Epoch  1/25 Iter  4000/12500]: train_loss = 0.9490\n",
            "[Epoch  1/25 Iter  4250/12500]: train_loss = 0.8639\n",
            "[Epoch  1/25 Iter  4500/12500]: train_loss = 0.8821\n",
            "[Epoch  1/25 Iter  4750/12500]: train_loss = 0.8439\n",
            "[Epoch  1/25 Iter  5000/12500]: train_loss = 0.8609\n",
            "[Epoch  1/25 Iter  5250/12500]: train_loss = 0.8592\n",
            "[Epoch  1/25 Iter  5500/12500]: train_loss = 0.8452\n",
            "[Epoch  1/25 Iter  5750/12500]: train_loss = 0.8831\n",
            "[Epoch  1/25 Iter  6000/12500]: train_loss = 0.7953\n",
            "[Epoch  1/25 Iter  6250/12500]: train_loss = 0.8187\n",
            "[Epoch  1/25 Iter  6500/12500]: train_loss = 0.8712\n",
            "[Epoch  1/25 Iter  6750/12500]: train_loss = 0.8351\n",
            "[Epoch  1/25 Iter  7000/12500]: train_loss = 0.7969\n",
            "[Epoch  1/25 Iter  7250/12500]: train_loss = 0.8055\n",
            "[Epoch  1/25 Iter  7500/12500]: train_loss = 0.7826\n",
            "[Epoch  1/25 Iter  7750/12500]: train_loss = 0.7491\n",
            "[Epoch  1/25 Iter  8000/12500]: train_loss = 0.8356\n",
            "[Epoch  1/25 Iter  8250/12500]: train_loss = 0.8152\n",
            "[Epoch  1/25 Iter  8500/12500]: train_loss = 0.8166\n",
            "[Epoch  1/25 Iter  8750/12500]: train_loss = 0.7841\n",
            "[Epoch  1/25 Iter  9000/12500]: train_loss = 0.7859\n",
            "[Epoch  1/25 Iter  9250/12500]: train_loss = 0.7638\n",
            "[Epoch  1/25 Iter  9500/12500]: train_loss = 0.7804\n",
            "[Epoch  1/25 Iter  9750/12500]: train_loss = 0.7935\n",
            "[Epoch  1/25 Iter 10000/12500]: train_loss = 0.7761\n",
            "[Epoch  1/25 Iter 10250/12500]: train_loss = 0.7691\n",
            "[Epoch  1/25 Iter 10500/12500]: train_loss = 0.7921\n",
            "[Epoch  1/25 Iter 10750/12500]: train_loss = 0.7273\n",
            "[Epoch  1/25 Iter 11000/12500]: train_loss = 0.7663\n",
            "[Epoch  1/25 Iter 11250/12500]: train_loss = 0.7866\n",
            "[Epoch  1/25 Iter 11500/12500]: train_loss = 0.7507\n",
            "[Epoch  1/25 Iter 11750/12500]: train_loss = 0.7118\n",
            "[Epoch  1/25 Iter 12000/12500]: train_loss = 0.7333\n",
            "[Epoch  1/25 Iter 12250/12500]: train_loss = 0.7705\n",
            "[Epoch  1/25 Iter 12500/12500]: train_loss = 0.7593\n",
            "[Epoch  2/25 Iter   250/12500]: train_loss = 0.6730\n",
            "[Epoch  2/25 Iter   500/12500]: train_loss = 0.7626\n",
            "[Epoch  2/25 Iter   750/12500]: train_loss = 0.7911\n",
            "[Epoch  2/25 Iter  1000/12500]: train_loss = 0.7286\n",
            "[Epoch  2/25 Iter  1250/12500]: train_loss = 0.7664\n",
            "[Epoch  2/25 Iter  1500/12500]: train_loss = 0.7651\n",
            "[Epoch  2/25 Iter  1750/12500]: train_loss = 0.7758\n",
            "[Epoch  2/25 Iter  2000/12500]: train_loss = 0.7876\n",
            "[Epoch  2/25 Iter  2250/12500]: train_loss = 0.6976\n",
            "[Epoch  2/25 Iter  2500/12500]: train_loss = 0.7146\n",
            "[Epoch  2/25 Iter  2750/12500]: train_loss = 0.7102\n",
            "[Epoch  2/25 Iter  3000/12500]: train_loss = 0.7286\n",
            "[Epoch  2/25 Iter  3250/12500]: train_loss = 0.7038\n",
            "[Epoch  2/25 Iter  3500/12500]: train_loss = 0.7267\n",
            "[Epoch  2/25 Iter  3750/12500]: train_loss = 0.7109\n",
            "[Epoch  2/25 Iter  4000/12500]: train_loss = 0.6310\n",
            "[Epoch  2/25 Iter  4250/12500]: train_loss = 0.7254\n",
            "[Epoch  2/25 Iter  4500/12500]: train_loss = 0.7181\n",
            "[Epoch  2/25 Iter  4750/12500]: train_loss = 0.6704\n",
            "[Epoch  2/25 Iter  5000/12500]: train_loss = 0.7482\n",
            "[Epoch  2/25 Iter  5250/12500]: train_loss = 0.6420\n",
            "[Epoch  2/25 Iter  5500/12500]: train_loss = 0.4742\n",
            "[Epoch  2/25 Iter  5750/12500]: train_loss = 0.4946\n",
            "[Epoch  2/25 Iter  6000/12500]: train_loss = 0.5092\n",
            "[Epoch  2/25 Iter  6250/12500]: train_loss = 0.4925\n",
            "[Epoch  2/25 Iter  6500/12500]: train_loss = 0.4540\n",
            "[Epoch  2/25 Iter  6750/12500]: train_loss = 0.4946\n",
            "[Epoch  2/25 Iter  7000/12500]: train_loss = 0.5011\n",
            "[Epoch  2/25 Iter  7250/12500]: train_loss = 0.4902\n",
            "[Epoch  2/25 Iter  7500/12500]: train_loss = 0.4517\n",
            "[Epoch  2/25 Iter  7750/12500]: train_loss = 0.4545\n",
            "[Epoch  2/25 Iter  8000/12500]: train_loss = 0.4253\n",
            "[Epoch  2/25 Iter  8250/12500]: train_loss = 0.4966\n",
            "[Epoch  2/25 Iter  8500/12500]: train_loss = 0.4760\n",
            "[Epoch  2/25 Iter  8750/12500]: train_loss = 0.4438\n",
            "[Epoch  2/25 Iter  9000/12500]: train_loss = 0.5176\n",
            "[Epoch  2/25 Iter  9250/12500]: train_loss = 0.4870\n",
            "[Epoch  2/25 Iter  9500/12500]: train_loss = 0.4886\n",
            "[Epoch  2/25 Iter  9750/12500]: train_loss = 0.4474\n",
            "[Epoch  2/25 Iter 10000/12500]: train_loss = 0.4360\n",
            "[Epoch  2/25 Iter 10250/12500]: train_loss = 0.4355\n",
            "[Epoch  2/25 Iter 10500/12500]: train_loss = 0.4118\n",
            "[Epoch  2/25 Iter 10750/12500]: train_loss = 0.4680\n",
            "[Epoch  2/25 Iter 11000/12500]: train_loss = 0.4783\n",
            "[Epoch  2/25 Iter 11250/12500]: train_loss = 0.4395\n",
            "[Epoch  2/25 Iter 11500/12500]: train_loss = 0.4546\n",
            "[Epoch  2/25 Iter 11750/12500]: train_loss = 0.3859\n",
            "[Epoch  2/25 Iter 12000/12500]: train_loss = 0.4253\n",
            "[Epoch  2/25 Iter 12250/12500]: train_loss = 0.4672\n",
            "[Epoch  2/25 Iter 12500/12500]: train_loss = 0.4606\n",
            "[Epoch  3/25 Iter   250/12500]: train_loss = 0.4282\n",
            "[Epoch  3/25 Iter   500/12500]: train_loss = 0.4214\n",
            "[Epoch  3/25 Iter   750/12500]: train_loss = 0.4566\n",
            "[Epoch  3/25 Iter  1000/12500]: train_loss = 0.4191\n",
            "[Epoch  3/25 Iter  1250/12500]: train_loss = 0.4373\n",
            "[Epoch  3/25 Iter  1500/12500]: train_loss = 0.4334\n",
            "[Epoch  3/25 Iter  1750/12500]: train_loss = 0.4002\n",
            "[Epoch  3/25 Iter  2000/12500]: train_loss = 0.4289\n",
            "[Epoch  3/25 Iter  2250/12500]: train_loss = 0.4062\n",
            "[Epoch  3/25 Iter  2500/12500]: train_loss = 0.4361\n",
            "[Epoch  3/25 Iter  2750/12500]: train_loss = 0.4200\n",
            "[Epoch  3/25 Iter  3000/12500]: train_loss = 0.4222\n",
            "[Epoch  3/25 Iter  3250/12500]: train_loss = 0.4069\n",
            "[Epoch  3/25 Iter  3500/12500]: train_loss = 0.4669\n",
            "[Epoch  3/25 Iter  3750/12500]: train_loss = 0.3935\n",
            "[Epoch  3/25 Iter  4000/12500]: train_loss = 0.4149\n",
            "[Epoch  3/25 Iter  4250/12500]: train_loss = 0.4385\n",
            "[Epoch  3/25 Iter  4500/12500]: train_loss = 0.4466\n",
            "[Epoch  3/25 Iter  4750/12500]: train_loss = 0.3812\n",
            "[Epoch  3/25 Iter  5000/12500]: train_loss = 0.4186\n",
            "[Epoch  3/25 Iter  5250/12500]: train_loss = 0.4469\n",
            "[Epoch  3/25 Iter  5500/12500]: train_loss = 0.3887\n",
            "[Epoch  3/25 Iter  5750/12500]: train_loss = 0.4056\n",
            "[Epoch  3/25 Iter  6000/12500]: train_loss = 0.3726\n",
            "[Epoch  3/25 Iter  6250/12500]: train_loss = 0.4287\n",
            "[Epoch  3/25 Iter  6500/12500]: train_loss = 0.3948\n",
            "[Epoch  3/25 Iter  6750/12500]: train_loss = 0.4204\n",
            "[Epoch  3/25 Iter  7000/12500]: train_loss = 0.4005\n",
            "[Epoch  3/25 Iter  7250/12500]: train_loss = 0.4196\n",
            "[Epoch  3/25 Iter  7500/12500]: train_loss = 0.4181\n",
            "[Epoch  3/25 Iter  7750/12500]: train_loss = 0.4018\n",
            "[Epoch  3/25 Iter  8000/12500]: train_loss = 0.4066\n",
            "[Epoch  3/25 Iter  8250/12500]: train_loss = 0.3834\n",
            "[Epoch  3/25 Iter  8500/12500]: train_loss = 0.4061\n",
            "[Epoch  3/25 Iter  8750/12500]: train_loss = 0.4145\n",
            "[Epoch  3/25 Iter  9000/12500]: train_loss = 0.3807\n",
            "[Epoch  3/25 Iter  9250/12500]: train_loss = 0.4245\n",
            "[Epoch  3/25 Iter  9500/12500]: train_loss = 0.4235\n",
            "[Epoch  3/25 Iter  9750/12500]: train_loss = 0.3699\n",
            "[Epoch  3/25 Iter 10000/12500]: train_loss = 0.3923\n",
            "[Epoch  3/25 Iter 10250/12500]: train_loss = 0.3920\n",
            "[Epoch  3/25 Iter 10500/12500]: train_loss = 0.4381\n",
            "[Epoch  3/25 Iter 10750/12500]: train_loss = 0.3997\n",
            "[Epoch  3/25 Iter 11000/12500]: train_loss = 0.4180\n",
            "[Epoch  3/25 Iter 11250/12500]: train_loss = 0.3944\n",
            "[Epoch  3/25 Iter 11500/12500]: train_loss = 0.4001\n",
            "[Epoch  3/25 Iter 11750/12500]: train_loss = 0.3893\n",
            "[Epoch  3/25 Iter 12000/12500]: train_loss = 0.4050\n",
            "[Epoch  3/25 Iter 12250/12500]: train_loss = 0.3935\n",
            "[Epoch  3/25 Iter 12500/12500]: train_loss = 0.3681\n",
            "[Epoch  4/25 Iter   250/12500]: train_loss = 0.3750\n",
            "[Epoch  4/25 Iter   500/12500]: train_loss = 0.3759\n",
            "[Epoch  4/25 Iter   750/12500]: train_loss = 0.3837\n",
            "[Epoch  4/25 Iter  1000/12500]: train_loss = 0.4141\n",
            "[Epoch  4/25 Iter  1250/12500]: train_loss = 0.4183\n",
            "[Epoch  4/25 Iter  1500/12500]: train_loss = 0.3631\n",
            "[Epoch  4/25 Iter  1750/12500]: train_loss = 0.3785\n",
            "[Epoch  4/25 Iter  2000/12500]: train_loss = 0.3440\n",
            "[Epoch  4/25 Iter  2250/12500]: train_loss = 0.3777\n",
            "[Epoch  4/25 Iter  2500/12500]: train_loss = 0.4175\n",
            "[Epoch  4/25 Iter  2750/12500]: train_loss = 0.3628\n",
            "[Epoch  4/25 Iter  3000/12500]: train_loss = 0.3401\n",
            "[Epoch  4/25 Iter  3250/12500]: train_loss = 0.3784\n",
            "[Epoch  4/25 Iter  3500/12500]: train_loss = 0.3344\n",
            "[Epoch  4/25 Iter  3750/12500]: train_loss = 0.4022\n",
            "[Epoch  4/25 Iter  4000/12500]: train_loss = 0.3724\n",
            "[Epoch  4/25 Iter  4250/12500]: train_loss = 0.3523\n",
            "[Epoch  4/25 Iter  4500/12500]: train_loss = 0.3743\n",
            "[Epoch  4/25 Iter  4750/12500]: train_loss = 0.3665\n",
            "[Epoch  4/25 Iter  5000/12500]: train_loss = 0.3680\n",
            "[Epoch  4/25 Iter  5250/12500]: train_loss = 0.3854\n",
            "[Epoch  4/25 Iter  5500/12500]: train_loss = 0.3743\n",
            "[Epoch  4/25 Iter  5750/12500]: train_loss = 0.3786\n",
            "[Epoch  4/25 Iter  6000/12500]: train_loss = 0.3770\n",
            "[Epoch  4/25 Iter  6250/12500]: train_loss = 0.3457\n",
            "[Epoch  4/25 Iter  6500/12500]: train_loss = 0.3469\n",
            "[Epoch  4/25 Iter  6750/12500]: train_loss = 0.3618\n",
            "[Epoch  4/25 Iter  7000/12500]: train_loss = 0.3792\n",
            "[Epoch  4/25 Iter  7250/12500]: train_loss = 0.3858\n",
            "[Epoch  4/25 Iter  7500/12500]: train_loss = 0.3698\n",
            "[Epoch  4/25 Iter  7750/12500]: train_loss = 0.3351\n",
            "[Epoch  4/25 Iter  8000/12500]: train_loss = 0.3409\n",
            "[Epoch  4/25 Iter  8250/12500]: train_loss = 0.3948\n",
            "[Epoch  4/25 Iter  8500/12500]: train_loss = 0.3761\n",
            "[Epoch  4/25 Iter  8750/12500]: train_loss = 0.3511\n",
            "[Epoch  4/25 Iter  9000/12500]: train_loss = 0.4184\n",
            "[Epoch  4/25 Iter  9250/12500]: train_loss = 0.3655\n",
            "[Epoch  4/25 Iter  9500/12500]: train_loss = 0.3543\n",
            "[Epoch  4/25 Iter  9750/12500]: train_loss = 0.3612\n",
            "[Epoch  4/25 Iter 10000/12500]: train_loss = 0.3926\n",
            "[Epoch  4/25 Iter 10250/12500]: train_loss = 0.3545\n",
            "[Epoch  4/25 Iter 10500/12500]: train_loss = 0.3630\n",
            "[Epoch  4/25 Iter 10750/12500]: train_loss = 0.3292\n",
            "[Epoch  4/25 Iter 11000/12500]: train_loss = 0.3285\n",
            "[Epoch  4/25 Iter 11250/12500]: train_loss = 0.3522\n",
            "[Epoch  4/25 Iter 11500/12500]: train_loss = 0.3223\n",
            "[Epoch  4/25 Iter 11750/12500]: train_loss = 0.3312\n",
            "[Epoch  4/25 Iter 12000/12500]: train_loss = 0.3413\n",
            "[Epoch  4/25 Iter 12250/12500]: train_loss = 0.3713\n",
            "[Epoch  4/25 Iter 12500/12500]: train_loss = 0.3603\n",
            "[Epoch  5/25 Iter   250/12500]: train_loss = 0.3591\n",
            "[Epoch  5/25 Iter   500/12500]: train_loss = 0.3673\n",
            "[Epoch  5/25 Iter   750/12500]: train_loss = 0.3476\n",
            "[Epoch  5/25 Iter  1000/12500]: train_loss = 0.3277\n",
            "[Epoch  5/25 Iter  1250/12500]: train_loss = 0.3339\n",
            "[Epoch  5/25 Iter  1500/12500]: train_loss = 0.3834\n",
            "[Epoch  5/25 Iter  1750/12500]: train_loss = 0.3446\n",
            "[Epoch  5/25 Iter  2000/12500]: train_loss = 0.3466\n",
            "[Epoch  5/25 Iter  2250/12500]: train_loss = 0.3624\n",
            "[Epoch  5/25 Iter  2500/12500]: train_loss = 0.3096\n",
            "[Epoch  5/25 Iter  2750/12500]: train_loss = 0.3746\n",
            "[Epoch  5/25 Iter  3000/12500]: train_loss = 0.3657\n",
            "[Epoch  5/25 Iter  3250/12500]: train_loss = 0.2993\n",
            "[Epoch  5/25 Iter  3500/12500]: train_loss = 0.3848\n",
            "[Epoch  5/25 Iter  3750/12500]: train_loss = 0.3435\n",
            "[Epoch  5/25 Iter  4000/12500]: train_loss = 0.3203\n",
            "[Epoch  5/25 Iter  4250/12500]: train_loss = 0.3479\n",
            "[Epoch  5/25 Iter  4500/12500]: train_loss = 0.3445\n",
            "[Epoch  5/25 Iter  4750/12500]: train_loss = 0.3598\n",
            "[Epoch  5/25 Iter  5000/12500]: train_loss = 0.3361\n",
            "[Epoch  5/25 Iter  5250/12500]: train_loss = 0.3808\n",
            "[Epoch  5/25 Iter  5500/12500]: train_loss = 0.3379\n",
            "[Epoch  5/25 Iter  5750/12500]: train_loss = 0.3248\n",
            "[Epoch  5/25 Iter  6000/12500]: train_loss = 0.3575\n",
            "[Epoch  5/25 Iter  6250/12500]: train_loss = 0.3120\n",
            "[Epoch  5/25 Iter  6500/12500]: train_loss = 0.3515\n",
            "[Epoch  5/25 Iter  6750/12500]: train_loss = 0.3535\n",
            "[Epoch  5/25 Iter  7000/12500]: train_loss = 0.3310\n",
            "[Epoch  5/25 Iter  7250/12500]: train_loss = 0.3412\n",
            "[Epoch  5/25 Iter  7500/12500]: train_loss = 0.3380\n",
            "[Epoch  5/25 Iter  7750/12500]: train_loss = 0.3674\n",
            "[Epoch  5/25 Iter  8000/12500]: train_loss = 0.3519\n",
            "[Epoch  5/25 Iter  8250/12500]: train_loss = 0.3339\n",
            "[Epoch  5/25 Iter  8500/12500]: train_loss = 0.3558\n",
            "[Epoch  5/25 Iter  8750/12500]: train_loss = 0.3319\n",
            "[Epoch  5/25 Iter  9000/12500]: train_loss = 0.3306\n",
            "[Epoch  5/25 Iter  9250/12500]: train_loss = 0.3471\n",
            "[Epoch  5/25 Iter  9500/12500]: train_loss = 0.3706\n",
            "[Epoch  5/25 Iter  9750/12500]: train_loss = 0.3107\n",
            "[Epoch  5/25 Iter 10000/12500]: train_loss = 0.2935\n",
            "[Epoch  5/25 Iter 10250/12500]: train_loss = 0.3512\n",
            "[Epoch  5/25 Iter 10500/12500]: train_loss = 0.3461\n",
            "[Epoch  5/25 Iter 10750/12500]: train_loss = 0.3565\n",
            "[Epoch  5/25 Iter 11000/12500]: train_loss = 0.3288\n",
            "[Epoch  5/25 Iter 11250/12500]: train_loss = 0.3525\n",
            "[Epoch  5/25 Iter 11500/12500]: train_loss = 0.3314\n",
            "[Epoch  5/25 Iter 11750/12500]: train_loss = 0.3801\n",
            "[Epoch  5/25 Iter 12000/12500]: train_loss = 0.3102\n",
            "[Epoch  5/25 Iter 12250/12500]: train_loss = 0.3123\n",
            "[Epoch  5/25 Iter 12500/12500]: train_loss = 0.3214\n",
            "[Epoch  6/25 Iter   250/12500]: train_loss = 0.2754\n",
            "[Epoch  6/25 Iter   500/12500]: train_loss = 0.3227\n",
            "[Epoch  6/25 Iter   750/12500]: train_loss = 0.3308\n",
            "[Epoch  6/25 Iter  1000/12500]: train_loss = 0.3213\n",
            "[Epoch  6/25 Iter  1250/12500]: train_loss = 0.3452\n",
            "[Epoch  6/25 Iter  1500/12500]: train_loss = 0.3422\n",
            "[Epoch  6/25 Iter  1750/12500]: train_loss = 0.3167\n",
            "[Epoch  6/25 Iter  2000/12500]: train_loss = 0.3178\n",
            "[Epoch  6/25 Iter  2250/12500]: train_loss = 0.3337\n",
            "[Epoch  6/25 Iter  2500/12500]: train_loss = 0.3293\n",
            "[Epoch  6/25 Iter  2750/12500]: train_loss = 0.3167\n",
            "[Epoch  6/25 Iter  3000/12500]: train_loss = 0.3165\n",
            "[Epoch  6/25 Iter  3250/12500]: train_loss = 0.2897\n",
            "[Epoch  6/25 Iter  3500/12500]: train_loss = 0.3352\n",
            "[Epoch  6/25 Iter  3750/12500]: train_loss = 0.3296\n",
            "[Epoch  6/25 Iter  4000/12500]: train_loss = 0.3259\n",
            "[Epoch  6/25 Iter  4250/12500]: train_loss = 0.3144\n",
            "[Epoch  6/25 Iter  4500/12500]: train_loss = 0.3730\n",
            "[Epoch  6/25 Iter  4750/12500]: train_loss = 0.3130\n",
            "[Epoch  6/25 Iter  5000/12500]: train_loss = 0.2885\n",
            "[Epoch  6/25 Iter  5250/12500]: train_loss = 0.3524\n",
            "[Epoch  6/25 Iter  5500/12500]: train_loss = 0.3437\n",
            "[Epoch  6/25 Iter  5750/12500]: train_loss = 0.3563\n",
            "[Epoch  6/25 Iter  6000/12500]: train_loss = 0.3077\n",
            "[Epoch  6/25 Iter  6250/12500]: train_loss = 0.2760\n",
            "[Epoch  6/25 Iter  6500/12500]: train_loss = 0.3027\n",
            "[Epoch  6/25 Iter  6750/12500]: train_loss = 0.2996\n",
            "[Epoch  6/25 Iter  7000/12500]: train_loss = 0.2730\n",
            "[Epoch  6/25 Iter  7250/12500]: train_loss = 0.3128\n",
            "[Epoch  6/25 Iter  7500/12500]: train_loss = 0.2998\n",
            "[Epoch  6/25 Iter  7750/12500]: train_loss = 0.3428\n",
            "[Epoch  6/25 Iter  8000/12500]: train_loss = 0.2877\n",
            "[Epoch  6/25 Iter  8250/12500]: train_loss = 0.3087\n",
            "[Epoch  6/25 Iter  8500/12500]: train_loss = 0.3251\n",
            "[Epoch  6/25 Iter  8750/12500]: train_loss = 0.3331\n",
            "[Epoch  6/25 Iter  9000/12500]: train_loss = 0.2926\n",
            "[Epoch  6/25 Iter  9250/12500]: train_loss = 0.3471\n",
            "[Epoch  6/25 Iter  9500/12500]: train_loss = 0.3089\n",
            "[Epoch  6/25 Iter  9750/12500]: train_loss = 0.3084\n",
            "[Epoch  6/25 Iter 10000/12500]: train_loss = 0.2988\n",
            "[Epoch  6/25 Iter 10250/12500]: train_loss = 0.3297\n",
            "[Epoch  6/25 Iter 10500/12500]: train_loss = 0.3784\n",
            "[Epoch  6/25 Iter 10750/12500]: train_loss = 0.3131\n",
            "[Epoch  6/25 Iter 11000/12500]: train_loss = 0.3033\n",
            "[Epoch  6/25 Iter 11250/12500]: train_loss = 0.2951\n",
            "[Epoch  6/25 Iter 11500/12500]: train_loss = 0.3140\n",
            "[Epoch  6/25 Iter 11750/12500]: train_loss = 0.3503\n",
            "[Epoch  6/25 Iter 12000/12500]: train_loss = 0.3290\n",
            "[Epoch  6/25 Iter 12250/12500]: train_loss = 0.3696\n",
            "[Epoch  6/25 Iter 12500/12500]: train_loss = 0.3548\n",
            "[Epoch  7/25 Iter   250/12500]: train_loss = 0.3102\n",
            "[Epoch  7/25 Iter   500/12500]: train_loss = 0.3261\n",
            "[Epoch  7/25 Iter   750/12500]: train_loss = 0.2941\n",
            "[Epoch  7/25 Iter  1000/12500]: train_loss = 0.2544\n",
            "[Epoch  7/25 Iter  1250/12500]: train_loss = 0.2883\n",
            "[Epoch  7/25 Iter  1500/12500]: train_loss = 0.3071\n",
            "[Epoch  7/25 Iter  1750/12500]: train_loss = 0.3235\n",
            "[Epoch  7/25 Iter  2000/12500]: train_loss = 0.3211\n",
            "[Epoch  7/25 Iter  2250/12500]: train_loss = 0.3403\n",
            "[Epoch  7/25 Iter  2500/12500]: train_loss = 0.3313\n",
            "[Epoch  7/25 Iter  2750/12500]: train_loss = 0.3202\n",
            "[Epoch  7/25 Iter  3000/12500]: train_loss = 0.3107\n",
            "[Epoch  7/25 Iter  3250/12500]: train_loss = 0.3118\n",
            "[Epoch  7/25 Iter  3500/12500]: train_loss = 0.2756\n",
            "[Epoch  7/25 Iter  3750/12500]: train_loss = 0.3371\n",
            "[Epoch  7/25 Iter  4000/12500]: train_loss = 0.2540\n",
            "[Epoch  7/25 Iter  4250/12500]: train_loss = 0.3012\n",
            "[Epoch  7/25 Iter  4500/12500]: train_loss = 0.3295\n",
            "[Epoch  7/25 Iter  4750/12500]: train_loss = 0.2751\n",
            "[Epoch  7/25 Iter  5000/12500]: train_loss = 0.2767\n",
            "[Epoch  7/25 Iter  5250/12500]: train_loss = 0.3253\n",
            "[Epoch  7/25 Iter  5500/12500]: train_loss = 0.3384\n",
            "[Epoch  7/25 Iter  5750/12500]: train_loss = 0.3116\n",
            "[Epoch  7/25 Iter  6000/12500]: train_loss = 0.2542\n",
            "[Epoch  7/25 Iter  6250/12500]: train_loss = 0.3295\n",
            "[Epoch  7/25 Iter  6500/12500]: train_loss = 0.2887\n",
            "[Epoch  7/25 Iter  6750/12500]: train_loss = 0.3202\n",
            "[Epoch  7/25 Iter  7000/12500]: train_loss = 0.2867\n",
            "[Epoch  7/25 Iter  7250/12500]: train_loss = 0.3248\n",
            "[Epoch  7/25 Iter  7500/12500]: train_loss = 0.2967\n",
            "[Epoch  7/25 Iter  7750/12500]: train_loss = 0.3088\n",
            "[Epoch  7/25 Iter  8000/12500]: train_loss = 0.3534\n",
            "[Epoch  7/25 Iter  8250/12500]: train_loss = 0.2785\n",
            "[Epoch  7/25 Iter  8500/12500]: train_loss = 0.2986\n",
            "[Epoch  7/25 Iter  8750/12500]: train_loss = 0.3142\n",
            "[Epoch  7/25 Iter  9000/12500]: train_loss = 0.3218\n",
            "[Epoch  7/25 Iter  9250/12500]: train_loss = 0.2941\n",
            "[Epoch  7/25 Iter  9500/12500]: train_loss = 0.3150\n",
            "[Epoch  7/25 Iter  9750/12500]: train_loss = 0.3141\n",
            "[Epoch  7/25 Iter 10000/12500]: train_loss = 0.3255\n",
            "[Epoch  7/25 Iter 10250/12500]: train_loss = 0.3248\n",
            "[Epoch  7/25 Iter 10500/12500]: train_loss = 0.3330\n",
            "[Epoch  7/25 Iter 10750/12500]: train_loss = 0.2663\n",
            "[Epoch  7/25 Iter 11000/12500]: train_loss = 0.2965\n",
            "[Epoch  7/25 Iter 11250/12500]: train_loss = 0.3229\n",
            "[Epoch  7/25 Iter 11500/12500]: train_loss = 0.2799\n",
            "[Epoch  7/25 Iter 11750/12500]: train_loss = 0.3005\n",
            "[Epoch  7/25 Iter 12000/12500]: train_loss = 0.2926\n",
            "[Epoch  7/25 Iter 12250/12500]: train_loss = 0.3245\n",
            "[Epoch  7/25 Iter 12500/12500]: train_loss = 0.2701\n",
            "[Epoch  8/25 Iter   250/12500]: train_loss = 0.2883\n",
            "[Epoch  8/25 Iter   500/12500]: train_loss = 0.3126\n",
            "[Epoch  8/25 Iter   750/12500]: train_loss = 0.3100\n",
            "[Epoch  8/25 Iter  1000/12500]: train_loss = 0.3115\n",
            "[Epoch  8/25 Iter  1250/12500]: train_loss = 0.2964\n",
            "[Epoch  8/25 Iter  1500/12500]: train_loss = 0.3005\n",
            "[Epoch  8/25 Iter  1750/12500]: train_loss = 0.2634\n",
            "[Epoch  8/25 Iter  2000/12500]: train_loss = 0.2491\n",
            "[Epoch  8/25 Iter  2250/12500]: train_loss = 0.3259\n",
            "[Epoch  8/25 Iter  2500/12500]: train_loss = 0.2768\n",
            "[Epoch  8/25 Iter  2750/12500]: train_loss = 0.2646\n",
            "[Epoch  8/25 Iter  3000/12500]: train_loss = 0.3022\n",
            "[Epoch  8/25 Iter  3250/12500]: train_loss = 0.3020\n",
            "[Epoch  8/25 Iter  3500/12500]: train_loss = 0.3020\n",
            "[Epoch  8/25 Iter  3750/12500]: train_loss = 0.3321\n",
            "[Epoch  8/25 Iter  4000/12500]: train_loss = 0.3588\n",
            "[Epoch  8/25 Iter  4250/12500]: train_loss = 0.2733\n",
            "[Epoch  8/25 Iter  4500/12500]: train_loss = 0.3123\n",
            "[Epoch  8/25 Iter  4750/12500]: train_loss = 0.3034\n",
            "[Epoch  8/25 Iter  5000/12500]: train_loss = 0.3072\n",
            "[Epoch  8/25 Iter  5250/12500]: train_loss = 0.3129\n",
            "[Epoch  8/25 Iter  5500/12500]: train_loss = 0.2954\n",
            "[Epoch  8/25 Iter  5750/12500]: train_loss = 0.3061\n",
            "[Epoch  8/25 Iter  6000/12500]: train_loss = 0.2627\n",
            "[Epoch  8/25 Iter  6250/12500]: train_loss = 0.2688\n",
            "[Epoch  8/25 Iter  6500/12500]: train_loss = 0.3104\n",
            "[Epoch  8/25 Iter  6750/12500]: train_loss = 0.2802\n",
            "[Epoch  8/25 Iter  7000/12500]: train_loss = 0.2862\n",
            "[Epoch  8/25 Iter  7250/12500]: train_loss = 0.2487\n",
            "[Epoch  8/25 Iter  7500/12500]: train_loss = 0.3045\n",
            "[Epoch  8/25 Iter  7750/12500]: train_loss = 0.2797\n",
            "[Epoch  8/25 Iter  8000/12500]: train_loss = 0.2521\n",
            "[Epoch  8/25 Iter  8250/12500]: train_loss = 0.2831\n",
            "[Epoch  8/25 Iter  8500/12500]: train_loss = 0.3246\n",
            "[Epoch  8/25 Iter  8750/12500]: train_loss = 0.2727\n",
            "[Epoch  8/25 Iter  9000/12500]: train_loss = 0.2502\n",
            "[Epoch  8/25 Iter  9250/12500]: train_loss = 0.2740\n",
            "[Epoch  8/25 Iter  9500/12500]: train_loss = 0.3426\n",
            "[Epoch  8/25 Iter  9750/12500]: train_loss = 0.2841\n",
            "[Epoch  8/25 Iter 10000/12500]: train_loss = 0.3004\n",
            "[Epoch  8/25 Iter 10250/12500]: train_loss = 0.3120\n",
            "[Epoch  8/25 Iter 10500/12500]: train_loss = 0.3195\n",
            "[Epoch  8/25 Iter 10750/12500]: train_loss = 0.3118\n",
            "[Epoch  8/25 Iter 11000/12500]: train_loss = 0.2901\n",
            "[Epoch  8/25 Iter 11250/12500]: train_loss = 0.2631\n",
            "[Epoch  8/25 Iter 11500/12500]: train_loss = 0.2884\n",
            "[Epoch  8/25 Iter 11750/12500]: train_loss = 0.3144\n",
            "[Epoch  8/25 Iter 12000/12500]: train_loss = 0.2672\n",
            "[Epoch  8/25 Iter 12250/12500]: train_loss = 0.2578\n",
            "[Epoch  8/25 Iter 12500/12500]: train_loss = 0.2927\n",
            "[Epoch  9/25 Iter   250/12500]: train_loss = 0.3293\n",
            "[Epoch  9/25 Iter   500/12500]: train_loss = 0.2991\n",
            "[Epoch  9/25 Iter   750/12500]: train_loss = 0.2980\n",
            "[Epoch  9/25 Iter  1000/12500]: train_loss = 0.2525\n",
            "[Epoch  9/25 Iter  1250/12500]: train_loss = 0.3056\n",
            "[Epoch  9/25 Iter  1500/12500]: train_loss = 0.2617\n",
            "[Epoch  9/25 Iter  1750/12500]: train_loss = 0.2738\n",
            "[Epoch  9/25 Iter  2000/12500]: train_loss = 0.2689\n",
            "[Epoch  9/25 Iter  2250/12500]: train_loss = 0.2719\n",
            "[Epoch  9/25 Iter  2500/12500]: train_loss = 0.3358\n",
            "[Epoch  9/25 Iter  2750/12500]: train_loss = 0.2858\n",
            "[Epoch  9/25 Iter  3000/12500]: train_loss = 0.2722\n",
            "[Epoch  9/25 Iter  3250/12500]: train_loss = 0.3009\n",
            "[Epoch  9/25 Iter  3500/12500]: train_loss = 0.2446\n",
            "[Epoch  9/25 Iter  3750/12500]: train_loss = 0.2657\n",
            "[Epoch  9/25 Iter  4000/12500]: train_loss = 0.2925\n",
            "[Epoch  9/25 Iter  4250/12500]: train_loss = 0.2338\n",
            "[Epoch  9/25 Iter  4500/12500]: train_loss = 0.3095\n",
            "[Epoch  9/25 Iter  4750/12500]: train_loss = 0.2874\n",
            "[Epoch  9/25 Iter  5000/12500]: train_loss = 0.2958\n",
            "[Epoch  9/25 Iter  5250/12500]: train_loss = 0.2331\n",
            "[Epoch  9/25 Iter  5500/12500]: train_loss = 0.3174\n",
            "[Epoch  9/25 Iter  5750/12500]: train_loss = 0.2440\n",
            "[Epoch  9/25 Iter  6000/12500]: train_loss = 0.3070\n",
            "[Epoch  9/25 Iter  6250/12500]: train_loss = 0.3195\n",
            "[Epoch  9/25 Iter  6500/12500]: train_loss = 0.2874\n",
            "[Epoch  9/25 Iter  6750/12500]: train_loss = 0.2805\n",
            "[Epoch  9/25 Iter  7000/12500]: train_loss = 0.2671\n",
            "[Epoch  9/25 Iter  7250/12500]: train_loss = 0.2514\n",
            "[Epoch  9/25 Iter  7500/12500]: train_loss = 0.2826\n",
            "[Epoch  9/25 Iter  7750/12500]: train_loss = 0.2804\n",
            "[Epoch  9/25 Iter  8000/12500]: train_loss = 0.2972\n",
            "[Epoch  9/25 Iter  8250/12500]: train_loss = 0.2792\n",
            "[Epoch  9/25 Iter  8500/12500]: train_loss = 0.2885\n",
            "[Epoch  9/25 Iter  8750/12500]: train_loss = 0.2699\n",
            "[Epoch  9/25 Iter  9000/12500]: train_loss = 0.3356\n",
            "[Epoch  9/25 Iter  9250/12500]: train_loss = 0.3000\n",
            "[Epoch  9/25 Iter  9500/12500]: train_loss = 0.3104\n",
            "[Epoch  9/25 Iter  9750/12500]: train_loss = 0.2817\n",
            "[Epoch  9/25 Iter 10000/12500]: train_loss = 0.2356\n",
            "[Epoch  9/25 Iter 10250/12500]: train_loss = 0.3226\n",
            "[Epoch  9/25 Iter 10500/12500]: train_loss = 0.2936\n",
            "[Epoch  9/25 Iter 10750/12500]: train_loss = 0.2642\n",
            "[Epoch  9/25 Iter 11000/12500]: train_loss = 0.2458\n",
            "[Epoch  9/25 Iter 11250/12500]: train_loss = 0.3274\n",
            "[Epoch  9/25 Iter 11500/12500]: train_loss = 0.2569\n",
            "[Epoch  9/25 Iter 11750/12500]: train_loss = 0.2840\n",
            "[Epoch  9/25 Iter 12000/12500]: train_loss = 0.2633\n",
            "[Epoch  9/25 Iter 12250/12500]: train_loss = 0.2730\n",
            "[Epoch  9/25 Iter 12500/12500]: train_loss = 0.2853\n",
            "[Epoch 10/25 Iter   250/12500]: train_loss = 0.3022\n",
            "[Epoch 10/25 Iter   500/12500]: train_loss = 0.3087\n",
            "[Epoch 10/25 Iter   750/12500]: train_loss = 0.3067\n",
            "[Epoch 10/25 Iter  1000/12500]: train_loss = 0.2593\n",
            "[Epoch 10/25 Iter  1250/12500]: train_loss = 0.2910\n",
            "[Epoch 10/25 Iter  1500/12500]: train_loss = 0.2614\n",
            "[Epoch 10/25 Iter  1750/12500]: train_loss = 0.2660\n",
            "[Epoch 10/25 Iter  2000/12500]: train_loss = 0.2808\n",
            "[Epoch 10/25 Iter  2250/12500]: train_loss = 0.2951\n",
            "[Epoch 10/25 Iter  2500/12500]: train_loss = 0.2422\n",
            "[Epoch 10/25 Iter  2750/12500]: train_loss = 0.2747\n",
            "[Epoch 10/25 Iter  3000/12500]: train_loss = 0.2759\n",
            "[Epoch 10/25 Iter  3250/12500]: train_loss = 0.2628\n",
            "[Epoch 10/25 Iter  3500/12500]: train_loss = 0.2697\n",
            "[Epoch 10/25 Iter  3750/12500]: train_loss = 0.2885\n",
            "[Epoch 10/25 Iter  4000/12500]: train_loss = 0.2447\n",
            "[Epoch 10/25 Iter  4250/12500]: train_loss = 0.2381\n",
            "[Epoch 10/25 Iter  4500/12500]: train_loss = 0.2709\n",
            "[Epoch 10/25 Iter  4750/12500]: train_loss = 0.2679\n",
            "[Epoch 10/25 Iter  5000/12500]: train_loss = 0.2612\n",
            "[Epoch 10/25 Iter  5250/12500]: train_loss = 0.2582\n",
            "[Epoch 10/25 Iter  5500/12500]: train_loss = 0.2820\n",
            "[Epoch 10/25 Iter  5750/12500]: train_loss = 0.2763\n",
            "[Epoch 10/25 Iter  6000/12500]: train_loss = 0.2505\n",
            "[Epoch 10/25 Iter  6250/12500]: train_loss = 0.2445\n",
            "[Epoch 10/25 Iter  6500/12500]: train_loss = 0.2541\n",
            "[Epoch 10/25 Iter  6750/12500]: train_loss = 0.2539\n",
            "[Epoch 10/25 Iter  7000/12500]: train_loss = 0.2595\n",
            "[Epoch 10/25 Iter  7250/12500]: train_loss = 0.2586\n",
            "[Epoch 10/25 Iter  7500/12500]: train_loss = 0.2680\n",
            "[Epoch 10/25 Iter  7750/12500]: train_loss = 0.3089\n",
            "[Epoch 10/25 Iter  8000/12500]: train_loss = 0.3112\n",
            "[Epoch 10/25 Iter  8250/12500]: train_loss = 0.2768\n",
            "[Epoch 10/25 Iter  8500/12500]: train_loss = 0.2604\n",
            "[Epoch 10/25 Iter  8750/12500]: train_loss = 0.2801\n",
            "[Epoch 10/25 Iter  9000/12500]: train_loss = 0.2506\n",
            "[Epoch 10/25 Iter  9250/12500]: train_loss = 0.2597\n",
            "[Epoch 10/25 Iter  9500/12500]: train_loss = 0.2525\n",
            "[Epoch 10/25 Iter  9750/12500]: train_loss = 0.2643\n",
            "[Epoch 10/25 Iter 10000/12500]: train_loss = 0.2751\n",
            "[Epoch 10/25 Iter 10250/12500]: train_loss = 0.2457\n",
            "[Epoch 10/25 Iter 10500/12500]: train_loss = 0.2759\n",
            "[Epoch 10/25 Iter 10750/12500]: train_loss = 0.3000\n",
            "[Epoch 10/25 Iter 11000/12500]: train_loss = 0.2521\n",
            "[Epoch 10/25 Iter 11250/12500]: train_loss = 0.2744\n",
            "[Epoch 10/25 Iter 11500/12500]: train_loss = 0.3144\n",
            "[Epoch 10/25 Iter 11750/12500]: train_loss = 0.3398\n",
            "[Epoch 10/25 Iter 12000/12500]: train_loss = 0.2978\n",
            "[Epoch 10/25 Iter 12250/12500]: train_loss = 0.2669\n",
            "[Epoch 10/25 Iter 12500/12500]: train_loss = 0.2601\n",
            "[Epoch 11/25 Iter   250/12500]: train_loss = 0.2409\n",
            "[Epoch 11/25 Iter   500/12500]: train_loss = 0.2684\n",
            "[Epoch 11/25 Iter   750/12500]: train_loss = 0.2971\n",
            "[Epoch 11/25 Iter  1000/12500]: train_loss = 0.2679\n",
            "[Epoch 11/25 Iter  1250/12500]: train_loss = 0.2762\n",
            "[Epoch 11/25 Iter  1500/12500]: train_loss = 0.2497\n",
            "[Epoch 11/25 Iter  1750/12500]: train_loss = 0.2707\n",
            "[Epoch 11/25 Iter  2000/12500]: train_loss = 0.2574\n",
            "[Epoch 11/25 Iter  2250/12500]: train_loss = 0.2784\n",
            "[Epoch 11/25 Iter  2500/12500]: train_loss = 0.2774\n",
            "[Epoch 11/25 Iter  2750/12500]: train_loss = 0.2179\n",
            "[Epoch 11/25 Iter  3000/12500]: train_loss = 0.2878\n",
            "[Epoch 11/25 Iter  3250/12500]: train_loss = 0.2444\n",
            "[Epoch 11/25 Iter  3500/12500]: train_loss = 0.2942\n",
            "[Epoch 11/25 Iter  3750/12500]: train_loss = 0.1882\n",
            "[Epoch 11/25 Iter  4000/12500]: train_loss = 0.2602\n",
            "[Epoch 11/25 Iter  4250/12500]: train_loss = 0.2780\n",
            "[Epoch 11/25 Iter  4500/12500]: train_loss = 0.2799\n",
            "[Epoch 11/25 Iter  4750/12500]: train_loss = 0.2627\n",
            "[Epoch 11/25 Iter  5000/12500]: train_loss = 0.2674\n",
            "[Epoch 11/25 Iter  5250/12500]: train_loss = 0.2430\n",
            "[Epoch 11/25 Iter  5500/12500]: train_loss = 0.2752\n",
            "[Epoch 11/25 Iter  5750/12500]: train_loss = 0.2919\n",
            "[Epoch 11/25 Iter  6000/12500]: train_loss = 0.3039\n",
            "[Epoch 11/25 Iter  6250/12500]: train_loss = 0.2696\n",
            "[Epoch 11/25 Iter  6500/12500]: train_loss = 0.2571\n",
            "[Epoch 11/25 Iter  6750/12500]: train_loss = 0.2504\n",
            "[Epoch 11/25 Iter  7000/12500]: train_loss = 0.2393\n",
            "[Epoch 11/25 Iter  7250/12500]: train_loss = 0.2664\n",
            "[Epoch 11/25 Iter  7500/12500]: train_loss = 0.2712\n",
            "[Epoch 11/25 Iter  7750/12500]: train_loss = 0.2877\n",
            "[Epoch 11/25 Iter  8000/12500]: train_loss = 0.2837\n",
            "[Epoch 11/25 Iter  8250/12500]: train_loss = 0.2387\n",
            "[Epoch 11/25 Iter  8500/12500]: train_loss = 0.2713\n",
            "[Epoch 11/25 Iter  8750/12500]: train_loss = 0.3086\n",
            "[Epoch 11/25 Iter  9000/12500]: train_loss = 0.2444\n",
            "[Epoch 11/25 Iter  9250/12500]: train_loss = 0.2709\n",
            "[Epoch 11/25 Iter  9500/12500]: train_loss = 0.2750\n",
            "[Epoch 11/25 Iter  9750/12500]: train_loss = 0.2670\n",
            "[Epoch 11/25 Iter 10000/12500]: train_loss = 0.2689\n",
            "[Epoch 11/25 Iter 10250/12500]: train_loss = 0.2430\n",
            "[Epoch 11/25 Iter 10500/12500]: train_loss = 0.2670\n",
            "[Epoch 11/25 Iter 10750/12500]: train_loss = 0.2709\n",
            "[Epoch 11/25 Iter 11000/12500]: train_loss = 0.2587\n",
            "[Epoch 11/25 Iter 11250/12500]: train_loss = 0.2657\n",
            "[Epoch 11/25 Iter 11500/12500]: train_loss = 0.2696\n",
            "[Epoch 11/25 Iter 11750/12500]: train_loss = 0.2988\n",
            "[Epoch 11/25 Iter 12000/12500]: train_loss = 0.2618\n",
            "[Epoch 11/25 Iter 12250/12500]: train_loss = 0.2211\n",
            "[Epoch 11/25 Iter 12500/12500]: train_loss = 0.2278\n",
            "[Epoch 12/25 Iter   250/12500]: train_loss = 0.2297\n",
            "[Epoch 12/25 Iter   500/12500]: train_loss = 0.2513\n",
            "[Epoch 12/25 Iter   750/12500]: train_loss = 0.2743\n",
            "[Epoch 12/25 Iter  1000/12500]: train_loss = 0.2685\n",
            "[Epoch 12/25 Iter  1250/12500]: train_loss = 0.2468\n",
            "[Epoch 12/25 Iter  1500/12500]: train_loss = 0.2667\n",
            "[Epoch 12/25 Iter  1750/12500]: train_loss = 0.2574\n",
            "[Epoch 12/25 Iter  2000/12500]: train_loss = 0.2402\n",
            "[Epoch 12/25 Iter  2250/12500]: train_loss = 0.2830\n",
            "[Epoch 12/25 Iter  2500/12500]: train_loss = 0.2884\n",
            "[Epoch 12/25 Iter  2750/12500]: train_loss = 0.2846\n",
            "[Epoch 12/25 Iter  3000/12500]: train_loss = 0.2647\n",
            "[Epoch 12/25 Iter  3250/12500]: train_loss = 0.2393\n",
            "[Epoch 12/25 Iter  3500/12500]: train_loss = 0.2620\n",
            "[Epoch 12/25 Iter  3750/12500]: train_loss = 0.2649\n",
            "[Epoch 12/25 Iter  4000/12500]: train_loss = 0.2612\n",
            "[Epoch 12/25 Iter  4250/12500]: train_loss = 0.2740\n",
            "[Epoch 12/25 Iter  4500/12500]: train_loss = 0.2488\n",
            "[Epoch 12/25 Iter  4750/12500]: train_loss = 0.2745\n",
            "[Epoch 12/25 Iter  5000/12500]: train_loss = 0.2793\n",
            "[Epoch 12/25 Iter  5250/12500]: train_loss = 0.2503\n",
            "[Epoch 12/25 Iter  5500/12500]: train_loss = 0.2435\n",
            "[Epoch 12/25 Iter  5750/12500]: train_loss = 0.2816\n",
            "[Epoch 12/25 Iter  6000/12500]: train_loss = 0.2699\n",
            "[Epoch 12/25 Iter  6250/12500]: train_loss = 0.2677\n",
            "[Epoch 12/25 Iter  6500/12500]: train_loss = 0.2278\n",
            "[Epoch 12/25 Iter  6750/12500]: train_loss = 0.2500\n",
            "[Epoch 12/25 Iter  7000/12500]: train_loss = 0.2398\n",
            "[Epoch 12/25 Iter  7250/12500]: train_loss = 0.2584\n",
            "[Epoch 12/25 Iter  7500/12500]: train_loss = 0.2619\n",
            "[Epoch 12/25 Iter  7750/12500]: train_loss = 0.2429\n",
            "[Epoch 12/25 Iter  8000/12500]: train_loss = 0.2721\n",
            "[Epoch 12/25 Iter  8250/12500]: train_loss = 0.2365\n",
            "[Epoch 12/25 Iter  8500/12500]: train_loss = 0.2304\n",
            "[Epoch 12/25 Iter  8750/12500]: train_loss = 0.2419\n",
            "[Epoch 12/25 Iter  9000/12500]: train_loss = 0.2460\n",
            "[Epoch 12/25 Iter  9250/12500]: train_loss = 0.2708\n",
            "[Epoch 12/25 Iter  9500/12500]: train_loss = 0.2314\n",
            "[Epoch 12/25 Iter  9750/12500]: train_loss = 0.2736\n",
            "[Epoch 12/25 Iter 10000/12500]: train_loss = 0.2873\n",
            "[Epoch 12/25 Iter 10250/12500]: train_loss = 0.2672\n",
            "[Epoch 12/25 Iter 10500/12500]: train_loss = 0.2846\n",
            "[Epoch 12/25 Iter 10750/12500]: train_loss = 0.2311\n",
            "[Epoch 12/25 Iter 11000/12500]: train_loss = 0.2263\n",
            "[Epoch 12/25 Iter 11250/12500]: train_loss = 0.2425\n",
            "[Epoch 12/25 Iter 11500/12500]: train_loss = 0.2603\n",
            "[Epoch 12/25 Iter 11750/12500]: train_loss = 0.2654\n",
            "[Epoch 12/25 Iter 12000/12500]: train_loss = 0.2454\n",
            "[Epoch 12/25 Iter 12250/12500]: train_loss = 0.2290\n",
            "[Epoch 12/25 Iter 12500/12500]: train_loss = 0.2494\n",
            "[Epoch 13/25 Iter   250/12500]: train_loss = 0.2957\n",
            "[Epoch 13/25 Iter   500/12500]: train_loss = 0.2479\n",
            "[Epoch 13/25 Iter   750/12500]: train_loss = 0.2760\n",
            "[Epoch 13/25 Iter  1000/12500]: train_loss = 0.2679\n",
            "[Epoch 13/25 Iter  1250/12500]: train_loss = 0.2586\n",
            "[Epoch 13/25 Iter  1500/12500]: train_loss = 0.2784\n",
            "[Epoch 13/25 Iter  1750/12500]: train_loss = 0.2646\n",
            "[Epoch 13/25 Iter  2000/12500]: train_loss = 0.2699\n",
            "[Epoch 13/25 Iter  2250/12500]: train_loss = 0.2245\n",
            "[Epoch 13/25 Iter  2500/12500]: train_loss = 0.2472\n",
            "[Epoch 13/25 Iter  2750/12500]: train_loss = 0.2453\n",
            "[Epoch 13/25 Iter  3000/12500]: train_loss = 0.2797\n",
            "[Epoch 13/25 Iter  3250/12500]: train_loss = 0.2532\n",
            "[Epoch 13/25 Iter  3500/12500]: train_loss = 0.2015\n",
            "[Epoch 13/25 Iter  3750/12500]: train_loss = 0.2487\n",
            "[Epoch 13/25 Iter  4000/12500]: train_loss = 0.2174\n",
            "[Epoch 13/25 Iter  4250/12500]: train_loss = 0.2546\n",
            "[Epoch 13/25 Iter  4500/12500]: train_loss = 0.2666\n",
            "[Epoch 13/25 Iter  4750/12500]: train_loss = 0.1960\n",
            "[Epoch 13/25 Iter  5000/12500]: train_loss = 0.2891\n",
            "[Epoch 13/25 Iter  5250/12500]: train_loss = 0.2281\n",
            "[Epoch 13/25 Iter  5500/12500]: train_loss = 0.2492\n",
            "[Epoch 13/25 Iter  5750/12500]: train_loss = 0.2516\n",
            "[Epoch 13/25 Iter  6000/12500]: train_loss = 0.2040\n",
            "[Epoch 13/25 Iter  6250/12500]: train_loss = 0.2916\n",
            "[Epoch 13/25 Iter  6500/12500]: train_loss = 0.2730\n",
            "[Epoch 13/25 Iter  6750/12500]: train_loss = 0.2534\n",
            "[Epoch 13/25 Iter  7000/12500]: train_loss = 0.2176\n",
            "[Epoch 13/25 Iter  7250/12500]: train_loss = 0.2757\n",
            "[Epoch 13/25 Iter  7500/12500]: train_loss = 0.2522\n",
            "[Epoch 13/25 Iter  7750/12500]: train_loss = 0.2634\n",
            "[Epoch 13/25 Iter  8000/12500]: train_loss = 0.2502\n",
            "[Epoch 13/25 Iter  8250/12500]: train_loss = 0.2378\n",
            "[Epoch 13/25 Iter  8500/12500]: train_loss = 0.2503\n",
            "[Epoch 13/25 Iter  8750/12500]: train_loss = 0.2486\n",
            "[Epoch 13/25 Iter  9000/12500]: train_loss = 0.2697\n",
            "[Epoch 13/25 Iter  9250/12500]: train_loss = 0.2624\n",
            "[Epoch 13/25 Iter  9500/12500]: train_loss = 0.2334\n",
            "[Epoch 13/25 Iter  9750/12500]: train_loss = 0.2574\n",
            "[Epoch 13/25 Iter 10000/12500]: train_loss = 0.2110\n",
            "[Epoch 13/25 Iter 10250/12500]: train_loss = 0.2451\n",
            "[Epoch 13/25 Iter 10500/12500]: train_loss = 0.2692\n",
            "[Epoch 13/25 Iter 10750/12500]: train_loss = 0.2509\n",
            "[Epoch 13/25 Iter 11000/12500]: train_loss = 0.2585\n",
            "[Epoch 13/25 Iter 11250/12500]: train_loss = 0.2683\n",
            "[Epoch 13/25 Iter 11500/12500]: train_loss = 0.2680\n",
            "[Epoch 13/25 Iter 11750/12500]: train_loss = 0.2574\n",
            "[Epoch 13/25 Iter 12000/12500]: train_loss = 0.2738\n",
            "[Epoch 13/25 Iter 12250/12500]: train_loss = 0.2346\n",
            "[Epoch 13/25 Iter 12500/12500]: train_loss = 0.2618\n",
            "[Epoch 14/25 Iter   250/12500]: train_loss = 0.2453\n",
            "[Epoch 14/25 Iter   500/12500]: train_loss = 0.2193\n",
            "[Epoch 14/25 Iter   750/12500]: train_loss = 0.2610\n",
            "[Epoch 14/25 Iter  1000/12500]: train_loss = 0.2590\n",
            "[Epoch 14/25 Iter  1250/12500]: train_loss = 0.2457\n",
            "[Epoch 14/25 Iter  1500/12500]: train_loss = 0.2399\n",
            "[Epoch 14/25 Iter  1750/12500]: train_loss = 0.2426\n",
            "[Epoch 14/25 Iter  2000/12500]: train_loss = 0.2568\n",
            "[Epoch 14/25 Iter  2250/12500]: train_loss = 0.2151\n",
            "[Epoch 14/25 Iter  2500/12500]: train_loss = 0.2524\n",
            "[Epoch 14/25 Iter  2750/12500]: train_loss = 0.2310\n",
            "[Epoch 14/25 Iter  3000/12500]: train_loss = 0.2699\n",
            "[Epoch 14/25 Iter  3250/12500]: train_loss = 0.2700\n",
            "[Epoch 14/25 Iter  3500/12500]: train_loss = 0.2489\n",
            "[Epoch 14/25 Iter  3750/12500]: train_loss = 0.2279\n",
            "[Epoch 14/25 Iter  4000/12500]: train_loss = 0.2513\n",
            "[Epoch 14/25 Iter  4250/12500]: train_loss = 0.2288\n",
            "[Epoch 14/25 Iter  4500/12500]: train_loss = 0.2525\n",
            "[Epoch 14/25 Iter  4750/12500]: train_loss = 0.2616\n",
            "[Epoch 14/25 Iter  5000/12500]: train_loss = 0.2480\n",
            "[Epoch 14/25 Iter  5250/12500]: train_loss = 0.2393\n",
            "[Epoch 14/25 Iter  5500/12500]: train_loss = 0.2231\n",
            "[Epoch 14/25 Iter  5750/12500]: train_loss = 0.2786\n",
            "[Epoch 14/25 Iter  6000/12500]: train_loss = 0.2543\n",
            "[Epoch 14/25 Iter  6250/12500]: train_loss = 0.2692\n",
            "[Epoch 14/25 Iter  6500/12500]: train_loss = 0.2597\n",
            "[Epoch 14/25 Iter  6750/12500]: train_loss = 0.2379\n",
            "[Epoch 14/25 Iter  7000/12500]: train_loss = 0.2123\n",
            "[Epoch 14/25 Iter  7250/12500]: train_loss = 0.2494\n",
            "[Epoch 14/25 Iter  7500/12500]: train_loss = 0.2395\n",
            "[Epoch 14/25 Iter  7750/12500]: train_loss = 0.2376\n",
            "[Epoch 14/25 Iter  8000/12500]: train_loss = 0.2633\n",
            "[Epoch 14/25 Iter  8250/12500]: train_loss = 0.2339\n",
            "[Epoch 14/25 Iter  8500/12500]: train_loss = 0.2388\n",
            "[Epoch 14/25 Iter  8750/12500]: train_loss = 0.2287\n",
            "[Epoch 14/25 Iter  9000/12500]: train_loss = 0.2776\n",
            "[Epoch 14/25 Iter  9250/12500]: train_loss = 0.2289\n",
            "[Epoch 14/25 Iter  9500/12500]: train_loss = 0.2180\n",
            "[Epoch 14/25 Iter  9750/12500]: train_loss = 0.2596\n",
            "[Epoch 14/25 Iter 10000/12500]: train_loss = 0.2138\n",
            "[Epoch 14/25 Iter 10250/12500]: train_loss = 0.2645\n",
            "[Epoch 14/25 Iter 10500/12500]: train_loss = 0.2410\n",
            "[Epoch 14/25 Iter 10750/12500]: train_loss = 0.2351\n",
            "[Epoch 14/25 Iter 11000/12500]: train_loss = 0.2429\n",
            "[Epoch 14/25 Iter 11250/12500]: train_loss = 0.2092\n",
            "[Epoch 14/25 Iter 11500/12500]: train_loss = 0.2333\n",
            "[Epoch 14/25 Iter 11750/12500]: train_loss = 0.2350\n",
            "[Epoch 14/25 Iter 12000/12500]: train_loss = 0.2378\n",
            "[Epoch 14/25 Iter 12250/12500]: train_loss = 0.2387\n",
            "[Epoch 14/25 Iter 12500/12500]: train_loss = 0.2630\n",
            "[Epoch 15/25 Iter   250/12500]: train_loss = 0.2254\n",
            "[Epoch 15/25 Iter   500/12500]: train_loss = 0.2271\n",
            "[Epoch 15/25 Iter   750/12500]: train_loss = 0.2555\n",
            "[Epoch 15/25 Iter  1000/12500]: train_loss = 0.2616\n",
            "[Epoch 15/25 Iter  1250/12500]: train_loss = 0.2576\n",
            "[Epoch 15/25 Iter  1500/12500]: train_loss = 0.2226\n",
            "[Epoch 15/25 Iter  1750/12500]: train_loss = 0.2243\n",
            "[Epoch 15/25 Iter  2000/12500]: train_loss = 0.2292\n",
            "[Epoch 15/25 Iter  2250/12500]: train_loss = 0.2567\n",
            "[Epoch 15/25 Iter  2500/12500]: train_loss = 0.2156\n",
            "[Epoch 15/25 Iter  2750/12500]: train_loss = 0.2425\n",
            "[Epoch 15/25 Iter  3000/12500]: train_loss = 0.2424\n",
            "[Epoch 15/25 Iter  3250/12500]: train_loss = 0.2387\n",
            "[Epoch 15/25 Iter  3500/12500]: train_loss = 0.1962\n",
            "[Epoch 15/25 Iter  3750/12500]: train_loss = 0.2378\n",
            "[Epoch 15/25 Iter  4000/12500]: train_loss = 0.2393\n",
            "[Epoch 15/25 Iter  4250/12500]: train_loss = 0.2443\n",
            "[Epoch 15/25 Iter  4500/12500]: train_loss = 0.2590\n",
            "[Epoch 15/25 Iter  4750/12500]: train_loss = 0.2482\n",
            "[Epoch 15/25 Iter  5000/12500]: train_loss = 0.2412\n",
            "[Epoch 15/25 Iter  5250/12500]: train_loss = 0.2548\n",
            "[Epoch 15/25 Iter  5500/12500]: train_loss = 0.2583\n",
            "[Epoch 15/25 Iter  5750/12500]: train_loss = 0.2483\n",
            "[Epoch 15/25 Iter  6000/12500]: train_loss = 0.2249\n",
            "[Epoch 15/25 Iter  6250/12500]: train_loss = 0.2463\n",
            "[Epoch 15/25 Iter  6500/12500]: train_loss = 0.2720\n",
            "[Epoch 15/25 Iter  6750/12500]: train_loss = 0.2486\n",
            "[Epoch 15/25 Iter  7000/12500]: train_loss = 0.2106\n",
            "[Epoch 15/25 Iter  7250/12500]: train_loss = 0.2548\n",
            "[Epoch 15/25 Iter  7500/12500]: train_loss = 0.2387\n",
            "[Epoch 15/25 Iter  7750/12500]: train_loss = 0.2710\n",
            "[Epoch 15/25 Iter  8000/12500]: train_loss = 0.2313\n",
            "[Epoch 15/25 Iter  8250/12500]: train_loss = 0.2284\n",
            "[Epoch 15/25 Iter  8500/12500]: train_loss = 0.2043\n",
            "[Epoch 15/25 Iter  8750/12500]: train_loss = 0.2373\n",
            "[Epoch 15/25 Iter  9000/12500]: train_loss = 0.2067\n",
            "[Epoch 15/25 Iter  9250/12500]: train_loss = 0.2165\n",
            "[Epoch 15/25 Iter  9500/12500]: train_loss = 0.2523\n",
            "[Epoch 15/25 Iter  9750/12500]: train_loss = 0.2305\n",
            "[Epoch 15/25 Iter 10000/12500]: train_loss = 0.2296\n",
            "[Epoch 15/25 Iter 10250/12500]: train_loss = 0.2367\n",
            "[Epoch 15/25 Iter 10500/12500]: train_loss = 0.2180\n",
            "[Epoch 15/25 Iter 10750/12500]: train_loss = 0.2307\n",
            "[Epoch 15/25 Iter 11000/12500]: train_loss = 0.2332\n",
            "[Epoch 15/25 Iter 11250/12500]: train_loss = 0.2501\n",
            "[Epoch 15/25 Iter 11500/12500]: train_loss = 0.2477\n",
            "[Epoch 15/25 Iter 11750/12500]: train_loss = 0.2779\n",
            "[Epoch 15/25 Iter 12000/12500]: train_loss = 0.2860\n",
            "[Epoch 15/25 Iter 12250/12500]: train_loss = 0.2325\n",
            "[Epoch 15/25 Iter 12500/12500]: train_loss = 0.2361\n",
            "[Epoch 16/25 Iter   250/12500]: train_loss = 0.2346\n",
            "[Epoch 16/25 Iter   500/12500]: train_loss = 0.1962\n",
            "[Epoch 16/25 Iter   750/12500]: train_loss = 0.2078\n",
            "[Epoch 16/25 Iter  1000/12500]: train_loss = 0.2428\n",
            "[Epoch 16/25 Iter  1250/12500]: train_loss = 0.2391\n",
            "[Epoch 16/25 Iter  1500/12500]: train_loss = 0.2144\n",
            "[Epoch 16/25 Iter  1750/12500]: train_loss = 0.2708\n",
            "[Epoch 16/25 Iter  2000/12500]: train_loss = 0.2363\n",
            "[Epoch 16/25 Iter  2250/12500]: train_loss = 0.2422\n",
            "[Epoch 16/25 Iter  2500/12500]: train_loss = 0.2223\n",
            "[Epoch 16/25 Iter  2750/12500]: train_loss = 0.2272\n",
            "[Epoch 16/25 Iter  3000/12500]: train_loss = 0.2309\n",
            "[Epoch 16/25 Iter  3250/12500]: train_loss = 0.2264\n",
            "[Epoch 16/25 Iter  3500/12500]: train_loss = 0.1904\n",
            "[Epoch 16/25 Iter  3750/12500]: train_loss = 0.2608\n",
            "[Epoch 16/25 Iter  4000/12500]: train_loss = 0.2205\n",
            "[Epoch 16/25 Iter  4250/12500]: train_loss = 0.2492\n",
            "[Epoch 16/25 Iter  4500/12500]: train_loss = 0.2549\n",
            "[Epoch 16/25 Iter  4750/12500]: train_loss = 0.2147\n",
            "[Epoch 16/25 Iter  5000/12500]: train_loss = 0.2746\n",
            "[Epoch 16/25 Iter  5250/12500]: train_loss = 0.2428\n",
            "[Epoch 16/25 Iter  5500/12500]: train_loss = 0.2186\n",
            "[Epoch 16/25 Iter  5750/12500]: train_loss = 0.2048\n",
            "[Epoch 16/25 Iter  6000/12500]: train_loss = 0.2308\n",
            "[Epoch 16/25 Iter  6250/12500]: train_loss = 0.2197\n",
            "[Epoch 16/25 Iter  6500/12500]: train_loss = 0.2409\n",
            "[Epoch 16/25 Iter  6750/12500]: train_loss = 0.2404\n",
            "[Epoch 16/25 Iter  7000/12500]: train_loss = 0.2234\n",
            "[Epoch 16/25 Iter  7250/12500]: train_loss = 0.2328\n",
            "[Epoch 16/25 Iter  7500/12500]: train_loss = 0.1804\n",
            "[Epoch 16/25 Iter  7750/12500]: train_loss = 0.2341\n",
            "[Epoch 16/25 Iter  8000/12500]: train_loss = 0.2443\n",
            "[Epoch 16/25 Iter  8250/12500]: train_loss = 0.2638\n",
            "[Epoch 16/25 Iter  8500/12500]: train_loss = 0.2423\n",
            "[Epoch 16/25 Iter  8750/12500]: train_loss = 0.2302\n",
            "[Epoch 16/25 Iter  9000/12500]: train_loss = 0.2068\n",
            "[Epoch 16/25 Iter  9250/12500]: train_loss = 0.2276\n",
            "[Epoch 16/25 Iter  9500/12500]: train_loss = 0.2148\n",
            "[Epoch 16/25 Iter  9750/12500]: train_loss = 0.2391\n",
            "[Epoch 16/25 Iter 10000/12500]: train_loss = 0.2509\n",
            "[Epoch 16/25 Iter 10250/12500]: train_loss = 0.2634\n",
            "[Epoch 16/25 Iter 10500/12500]: train_loss = 0.2360\n",
            "[Epoch 16/25 Iter 10750/12500]: train_loss = 0.2278\n",
            "[Epoch 16/25 Iter 11000/12500]: train_loss = 0.2131\n",
            "[Epoch 16/25 Iter 11250/12500]: train_loss = 0.1920\n",
            "[Epoch 16/25 Iter 11500/12500]: train_loss = 0.2453\n",
            "[Epoch 16/25 Iter 11750/12500]: train_loss = 0.2327\n",
            "[Epoch 16/25 Iter 12000/12500]: train_loss = 0.2230\n",
            "[Epoch 16/25 Iter 12250/12500]: train_loss = 0.2259\n",
            "[Epoch 16/25 Iter 12500/12500]: train_loss = 0.2449\n",
            "[Epoch 17/25 Iter   250/12500]: train_loss = 0.2473\n",
            "[Epoch 17/25 Iter   500/12500]: train_loss = 0.2589\n",
            "[Epoch 17/25 Iter   750/12500]: train_loss = 0.2101\n",
            "[Epoch 17/25 Iter  1000/12500]: train_loss = 0.2150\n",
            "[Epoch 17/25 Iter  1250/12500]: train_loss = 0.2010\n",
            "[Epoch 17/25 Iter  1500/12500]: train_loss = 0.1918\n",
            "[Epoch 17/25 Iter  1750/12500]: train_loss = 0.2549\n",
            "[Epoch 17/25 Iter  2000/12500]: train_loss = 0.2011\n",
            "[Epoch 17/25 Iter  2250/12500]: train_loss = 0.2447\n",
            "[Epoch 17/25 Iter  2500/12500]: train_loss = 0.2345\n",
            "[Epoch 17/25 Iter  2750/12500]: train_loss = 0.2567\n",
            "[Epoch 17/25 Iter  3000/12500]: train_loss = 0.1970\n",
            "[Epoch 17/25 Iter  3250/12500]: train_loss = 0.2730\n",
            "[Epoch 17/25 Iter  3500/12500]: train_loss = 0.1895\n",
            "[Epoch 17/25 Iter  3750/12500]: train_loss = 0.2516\n",
            "[Epoch 17/25 Iter  4000/12500]: train_loss = 0.2185\n",
            "[Epoch 17/25 Iter  4250/12500]: train_loss = 0.2458\n",
            "[Epoch 17/25 Iter  4500/12500]: train_loss = 0.2295\n",
            "[Epoch 17/25 Iter  4750/12500]: train_loss = 0.2374\n",
            "[Epoch 17/25 Iter  5000/12500]: train_loss = 0.2401\n",
            "[Epoch 17/25 Iter  5250/12500]: train_loss = 0.2342\n",
            "[Epoch 17/25 Iter  5500/12500]: train_loss = 0.2183\n",
            "[Epoch 17/25 Iter  5750/12500]: train_loss = 0.1961\n",
            "[Epoch 17/25 Iter  6000/12500]: train_loss = 0.2178\n",
            "[Epoch 17/25 Iter  6250/12500]: train_loss = 0.1982\n",
            "[Epoch 17/25 Iter  6500/12500]: train_loss = 0.2596\n",
            "[Epoch 17/25 Iter  6750/12500]: train_loss = 0.2061\n",
            "[Epoch 17/25 Iter  7000/12500]: train_loss = 0.2405\n",
            "[Epoch 17/25 Iter  7250/12500]: train_loss = 0.2205\n",
            "[Epoch 17/25 Iter  7500/12500]: train_loss = 0.2150\n",
            "[Epoch 17/25 Iter  7750/12500]: train_loss = 0.2422\n",
            "[Epoch 17/25 Iter  8000/12500]: train_loss = 0.2356\n",
            "[Epoch 17/25 Iter  8250/12500]: train_loss = 0.2246\n",
            "[Epoch 17/25 Iter  8500/12500]: train_loss = 0.2163\n",
            "[Epoch 17/25 Iter  8750/12500]: train_loss = 0.2230\n",
            "[Epoch 17/25 Iter  9000/12500]: train_loss = 0.2186\n",
            "[Epoch 17/25 Iter  9250/12500]: train_loss = 0.2086\n",
            "[Epoch 17/25 Iter  9500/12500]: train_loss = 0.2191\n",
            "[Epoch 17/25 Iter  9750/12500]: train_loss = 0.2282\n",
            "[Epoch 17/25 Iter 10000/12500]: train_loss = 0.1869\n",
            "[Epoch 17/25 Iter 10250/12500]: train_loss = 0.2304\n",
            "[Epoch 17/25 Iter 10500/12500]: train_loss = 0.1926\n",
            "[Epoch 17/25 Iter 10750/12500]: train_loss = 0.2054\n",
            "[Epoch 17/25 Iter 11000/12500]: train_loss = 0.2208\n",
            "[Epoch 17/25 Iter 11250/12500]: train_loss = 0.2615\n",
            "[Epoch 17/25 Iter 11500/12500]: train_loss = 0.2435\n",
            "[Epoch 17/25 Iter 11750/12500]: train_loss = 0.2522\n",
            "[Epoch 17/25 Iter 12000/12500]: train_loss = 0.2221\n",
            "[Epoch 17/25 Iter 12250/12500]: train_loss = 0.2197\n",
            "[Epoch 17/25 Iter 12500/12500]: train_loss = 0.2253\n",
            "[Epoch 18/25 Iter   250/12500]: train_loss = 0.1852\n",
            "[Epoch 18/25 Iter   500/12500]: train_loss = 0.1941\n",
            "[Epoch 18/25 Iter   750/12500]: train_loss = 0.2754\n",
            "[Epoch 18/25 Iter  1000/12500]: train_loss = 0.1868\n",
            "[Epoch 18/25 Iter  1250/12500]: train_loss = 0.2202\n",
            "[Epoch 18/25 Iter  1500/12500]: train_loss = 0.2461\n",
            "[Epoch 18/25 Iter  1750/12500]: train_loss = 0.2318\n",
            "[Epoch 18/25 Iter  2000/12500]: train_loss = 0.2228\n",
            "[Epoch 18/25 Iter  2250/12500]: train_loss = 0.2027\n",
            "[Epoch 18/25 Iter  2500/12500]: train_loss = 0.2190\n",
            "[Epoch 18/25 Iter  2750/12500]: train_loss = 0.1693\n",
            "[Epoch 18/25 Iter  3000/12500]: train_loss = 0.2392\n",
            "[Epoch 18/25 Iter  3250/12500]: train_loss = 0.2043\n",
            "[Epoch 18/25 Iter  3500/12500]: train_loss = 0.1997\n",
            "[Epoch 18/25 Iter  3750/12500]: train_loss = 0.2343\n",
            "[Epoch 18/25 Iter  4000/12500]: train_loss = 0.2388\n",
            "[Epoch 18/25 Iter  4250/12500]: train_loss = 0.1952\n",
            "[Epoch 18/25 Iter  4500/12500]: train_loss = 0.2295\n",
            "[Epoch 18/25 Iter  4750/12500]: train_loss = 0.2064\n",
            "[Epoch 18/25 Iter  5000/12500]: train_loss = 0.2407\n",
            "[Epoch 18/25 Iter  5250/12500]: train_loss = 0.2219\n",
            "[Epoch 18/25 Iter  5500/12500]: train_loss = 0.2336\n",
            "[Epoch 18/25 Iter  5750/12500]: train_loss = 0.2247\n",
            "[Epoch 18/25 Iter  6000/12500]: train_loss = 0.2068\n",
            "[Epoch 18/25 Iter  6250/12500]: train_loss = 0.2142\n",
            "[Epoch 18/25 Iter  6500/12500]: train_loss = 0.2454\n",
            "[Epoch 18/25 Iter  6750/12500]: train_loss = 0.2179\n",
            "[Epoch 18/25 Iter  7000/12500]: train_loss = 0.2235\n",
            "[Epoch 18/25 Iter  7250/12500]: train_loss = 0.2531\n",
            "[Epoch 18/25 Iter  7500/12500]: train_loss = 0.2547\n",
            "[Epoch 18/25 Iter  7750/12500]: train_loss = 0.2126\n",
            "[Epoch 18/25 Iter  8000/12500]: train_loss = 0.2032\n",
            "[Epoch 18/25 Iter  8250/12500]: train_loss = 0.2310\n",
            "[Epoch 18/25 Iter  8500/12500]: train_loss = 0.1963\n",
            "[Epoch 18/25 Iter  8750/12500]: train_loss = 0.1963\n",
            "[Epoch 18/25 Iter  9000/12500]: train_loss = 0.2512\n",
            "[Epoch 18/25 Iter  9250/12500]: train_loss = 0.2239\n",
            "[Epoch 18/25 Iter  9500/12500]: train_loss = 0.2040\n",
            "[Epoch 18/25 Iter  9750/12500]: train_loss = 0.2331\n",
            "[Epoch 18/25 Iter 10000/12500]: train_loss = 0.2073\n",
            "[Epoch 18/25 Iter 10250/12500]: train_loss = 0.2328\n",
            "[Epoch 18/25 Iter 10500/12500]: train_loss = 0.2356\n",
            "[Epoch 18/25 Iter 10750/12500]: train_loss = 0.2186\n",
            "[Epoch 18/25 Iter 11000/12500]: train_loss = 0.2207\n",
            "[Epoch 18/25 Iter 11250/12500]: train_loss = 0.2352\n",
            "[Epoch 18/25 Iter 11500/12500]: train_loss = 0.2459\n",
            "[Epoch 18/25 Iter 11750/12500]: train_loss = 0.2733\n",
            "[Epoch 18/25 Iter 12000/12500]: train_loss = 0.2296\n",
            "[Epoch 18/25 Iter 12250/12500]: train_loss = 0.2189\n",
            "[Epoch 18/25 Iter 12500/12500]: train_loss = 0.2296\n",
            "[Epoch 19/25 Iter   250/12500]: train_loss = 0.2250\n",
            "[Epoch 19/25 Iter   500/12500]: train_loss = 0.1984\n",
            "[Epoch 19/25 Iter   750/12500]: train_loss = 0.2143\n",
            "[Epoch 19/25 Iter  1000/12500]: train_loss = 0.2234\n",
            "[Epoch 19/25 Iter  1250/12500]: train_loss = 0.2412\n",
            "[Epoch 19/25 Iter  1500/12500]: train_loss = 0.2083\n",
            "[Epoch 19/25 Iter  1750/12500]: train_loss = 0.1879\n",
            "[Epoch 19/25 Iter  2000/12500]: train_loss = 0.1887\n",
            "[Epoch 19/25 Iter  2250/12500]: train_loss = 0.1983\n",
            "[Epoch 19/25 Iter  2500/12500]: train_loss = 0.2225\n",
            "[Epoch 19/25 Iter  2750/12500]: train_loss = 0.2531\n",
            "[Epoch 19/25 Iter  3000/12500]: train_loss = 0.2357\n",
            "[Epoch 19/25 Iter  3250/12500]: train_loss = 0.2483\n",
            "[Epoch 19/25 Iter  3500/12500]: train_loss = 0.2042\n",
            "[Epoch 19/25 Iter  3750/12500]: train_loss = 0.2019\n",
            "[Epoch 19/25 Iter  4000/12500]: train_loss = 0.2024\n",
            "[Epoch 19/25 Iter  4250/12500]: train_loss = 0.2200\n",
            "[Epoch 19/25 Iter  4500/12500]: train_loss = 0.2302\n",
            "[Epoch 19/25 Iter  4750/12500]: train_loss = 0.2057\n",
            "[Epoch 19/25 Iter  5000/12500]: train_loss = 0.2218\n",
            "[Epoch 19/25 Iter  5250/12500]: train_loss = 0.1897\n",
            "[Epoch 19/25 Iter  5500/12500]: train_loss = 0.2118\n",
            "[Epoch 19/25 Iter  5750/12500]: train_loss = 0.2315\n",
            "[Epoch 19/25 Iter  6000/12500]: train_loss = 0.2356\n",
            "[Epoch 19/25 Iter  6250/12500]: train_loss = 0.2476\n",
            "[Epoch 19/25 Iter  6500/12500]: train_loss = 0.2155\n",
            "[Epoch 19/25 Iter  6750/12500]: train_loss = 0.2036\n",
            "[Epoch 19/25 Iter  7000/12500]: train_loss = 0.2350\n",
            "[Epoch 19/25 Iter  7250/12500]: train_loss = 0.2465\n",
            "[Epoch 19/25 Iter  7500/12500]: train_loss = 0.1955\n",
            "[Epoch 19/25 Iter  7750/12500]: train_loss = 0.2205\n",
            "[Epoch 19/25 Iter  8000/12500]: train_loss = 0.2620\n",
            "[Epoch 19/25 Iter  8250/12500]: train_loss = 0.2170\n",
            "[Epoch 19/25 Iter  8500/12500]: train_loss = 0.2096\n",
            "[Epoch 19/25 Iter  8750/12500]: train_loss = 0.2463\n",
            "[Epoch 19/25 Iter  9000/12500]: train_loss = 0.1906\n",
            "[Epoch 19/25 Iter  9250/12500]: train_loss = 0.2180\n",
            "[Epoch 19/25 Iter  9500/12500]: train_loss = 0.2366\n",
            "[Epoch 19/25 Iter  9750/12500]: train_loss = 0.2229\n",
            "[Epoch 19/25 Iter 10000/12500]: train_loss = 0.1864\n",
            "[Epoch 19/25 Iter 10250/12500]: train_loss = 0.1943\n",
            "[Epoch 19/25 Iter 10500/12500]: train_loss = 0.1921\n",
            "[Epoch 19/25 Iter 10750/12500]: train_loss = 0.1923\n",
            "[Epoch 19/25 Iter 11000/12500]: train_loss = 0.2292\n",
            "[Epoch 19/25 Iter 11250/12500]: train_loss = 0.2079\n",
            "[Epoch 19/25 Iter 11500/12500]: train_loss = 0.2680\n",
            "[Epoch 19/25 Iter 11750/12500]: train_loss = 0.2174\n",
            "[Epoch 19/25 Iter 12000/12500]: train_loss = 0.2124\n",
            "[Epoch 19/25 Iter 12250/12500]: train_loss = 0.2145\n",
            "[Epoch 19/25 Iter 12500/12500]: train_loss = 0.2007\n",
            "[Epoch 20/25 Iter   250/12500]: train_loss = 0.2116\n",
            "[Epoch 20/25 Iter   500/12500]: train_loss = 0.2027\n",
            "[Epoch 20/25 Iter   750/12500]: train_loss = 0.2118\n",
            "[Epoch 20/25 Iter  1000/12500]: train_loss = 0.2479\n",
            "[Epoch 20/25 Iter  1250/12500]: train_loss = 0.2489\n",
            "[Epoch 20/25 Iter  1500/12500]: train_loss = 0.1810\n",
            "[Epoch 20/25 Iter  1750/12500]: train_loss = 0.1657\n",
            "[Epoch 20/25 Iter  2000/12500]: train_loss = 0.1817\n",
            "[Epoch 20/25 Iter  2250/12500]: train_loss = 0.2080\n",
            "[Epoch 20/25 Iter  2500/12500]: train_loss = 0.2108\n",
            "[Epoch 20/25 Iter  2750/12500]: train_loss = 0.2158\n",
            "[Epoch 20/25 Iter  3000/12500]: train_loss = 0.2014\n",
            "[Epoch 20/25 Iter  3250/12500]: train_loss = 0.2055\n",
            "[Epoch 20/25 Iter  3500/12500]: train_loss = 0.1892\n",
            "[Epoch 20/25 Iter  3750/12500]: train_loss = 0.2318\n",
            "[Epoch 20/25 Iter  4000/12500]: train_loss = 0.1937\n",
            "[Epoch 20/25 Iter  4250/12500]: train_loss = 0.2087\n",
            "[Epoch 20/25 Iter  4500/12500]: train_loss = 0.2069\n",
            "[Epoch 20/25 Iter  4750/12500]: train_loss = 0.2516\n",
            "[Epoch 20/25 Iter  5000/12500]: train_loss = 0.2294\n",
            "[Epoch 20/25 Iter  5250/12500]: train_loss = 0.1903\n",
            "[Epoch 20/25 Iter  5500/12500]: train_loss = 0.2006\n",
            "[Epoch 20/25 Iter  5750/12500]: train_loss = 0.2280\n",
            "[Epoch 20/25 Iter  6000/12500]: train_loss = 0.2082\n",
            "[Epoch 20/25 Iter  6250/12500]: train_loss = 0.2028\n",
            "[Epoch 20/25 Iter  6500/12500]: train_loss = 0.2043\n",
            "[Epoch 20/25 Iter  6750/12500]: train_loss = 0.2523\n",
            "[Epoch 20/25 Iter  7000/12500]: train_loss = 0.2251\n",
            "[Epoch 20/25 Iter  7250/12500]: train_loss = 0.2127\n",
            "[Epoch 20/25 Iter  7500/12500]: train_loss = 0.2007\n",
            "[Epoch 20/25 Iter  7750/12500]: train_loss = 0.2418\n",
            "[Epoch 20/25 Iter  8000/12500]: train_loss = 0.2270\n",
            "[Epoch 20/25 Iter  8250/12500]: train_loss = 0.2076\n",
            "[Epoch 20/25 Iter  8500/12500]: train_loss = 0.2392\n",
            "[Epoch 20/25 Iter  8750/12500]: train_loss = 0.2170\n",
            "[Epoch 20/25 Iter  9000/12500]: train_loss = 0.1986\n",
            "[Epoch 20/25 Iter  9250/12500]: train_loss = 0.1828\n",
            "[Epoch 20/25 Iter  9500/12500]: train_loss = 0.2495\n",
            "[Epoch 20/25 Iter  9750/12500]: train_loss = 0.2227\n",
            "[Epoch 20/25 Iter 10000/12500]: train_loss = 0.2204\n",
            "[Epoch 20/25 Iter 10250/12500]: train_loss = 0.1867\n",
            "[Epoch 20/25 Iter 10500/12500]: train_loss = 0.2035\n",
            "[Epoch 20/25 Iter 10750/12500]: train_loss = 0.2129\n",
            "[Epoch 20/25 Iter 11000/12500]: train_loss = 0.2671\n",
            "[Epoch 20/25 Iter 11250/12500]: train_loss = 0.2018\n",
            "[Epoch 20/25 Iter 11500/12500]: train_loss = 0.2251\n",
            "[Epoch 20/25 Iter 11750/12500]: train_loss = 0.2028\n",
            "[Epoch 20/25 Iter 12000/12500]: train_loss = 0.2370\n",
            "[Epoch 20/25 Iter 12250/12500]: train_loss = 0.2305\n",
            "[Epoch 20/25 Iter 12500/12500]: train_loss = 0.2319\n",
            "[Epoch 21/25 Iter   250/12500]: train_loss = 0.2015\n",
            "[Epoch 21/25 Iter   500/12500]: train_loss = 0.2232\n",
            "[Epoch 21/25 Iter   750/12500]: train_loss = 0.2105\n",
            "[Epoch 21/25 Iter  1000/12500]: train_loss = 0.2253\n",
            "[Epoch 21/25 Iter  1250/12500]: train_loss = 0.2412\n",
            "[Epoch 21/25 Iter  1500/12500]: train_loss = 0.2043\n",
            "[Epoch 21/25 Iter  1750/12500]: train_loss = 0.2291\n",
            "[Epoch 21/25 Iter  2000/12500]: train_loss = 0.2087\n",
            "[Epoch 21/25 Iter  2250/12500]: train_loss = 0.1889\n",
            "[Epoch 21/25 Iter  2500/12500]: train_loss = 0.1938\n",
            "[Epoch 21/25 Iter  2750/12500]: train_loss = 0.1697\n",
            "[Epoch 21/25 Iter  3000/12500]: train_loss = 0.1787\n",
            "[Epoch 21/25 Iter  3250/12500]: train_loss = 0.1855\n",
            "[Epoch 21/25 Iter  3500/12500]: train_loss = 0.2213\n",
            "[Epoch 21/25 Iter  3750/12500]: train_loss = 0.2530\n",
            "[Epoch 21/25 Iter  4000/12500]: train_loss = 0.1952\n",
            "[Epoch 21/25 Iter  4250/12500]: train_loss = 0.2164\n",
            "[Epoch 21/25 Iter  4500/12500]: train_loss = 0.2297\n",
            "[Epoch 21/25 Iter  4750/12500]: train_loss = 0.2373\n",
            "[Epoch 21/25 Iter  5000/12500]: train_loss = 0.1989\n",
            "[Epoch 21/25 Iter  5250/12500]: train_loss = 0.1896\n",
            "[Epoch 21/25 Iter  5500/12500]: train_loss = 0.2091\n",
            "[Epoch 21/25 Iter  5750/12500]: train_loss = 0.2048\n",
            "[Epoch 21/25 Iter  6000/12500]: train_loss = 0.2235\n",
            "[Epoch 21/25 Iter  6250/12500]: train_loss = 0.1904\n",
            "[Epoch 21/25 Iter  6500/12500]: train_loss = 0.2033\n",
            "[Epoch 21/25 Iter  6750/12500]: train_loss = 0.2043\n",
            "[Epoch 21/25 Iter  7000/12500]: train_loss = 0.2231\n",
            "[Epoch 21/25 Iter  7250/12500]: train_loss = 0.2022\n",
            "[Epoch 21/25 Iter  7500/12500]: train_loss = 0.2163\n",
            "[Epoch 21/25 Iter  7750/12500]: train_loss = 0.2041\n",
            "[Epoch 21/25 Iter  8000/12500]: train_loss = 0.1968\n",
            "[Epoch 21/25 Iter  8250/12500]: train_loss = 0.1896\n",
            "[Epoch 21/25 Iter  8500/12500]: train_loss = 0.2038\n",
            "[Epoch 21/25 Iter  8750/12500]: train_loss = 0.2190\n",
            "[Epoch 21/25 Iter  9000/12500]: train_loss = 0.1644\n",
            "[Epoch 21/25 Iter  9250/12500]: train_loss = 0.1977\n",
            "[Epoch 21/25 Iter  9500/12500]: train_loss = 0.2312\n",
            "[Epoch 21/25 Iter  9750/12500]: train_loss = 0.2060\n",
            "[Epoch 21/25 Iter 10000/12500]: train_loss = 0.1974\n",
            "[Epoch 21/25 Iter 10250/12500]: train_loss = 0.2317\n",
            "[Epoch 21/25 Iter 10500/12500]: train_loss = 0.1942\n",
            "[Epoch 21/25 Iter 10750/12500]: train_loss = 0.2353\n",
            "[Epoch 21/25 Iter 11000/12500]: train_loss = 0.2122\n",
            "[Epoch 21/25 Iter 11250/12500]: train_loss = 0.2145\n",
            "[Epoch 21/25 Iter 11500/12500]: train_loss = 0.2180\n",
            "[Epoch 21/25 Iter 11750/12500]: train_loss = 0.2259\n",
            "[Epoch 21/25 Iter 12000/12500]: train_loss = 0.2007\n",
            "[Epoch 21/25 Iter 12250/12500]: train_loss = 0.2247\n",
            "[Epoch 21/25 Iter 12500/12500]: train_loss = 0.2293\n",
            "[Epoch 22/25 Iter   250/12500]: train_loss = 0.1833\n",
            "[Epoch 22/25 Iter   500/12500]: train_loss = 0.1853\n",
            "[Epoch 22/25 Iter   750/12500]: train_loss = 0.2010\n",
            "[Epoch 22/25 Iter  1000/12500]: train_loss = 0.2205\n",
            "[Epoch 22/25 Iter  1250/12500]: train_loss = 0.2059\n",
            "[Epoch 22/25 Iter  1500/12500]: train_loss = 0.1935\n",
            "[Epoch 22/25 Iter  1750/12500]: train_loss = 0.1906\n",
            "[Epoch 22/25 Iter  2000/12500]: train_loss = 0.2222\n",
            "[Epoch 22/25 Iter  2250/12500]: train_loss = 0.1975\n",
            "[Epoch 22/25 Iter  2500/12500]: train_loss = 0.1995\n",
            "[Epoch 22/25 Iter  2750/12500]: train_loss = 0.2154\n",
            "[Epoch 22/25 Iter  3000/12500]: train_loss = 0.2111\n",
            "[Epoch 22/25 Iter  3250/12500]: train_loss = 0.2029\n",
            "[Epoch 22/25 Iter  3500/12500]: train_loss = 0.1853\n",
            "[Epoch 22/25 Iter  3750/12500]: train_loss = 0.2042\n",
            "[Epoch 22/25 Iter  4000/12500]: train_loss = 0.1899\n",
            "[Epoch 22/25 Iter  4250/12500]: train_loss = 0.2291\n",
            "[Epoch 22/25 Iter  4500/12500]: train_loss = 0.2173\n",
            "[Epoch 22/25 Iter  4750/12500]: train_loss = 0.1902\n",
            "[Epoch 22/25 Iter  5000/12500]: train_loss = 0.1835\n",
            "[Epoch 22/25 Iter  5250/12500]: train_loss = 0.2184\n",
            "[Epoch 22/25 Iter  5500/12500]: train_loss = 0.2375\n",
            "[Epoch 22/25 Iter  5750/12500]: train_loss = 0.2102\n",
            "[Epoch 22/25 Iter  6000/12500]: train_loss = 0.2144\n",
            "[Epoch 22/25 Iter  6250/12500]: train_loss = 0.2001\n",
            "[Epoch 22/25 Iter  6500/12500]: train_loss = 0.1959\n",
            "[Epoch 22/25 Iter  6750/12500]: train_loss = 0.2212\n",
            "[Epoch 22/25 Iter  7000/12500]: train_loss = 0.2116\n",
            "[Epoch 22/25 Iter  7250/12500]: train_loss = 0.2110\n",
            "[Epoch 22/25 Iter  7500/12500]: train_loss = 0.2175\n",
            "[Epoch 22/25 Iter  7750/12500]: train_loss = 0.1711\n",
            "[Epoch 22/25 Iter  8000/12500]: train_loss = 0.1921\n",
            "[Epoch 22/25 Iter  8250/12500]: train_loss = 0.2076\n",
            "[Epoch 22/25 Iter  8500/12500]: train_loss = 0.1984\n",
            "[Epoch 22/25 Iter  8750/12500]: train_loss = 0.1645\n",
            "[Epoch 22/25 Iter  9000/12500]: train_loss = 0.1926\n",
            "[Epoch 22/25 Iter  9250/12500]: train_loss = 0.2103\n",
            "[Epoch 22/25 Iter  9500/12500]: train_loss = 0.2186\n",
            "[Epoch 22/25 Iter  9750/12500]: train_loss = 0.1945\n",
            "[Epoch 22/25 Iter 10000/12500]: train_loss = 0.1949\n",
            "[Epoch 22/25 Iter 10250/12500]: train_loss = 0.2346\n",
            "[Epoch 22/25 Iter 10500/12500]: train_loss = 0.1926\n",
            "[Epoch 22/25 Iter 10750/12500]: train_loss = 0.2384\n",
            "[Epoch 22/25 Iter 11000/12500]: train_loss = 0.2065\n",
            "[Epoch 22/25 Iter 11250/12500]: train_loss = 0.2100\n",
            "[Epoch 22/25 Iter 11500/12500]: train_loss = 0.2153\n",
            "[Epoch 22/25 Iter 11750/12500]: train_loss = 0.1841\n",
            "[Epoch 22/25 Iter 12000/12500]: train_loss = 0.2218\n",
            "[Epoch 22/25 Iter 12250/12500]: train_loss = 0.2140\n",
            "[Epoch 22/25 Iter 12500/12500]: train_loss = 0.1924\n",
            "[Epoch 23/25 Iter   250/12500]: train_loss = 0.1845\n",
            "[Epoch 23/25 Iter   500/12500]: train_loss = 0.1969\n",
            "[Epoch 23/25 Iter   750/12500]: train_loss = 0.2097\n",
            "[Epoch 23/25 Iter  1000/12500]: train_loss = 0.1400\n",
            "[Epoch 23/25 Iter  1250/12500]: train_loss = 0.2070\n",
            "[Epoch 23/25 Iter  1500/12500]: train_loss = 0.1951\n",
            "[Epoch 23/25 Iter  1750/12500]: train_loss = 0.1951\n",
            "[Epoch 23/25 Iter  2000/12500]: train_loss = 0.2071\n",
            "[Epoch 23/25 Iter  2250/12500]: train_loss = 0.1998\n",
            "[Epoch 23/25 Iter  2500/12500]: train_loss = 0.1947\n",
            "[Epoch 23/25 Iter  2750/12500]: train_loss = 0.1991\n",
            "[Epoch 23/25 Iter  3000/12500]: train_loss = 0.1605\n",
            "[Epoch 23/25 Iter  3250/12500]: train_loss = 0.2105\n",
            "[Epoch 23/25 Iter  3500/12500]: train_loss = 0.2317\n",
            "[Epoch 23/25 Iter  3750/12500]: train_loss = 0.1853\n",
            "[Epoch 23/25 Iter  4000/12500]: train_loss = 0.2174\n",
            "[Epoch 23/25 Iter  4250/12500]: train_loss = 0.1970\n",
            "[Epoch 23/25 Iter  4500/12500]: train_loss = 0.1991\n",
            "[Epoch 23/25 Iter  4750/12500]: train_loss = 0.2103\n",
            "[Epoch 23/25 Iter  5000/12500]: train_loss = 0.1807\n",
            "[Epoch 23/25 Iter  5250/12500]: train_loss = 0.1924\n",
            "[Epoch 23/25 Iter  5500/12500]: train_loss = 0.2085\n",
            "[Epoch 23/25 Iter  5750/12500]: train_loss = 0.1964\n",
            "[Epoch 23/25 Iter  6000/12500]: train_loss = 0.2204\n",
            "[Epoch 23/25 Iter  6250/12500]: train_loss = 0.1984\n",
            "[Epoch 23/25 Iter  6500/12500]: train_loss = 0.1532\n",
            "[Epoch 23/25 Iter  6750/12500]: train_loss = 0.1672\n",
            "[Epoch 23/25 Iter  7000/12500]: train_loss = 0.1899\n",
            "[Epoch 23/25 Iter  7250/12500]: train_loss = 0.2220\n",
            "[Epoch 23/25 Iter  7500/12500]: train_loss = 0.2009\n",
            "[Epoch 23/25 Iter  7750/12500]: train_loss = 0.1982\n",
            "[Epoch 23/25 Iter  8000/12500]: train_loss = 0.2127\n",
            "[Epoch 23/25 Iter  8250/12500]: train_loss = 0.1959\n",
            "[Epoch 23/25 Iter  8500/12500]: train_loss = 0.2098\n",
            "[Epoch 23/25 Iter  8750/12500]: train_loss = 0.2010\n",
            "[Epoch 23/25 Iter  9000/12500]: train_loss = 0.2073\n",
            "[Epoch 23/25 Iter  9250/12500]: train_loss = 0.2181\n",
            "[Epoch 23/25 Iter  9500/12500]: train_loss = 0.1797\n",
            "[Epoch 23/25 Iter  9750/12500]: train_loss = 0.2139\n",
            "[Epoch 23/25 Iter 10000/12500]: train_loss = 0.1903\n",
            "[Epoch 23/25 Iter 10250/12500]: train_loss = 0.1941\n",
            "[Epoch 23/25 Iter 10500/12500]: train_loss = 0.1878\n",
            "[Epoch 23/25 Iter 10750/12500]: train_loss = 0.2104\n",
            "[Epoch 23/25 Iter 11000/12500]: train_loss = 0.1889\n",
            "[Epoch 23/25 Iter 11250/12500]: train_loss = 0.2227\n",
            "[Epoch 23/25 Iter 11500/12500]: train_loss = 0.1747\n",
            "[Epoch 23/25 Iter 11750/12500]: train_loss = 0.1939\n",
            "[Epoch 23/25 Iter 12000/12500]: train_loss = 0.2087\n",
            "[Epoch 23/25 Iter 12250/12500]: train_loss = 0.2149\n",
            "[Epoch 23/25 Iter 12500/12500]: train_loss = 0.2037\n",
            "[Epoch 24/25 Iter   250/12500]: train_loss = 0.1916\n",
            "[Epoch 24/25 Iter   500/12500]: train_loss = 0.2295\n",
            "[Epoch 24/25 Iter   750/12500]: train_loss = 0.2052\n",
            "[Epoch 24/25 Iter  1000/12500]: train_loss = 0.2215\n",
            "[Epoch 24/25 Iter  1250/12500]: train_loss = 0.2043\n",
            "[Epoch 24/25 Iter  1500/12500]: train_loss = 0.1818\n",
            "[Epoch 24/25 Iter  1750/12500]: train_loss = 0.2110\n",
            "[Epoch 24/25 Iter  2000/12500]: train_loss = 0.2284\n",
            "[Epoch 24/25 Iter  2250/12500]: train_loss = 0.1755\n",
            "[Epoch 24/25 Iter  2500/12500]: train_loss = 0.1799\n",
            "[Epoch 24/25 Iter  2750/12500]: train_loss = 0.2205\n",
            "[Epoch 24/25 Iter  3000/12500]: train_loss = 0.1927\n",
            "[Epoch 24/25 Iter  3250/12500]: train_loss = 0.1998\n",
            "[Epoch 24/25 Iter  3500/12500]: train_loss = 0.2181\n",
            "[Epoch 24/25 Iter  3750/12500]: train_loss = 0.1975\n",
            "[Epoch 24/25 Iter  4000/12500]: train_loss = 0.1710\n",
            "[Epoch 24/25 Iter  4250/12500]: train_loss = 0.2300\n",
            "[Epoch 24/25 Iter  4500/12500]: train_loss = 0.2040\n",
            "[Epoch 24/25 Iter  4750/12500]: train_loss = 0.1776\n",
            "[Epoch 24/25 Iter  5000/12500]: train_loss = 0.1897\n",
            "[Epoch 24/25 Iter  5250/12500]: train_loss = 0.1721\n",
            "[Epoch 24/25 Iter  5500/12500]: train_loss = 0.1987\n",
            "[Epoch 24/25 Iter  5750/12500]: train_loss = 0.2205\n",
            "[Epoch 24/25 Iter  6000/12500]: train_loss = 0.1636\n",
            "[Epoch 24/25 Iter  6250/12500]: train_loss = 0.1926\n",
            "[Epoch 24/25 Iter  6500/12500]: train_loss = 0.1851\n",
            "[Epoch 24/25 Iter  6750/12500]: train_loss = 0.2009\n",
            "[Epoch 24/25 Iter  7000/12500]: train_loss = 0.1889\n",
            "[Epoch 24/25 Iter  7250/12500]: train_loss = 0.1849\n",
            "[Epoch 24/25 Iter  7500/12500]: train_loss = 0.1949\n",
            "[Epoch 24/25 Iter  7750/12500]: train_loss = 0.2126\n",
            "[Epoch 24/25 Iter  8000/12500]: train_loss = 0.1911\n",
            "[Epoch 24/25 Iter  8250/12500]: train_loss = 0.2006\n",
            "[Epoch 24/25 Iter  8500/12500]: train_loss = 0.1936\n",
            "[Epoch 24/25 Iter  8750/12500]: train_loss = 0.1644\n",
            "[Epoch 24/25 Iter  9000/12500]: train_loss = 0.1647\n",
            "[Epoch 24/25 Iter  9250/12500]: train_loss = 0.1786\n",
            "[Epoch 24/25 Iter  9500/12500]: train_loss = 0.2056\n",
            "[Epoch 24/25 Iter  9750/12500]: train_loss = 0.2058\n",
            "[Epoch 24/25 Iter 10000/12500]: train_loss = 0.2118\n",
            "[Epoch 24/25 Iter 10250/12500]: train_loss = 0.1946\n",
            "[Epoch 24/25 Iter 10500/12500]: train_loss = 0.1657\n",
            "[Epoch 24/25 Iter 10750/12500]: train_loss = 0.1874\n",
            "[Epoch 24/25 Iter 11000/12500]: train_loss = 0.2006\n",
            "[Epoch 24/25 Iter 11250/12500]: train_loss = 0.2075\n",
            "[Epoch 24/25 Iter 11500/12500]: train_loss = 0.2351\n",
            "[Epoch 24/25 Iter 11750/12500]: train_loss = 0.1898\n",
            "[Epoch 24/25 Iter 12000/12500]: train_loss = 0.2158\n",
            "[Epoch 24/25 Iter 12250/12500]: train_loss = 0.1520\n",
            "[Epoch 24/25 Iter 12500/12500]: train_loss = 0.1900\n",
            "[Epoch 25/25 Iter   250/12500]: train_loss = 0.2108\n",
            "[Epoch 25/25 Iter   500/12500]: train_loss = 0.1894\n",
            "[Epoch 25/25 Iter   750/12500]: train_loss = 0.1807\n",
            "[Epoch 25/25 Iter  1000/12500]: train_loss = 0.1981\n",
            "[Epoch 25/25 Iter  1250/12500]: train_loss = 0.2150\n",
            "[Epoch 25/25 Iter  1500/12500]: train_loss = 0.1698\n",
            "[Epoch 25/25 Iter  1750/12500]: train_loss = 0.1823\n",
            "[Epoch 25/25 Iter  2000/12500]: train_loss = 0.2052\n",
            "[Epoch 25/25 Iter  2250/12500]: train_loss = 0.1658\n",
            "[Epoch 25/25 Iter  2500/12500]: train_loss = 0.1664\n",
            "[Epoch 25/25 Iter  2750/12500]: train_loss = 0.2066\n",
            "[Epoch 25/25 Iter  3000/12500]: train_loss = 0.1713\n",
            "[Epoch 25/25 Iter  3250/12500]: train_loss = 0.2216\n",
            "[Epoch 25/25 Iter  3500/12500]: train_loss = 0.1848\n",
            "[Epoch 25/25 Iter  3750/12500]: train_loss = 0.1954\n",
            "[Epoch 25/25 Iter  4000/12500]: train_loss = 0.1752\n",
            "[Epoch 25/25 Iter  4250/12500]: train_loss = 0.1734\n",
            "[Epoch 25/25 Iter  4500/12500]: train_loss = 0.1933\n",
            "[Epoch 25/25 Iter  4750/12500]: train_loss = 0.2008\n",
            "[Epoch 25/25 Iter  5000/12500]: train_loss = 0.2075\n",
            "[Epoch 25/25 Iter  5250/12500]: train_loss = 0.1568\n",
            "[Epoch 25/25 Iter  5500/12500]: train_loss = 0.2455\n",
            "[Epoch 25/25 Iter  5750/12500]: train_loss = 0.1670\n",
            "[Epoch 25/25 Iter  6000/12500]: train_loss = 0.1932\n",
            "[Epoch 25/25 Iter  6250/12500]: train_loss = 0.1539\n",
            "[Epoch 25/25 Iter  6500/12500]: train_loss = 0.1778\n",
            "[Epoch 25/25 Iter  6750/12500]: train_loss = 0.1799\n",
            "[Epoch 25/25 Iter  7000/12500]: train_loss = 0.1848\n",
            "[Epoch 25/25 Iter  7250/12500]: train_loss = 0.2123\n",
            "[Epoch 25/25 Iter  7500/12500]: train_loss = 0.1923\n",
            "[Epoch 25/25 Iter  7750/12500]: train_loss = 0.1915\n",
            "[Epoch 25/25 Iter  8000/12500]: train_loss = 0.1862\n",
            "[Epoch 25/25 Iter  8250/12500]: train_loss = 0.1758\n",
            "[Epoch 25/25 Iter  8500/12500]: train_loss = 0.2029\n",
            "[Epoch 25/25 Iter  8750/12500]: train_loss = 0.1725\n",
            "[Epoch 25/25 Iter  9000/12500]: train_loss = 0.2095\n",
            "[Epoch 25/25 Iter  9250/12500]: train_loss = 0.1955\n",
            "[Epoch 25/25 Iter  9500/12500]: train_loss = 0.1708\n",
            "[Epoch 25/25 Iter  9750/12500]: train_loss = 0.2316\n",
            "[Epoch 25/25 Iter 10000/12500]: train_loss = 0.1877\n",
            "[Epoch 25/25 Iter 10250/12500]: train_loss = 0.1771\n",
            "[Epoch 25/25 Iter 10500/12500]: train_loss = 0.2066\n",
            "[Epoch 25/25 Iter 10750/12500]: train_loss = 0.2240\n",
            "[Epoch 25/25 Iter 11000/12500]: train_loss = 0.1983\n",
            "[Epoch 25/25 Iter 11250/12500]: train_loss = 0.1981\n",
            "[Epoch 25/25 Iter 11500/12500]: train_loss = 0.1774\n",
            "[Epoch 25/25 Iter 11750/12500]: train_loss = 0.1711\n",
            "[Epoch 25/25 Iter 12000/12500]: train_loss = 0.2183\n",
            "[Epoch 25/25 Iter 12250/12500]: train_loss = 0.2300\n",
            "[Epoch 25/25 Iter 12500/12500]: train_loss = 0.1988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5SCKLgwprty",
        "outputId": "975afd1a-3035-459c-a830-fd3e4d15955a"
      },
      "source": [
        "evaluate(model, testloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 91.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDM0lXg5r-Y"
      },
      "source": [
        "## 4. Convoluted Model II\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQjDibzm5962"
      },
      "source": [
        "class cnn_model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        channel_size = [3, 32, 32]\n",
        "        layer_size = [32 * 7 * 7,128, 10]\n",
        "        p = 0.25\n",
        "        \n",
        "        # define blocks\n",
        "        self.conv_block = nn.ModuleList(\n",
        "            build_block_cnn(in_channel, out_channel) for(in_channel, out_channel) in zip(channel_size[0:-1], channel_size[1:])\n",
        "        )\n",
        "        \n",
        "        # define Fully Connected Layers\n",
        "        self.fc = nn.ModuleList(\n",
        "            [nn.Linear(in_channel, out_channel) for(in_channel, out_channel) in zip(layer_size[0:-1], layer_size[1:])]\n",
        "        )\n",
        "\n",
        "        # Define Dropout\n",
        "        self.dropout = nn.ModuleList(\n",
        "            [nn.Dropout(p) for i in range(len(layer_size) - 2)]\n",
        "        )\n",
        "\n",
        "        # Define Batch Normalization\n",
        "        self.bn = nn.ModuleList(\n",
        "            [nn.BatchNorm2d(i) for i in channel_size[:-1]]\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for (bn, block) in zip(self.bn, self.conv_block):\n",
        "          x = F.max_pool2d(block(bn(x)), kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        for (dropout, fc) in zip(self.dropout, self.fc[:-1]):\n",
        "          x = dropout(torch.relu(fc(x)))\n",
        "        \n",
        "        x = torch.relu(self.fc[1](x))\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW6_Lz9Q6xO4",
        "outputId": "c06f642f-69b9-480a-d88e-f07f67e038fa"
      },
      "source": [
        "model = cnn_model2()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnn_model2(\n",
            "  (conv_block): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=1568, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            "  (dropout): ModuleList(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (bn): ModuleList(\n",
            "    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG7uYDnR6zwx",
        "outputId": "446542ec-6433-4206-d0d7-4aafe9fe04a9"
      },
      "source": [
        "summary(model, (3, 28, 28), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 28, 28]               6\n",
            "            Conv2d-2           [-1, 32, 28, 28]             896\n",
            "              ReLU-3           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-4           [-1, 32, 14, 14]              64\n",
            "            Conv2d-5           [-1, 32, 14, 14]           9,248\n",
            "              ReLU-6           [-1, 32, 14, 14]               0\n",
            "            Linear-7                  [-1, 128]         200,832\n",
            "           Dropout-8                  [-1, 128]               0\n",
            "            Linear-9                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 212,336\n",
            "Trainable params: 212,336\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.55\n",
            "Params size (MB): 0.81\n",
            "Estimated Total Size (MB): 1.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y5Epdgd62Im",
        "outputId": "80faba2a-d25a-4a20-8382-624ed6c614df"
      },
      "source": [
        "history2 = train(model, trainloader, num_epochs = 10, loss_iter = 50, lr = 0.001, momentum = 0.9, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/10 Iter   250/12500]: train_loss = 1.7692\n",
            "[Epoch  1/10 Iter   500/12500]: train_loss = 0.9567\n",
            "[Epoch  1/10 Iter   750/12500]: train_loss = 0.8299\n",
            "[Epoch  1/10 Iter  1000/12500]: train_loss = 0.7366\n",
            "[Epoch  1/10 Iter  1250/12500]: train_loss = 0.7991\n",
            "[Epoch  1/10 Iter  1500/12500]: train_loss = 0.6725\n",
            "[Epoch  1/10 Iter  1750/12500]: train_loss = 0.6571\n",
            "[Epoch  1/10 Iter  2000/12500]: train_loss = 0.6358\n",
            "[Epoch  1/10 Iter  2250/12500]: train_loss = 0.7141\n",
            "[Epoch  1/10 Iter  2500/12500]: train_loss = 0.6455\n",
            "[Epoch  1/10 Iter  2750/12500]: train_loss = 0.5972\n",
            "[Epoch  1/10 Iter  3000/12500]: train_loss = 0.5826\n",
            "[Epoch  1/10 Iter  3250/12500]: train_loss = 0.6375\n",
            "[Epoch  1/10 Iter  3500/12500]: train_loss = 0.6026\n",
            "[Epoch  1/10 Iter  3750/12500]: train_loss = 0.5795\n",
            "[Epoch  1/10 Iter  4000/12500]: train_loss = 0.5443\n",
            "[Epoch  1/10 Iter  4250/12500]: train_loss = 0.5564\n",
            "[Epoch  1/10 Iter  4500/12500]: train_loss = 0.6172\n",
            "[Epoch  1/10 Iter  4750/12500]: train_loss = 0.5335\n",
            "[Epoch  1/10 Iter  5000/12500]: train_loss = 0.5540\n",
            "[Epoch  1/10 Iter  5250/12500]: train_loss = 0.6012\n",
            "[Epoch  1/10 Iter  5500/12500]: train_loss = 0.5757\n",
            "[Epoch  1/10 Iter  5750/12500]: train_loss = 0.5898\n",
            "[Epoch  1/10 Iter  6000/12500]: train_loss = 0.5541\n",
            "[Epoch  1/10 Iter  6250/12500]: train_loss = 0.5450\n",
            "[Epoch  1/10 Iter  6500/12500]: train_loss = 0.5449\n",
            "[Epoch  1/10 Iter  6750/12500]: train_loss = 0.5321\n",
            "[Epoch  1/10 Iter  7000/12500]: train_loss = 0.5311\n",
            "[Epoch  1/10 Iter  7250/12500]: train_loss = 0.5433\n",
            "[Epoch  1/10 Iter  7500/12500]: train_loss = 0.5186\n",
            "[Epoch  1/10 Iter  7750/12500]: train_loss = 0.5265\n",
            "[Epoch  1/10 Iter  8000/12500]: train_loss = 0.5343\n",
            "[Epoch  1/10 Iter  8250/12500]: train_loss = 0.4774\n",
            "[Epoch  1/10 Iter  8500/12500]: train_loss = 0.5436\n",
            "[Epoch  1/10 Iter  8750/12500]: train_loss = 0.5116\n",
            "[Epoch  1/10 Iter  9000/12500]: train_loss = 0.4761\n",
            "[Epoch  1/10 Iter  9250/12500]: train_loss = 0.5172\n",
            "[Epoch  1/10 Iter  9500/12500]: train_loss = 0.4715\n",
            "[Epoch  1/10 Iter  9750/12500]: train_loss = 0.5302\n",
            "[Epoch  1/10 Iter 10000/12500]: train_loss = 0.5205\n",
            "[Epoch  1/10 Iter 10250/12500]: train_loss = 0.5127\n",
            "[Epoch  1/10 Iter 10500/12500]: train_loss = 0.4639\n",
            "[Epoch  1/10 Iter 10750/12500]: train_loss = 0.4841\n",
            "[Epoch  1/10 Iter 11000/12500]: train_loss = 0.5353\n",
            "[Epoch  1/10 Iter 11250/12500]: train_loss = 0.5022\n",
            "[Epoch  1/10 Iter 11500/12500]: train_loss = 0.4848\n",
            "[Epoch  1/10 Iter 11750/12500]: train_loss = 0.4867\n",
            "[Epoch  1/10 Iter 12000/12500]: train_loss = 0.5028\n",
            "[Epoch  1/10 Iter 12250/12500]: train_loss = 0.4592\n",
            "[Epoch  1/10 Iter 12500/12500]: train_loss = 0.4626\n",
            "[Epoch  2/10 Iter   250/12500]: train_loss = 0.4462\n",
            "[Epoch  2/10 Iter   500/12500]: train_loss = 0.4498\n",
            "[Epoch  2/10 Iter   750/12500]: train_loss = 0.4693\n",
            "[Epoch  2/10 Iter  1000/12500]: train_loss = 0.4597\n",
            "[Epoch  2/10 Iter  1250/12500]: train_loss = 0.4703\n",
            "[Epoch  2/10 Iter  1500/12500]: train_loss = 0.4702\n",
            "[Epoch  2/10 Iter  1750/12500]: train_loss = 0.4317\n",
            "[Epoch  2/10 Iter  2000/12500]: train_loss = 0.4266\n",
            "[Epoch  2/10 Iter  2250/12500]: train_loss = 0.4394\n",
            "[Epoch  2/10 Iter  2500/12500]: train_loss = 0.4502\n",
            "[Epoch  2/10 Iter  2750/12500]: train_loss = 0.4371\n",
            "[Epoch  2/10 Iter  3000/12500]: train_loss = 0.4633\n",
            "[Epoch  2/10 Iter  3250/12500]: train_loss = 0.4644\n",
            "[Epoch  2/10 Iter  3500/12500]: train_loss = 0.4546\n",
            "[Epoch  2/10 Iter  3750/12500]: train_loss = 0.4375\n",
            "[Epoch  2/10 Iter  4000/12500]: train_loss = 0.4105\n",
            "[Epoch  2/10 Iter  4250/12500]: train_loss = 0.4881\n",
            "[Epoch  2/10 Iter  4500/12500]: train_loss = 0.4748\n",
            "[Epoch  2/10 Iter  4750/12500]: train_loss = 0.3889\n",
            "[Epoch  2/10 Iter  5000/12500]: train_loss = 0.4576\n",
            "[Epoch  2/10 Iter  5250/12500]: train_loss = 0.3532\n",
            "[Epoch  2/10 Iter  5500/12500]: train_loss = 0.4404\n",
            "[Epoch  2/10 Iter  5750/12500]: train_loss = 0.4828\n",
            "[Epoch  2/10 Iter  6000/12500]: train_loss = 0.4896\n",
            "[Epoch  2/10 Iter  6250/12500]: train_loss = 0.4209\n",
            "[Epoch  2/10 Iter  6500/12500]: train_loss = 0.4425\n",
            "[Epoch  2/10 Iter  6750/12500]: train_loss = 0.5208\n",
            "[Epoch  2/10 Iter  7000/12500]: train_loss = 0.4351\n",
            "[Epoch  2/10 Iter  7250/12500]: train_loss = 0.4328\n",
            "[Epoch  2/10 Iter  7500/12500]: train_loss = 0.4421\n",
            "[Epoch  2/10 Iter  7750/12500]: train_loss = 0.4477\n",
            "[Epoch  2/10 Iter  8000/12500]: train_loss = 0.3801\n",
            "[Epoch  2/10 Iter  8250/12500]: train_loss = 0.4459\n",
            "[Epoch  2/10 Iter  8500/12500]: train_loss = 0.4541\n",
            "[Epoch  2/10 Iter  8750/12500]: train_loss = 0.3941\n",
            "[Epoch  2/10 Iter  9000/12500]: train_loss = 0.4582\n",
            "[Epoch  2/10 Iter  9250/12500]: train_loss = 0.4321\n",
            "[Epoch  2/10 Iter  9500/12500]: train_loss = 0.4284\n",
            "[Epoch  2/10 Iter  9750/12500]: train_loss = 0.3834\n",
            "[Epoch  2/10 Iter 10000/12500]: train_loss = 0.3907\n",
            "[Epoch  2/10 Iter 10250/12500]: train_loss = 0.4072\n",
            "[Epoch  2/10 Iter 10500/12500]: train_loss = 0.4304\n",
            "[Epoch  2/10 Iter 10750/12500]: train_loss = 0.4265\n",
            "[Epoch  2/10 Iter 11000/12500]: train_loss = 0.3617\n",
            "[Epoch  2/10 Iter 11250/12500]: train_loss = 0.4243\n",
            "[Epoch  2/10 Iter 11500/12500]: train_loss = 0.4055\n",
            "[Epoch  2/10 Iter 11750/12500]: train_loss = 0.3746\n",
            "[Epoch  2/10 Iter 12000/12500]: train_loss = 0.4451\n",
            "[Epoch  2/10 Iter 12250/12500]: train_loss = 0.4142\n",
            "[Epoch  2/10 Iter 12500/12500]: train_loss = 0.3782\n",
            "[Epoch  3/10 Iter   250/12500]: train_loss = 0.4279\n",
            "[Epoch  3/10 Iter   500/12500]: train_loss = 0.3913\n",
            "[Epoch  3/10 Iter   750/12500]: train_loss = 0.3920\n",
            "[Epoch  3/10 Iter  1000/12500]: train_loss = 0.3745\n",
            "[Epoch  3/10 Iter  1250/12500]: train_loss = 0.4155\n",
            "[Epoch  3/10 Iter  1500/12500]: train_loss = 0.3842\n",
            "[Epoch  3/10 Iter  1750/12500]: train_loss = 0.3759\n",
            "[Epoch  3/10 Iter  2000/12500]: train_loss = 0.4265\n",
            "[Epoch  3/10 Iter  2250/12500]: train_loss = 0.3971\n",
            "[Epoch  3/10 Iter  2500/12500]: train_loss = 0.4098\n",
            "[Epoch  3/10 Iter  2750/12500]: train_loss = 0.3978\n",
            "[Epoch  3/10 Iter  3000/12500]: train_loss = 0.3699\n",
            "[Epoch  3/10 Iter  3250/12500]: train_loss = 0.4129\n",
            "[Epoch  3/10 Iter  3500/12500]: train_loss = 0.4460\n",
            "[Epoch  3/10 Iter  3750/12500]: train_loss = 0.3823\n",
            "[Epoch  3/10 Iter  4000/12500]: train_loss = 0.3960\n",
            "[Epoch  3/10 Iter  4250/12500]: train_loss = 0.3682\n",
            "[Epoch  3/10 Iter  4500/12500]: train_loss = 0.3906\n",
            "[Epoch  3/10 Iter  4750/12500]: train_loss = 0.3545\n",
            "[Epoch  3/10 Iter  5000/12500]: train_loss = 0.4050\n",
            "[Epoch  3/10 Iter  5250/12500]: train_loss = 0.4185\n",
            "[Epoch  3/10 Iter  5500/12500]: train_loss = 0.3742\n",
            "[Epoch  3/10 Iter  5750/12500]: train_loss = 0.3871\n",
            "[Epoch  3/10 Iter  6000/12500]: train_loss = 0.3970\n",
            "[Epoch  3/10 Iter  6250/12500]: train_loss = 0.3598\n",
            "[Epoch  3/10 Iter  6500/12500]: train_loss = 0.4098\n",
            "[Epoch  3/10 Iter  6750/12500]: train_loss = 0.3893\n",
            "[Epoch  3/10 Iter  7000/12500]: train_loss = 0.4119\n",
            "[Epoch  3/10 Iter  7250/12500]: train_loss = 0.3763\n",
            "[Epoch  3/10 Iter  7500/12500]: train_loss = 0.3991\n",
            "[Epoch  3/10 Iter  7750/12500]: train_loss = 0.3617\n",
            "[Epoch  3/10 Iter  8000/12500]: train_loss = 0.4187\n",
            "[Epoch  3/10 Iter  8250/12500]: train_loss = 0.3895\n",
            "[Epoch  3/10 Iter  8500/12500]: train_loss = 0.3818\n",
            "[Epoch  3/10 Iter  8750/12500]: train_loss = 0.3925\n",
            "[Epoch  3/10 Iter  9000/12500]: train_loss = 0.3772\n",
            "[Epoch  3/10 Iter  9250/12500]: train_loss = 0.3641\n",
            "[Epoch  3/10 Iter  9500/12500]: train_loss = 0.3997\n",
            "[Epoch  3/10 Iter  9750/12500]: train_loss = 0.3636\n",
            "[Epoch  3/10 Iter 10000/12500]: train_loss = 0.4185\n",
            "[Epoch  3/10 Iter 10250/12500]: train_loss = 0.3574\n",
            "[Epoch  3/10 Iter 10500/12500]: train_loss = 0.3407\n",
            "[Epoch  3/10 Iter 10750/12500]: train_loss = 0.3998\n",
            "[Epoch  3/10 Iter 11000/12500]: train_loss = 0.3744\n",
            "[Epoch  3/10 Iter 11250/12500]: train_loss = 0.3737\n",
            "[Epoch  3/10 Iter 11500/12500]: train_loss = 0.3649\n",
            "[Epoch  3/10 Iter 11750/12500]: train_loss = 0.3578\n",
            "[Epoch  3/10 Iter 12000/12500]: train_loss = 0.3986\n",
            "[Epoch  3/10 Iter 12250/12500]: train_loss = 0.3492\n",
            "[Epoch  3/10 Iter 12500/12500]: train_loss = 0.3429\n",
            "[Epoch  4/10 Iter   250/12500]: train_loss = 0.3704\n",
            "[Epoch  4/10 Iter   500/12500]: train_loss = 0.3528\n",
            "[Epoch  4/10 Iter   750/12500]: train_loss = 0.3634\n",
            "[Epoch  4/10 Iter  1000/12500]: train_loss = 0.3345\n",
            "[Epoch  4/10 Iter  1250/12500]: train_loss = 0.4135\n",
            "[Epoch  4/10 Iter  1500/12500]: train_loss = 0.3783\n",
            "[Epoch  4/10 Iter  1750/12500]: train_loss = 0.3681\n",
            "[Epoch  4/10 Iter  2000/12500]: train_loss = 0.3458\n",
            "[Epoch  4/10 Iter  2250/12500]: train_loss = 0.3606\n",
            "[Epoch  4/10 Iter  2500/12500]: train_loss = 0.3431\n",
            "[Epoch  4/10 Iter  2750/12500]: train_loss = 0.3297\n",
            "[Epoch  4/10 Iter  3000/12500]: train_loss = 0.3559\n",
            "[Epoch  4/10 Iter  3250/12500]: train_loss = 0.4235\n",
            "[Epoch  4/10 Iter  3500/12500]: train_loss = 0.3805\n",
            "[Epoch  4/10 Iter  3750/12500]: train_loss = 0.3296\n",
            "[Epoch  4/10 Iter  4000/12500]: train_loss = 0.3951\n",
            "[Epoch  4/10 Iter  4250/12500]: train_loss = 0.3394\n",
            "[Epoch  4/10 Iter  4500/12500]: train_loss = 0.3856\n",
            "[Epoch  4/10 Iter  4750/12500]: train_loss = 0.3665\n",
            "[Epoch  4/10 Iter  5000/12500]: train_loss = 0.3505\n",
            "[Epoch  4/10 Iter  5250/12500]: train_loss = 0.3675\n",
            "[Epoch  4/10 Iter  5500/12500]: train_loss = 0.4004\n",
            "[Epoch  4/10 Iter  5750/12500]: train_loss = 0.3620\n",
            "[Epoch  4/10 Iter  6000/12500]: train_loss = 0.3322\n",
            "[Epoch  4/10 Iter  6250/12500]: train_loss = 0.3410\n",
            "[Epoch  4/10 Iter  6500/12500]: train_loss = 0.3371\n",
            "[Epoch  4/10 Iter  6750/12500]: train_loss = 0.3875\n",
            "[Epoch  4/10 Iter  7000/12500]: train_loss = 0.3470\n",
            "[Epoch  4/10 Iter  7250/12500]: train_loss = 0.3836\n",
            "[Epoch  4/10 Iter  7500/12500]: train_loss = 0.3859\n",
            "[Epoch  4/10 Iter  7750/12500]: train_loss = 0.3345\n",
            "[Epoch  4/10 Iter  8000/12500]: train_loss = 0.3654\n",
            "[Epoch  4/10 Iter  8250/12500]: train_loss = 0.3315\n",
            "[Epoch  4/10 Iter  8500/12500]: train_loss = 0.3750\n",
            "[Epoch  4/10 Iter  8750/12500]: train_loss = 0.3198\n",
            "[Epoch  4/10 Iter  9000/12500]: train_loss = 0.3770\n",
            "[Epoch  4/10 Iter  9250/12500]: train_loss = 0.3708\n",
            "[Epoch  4/10 Iter  9500/12500]: train_loss = 0.3291\n",
            "[Epoch  4/10 Iter  9750/12500]: train_loss = 0.3513\n",
            "[Epoch  4/10 Iter 10000/12500]: train_loss = 0.3456\n",
            "[Epoch  4/10 Iter 10250/12500]: train_loss = 0.3800\n",
            "[Epoch  4/10 Iter 10500/12500]: train_loss = 0.3748\n",
            "[Epoch  4/10 Iter 10750/12500]: train_loss = 0.3360\n",
            "[Epoch  4/10 Iter 11000/12500]: train_loss = 0.3270\n",
            "[Epoch  4/10 Iter 11250/12500]: train_loss = 0.2998\n",
            "[Epoch  4/10 Iter 11500/12500]: train_loss = 0.3417\n",
            "[Epoch  4/10 Iter 11750/12500]: train_loss = 0.3528\n",
            "[Epoch  4/10 Iter 12000/12500]: train_loss = 0.3426\n",
            "[Epoch  4/10 Iter 12250/12500]: train_loss = 0.3277\n",
            "[Epoch  4/10 Iter 12500/12500]: train_loss = 0.3400\n",
            "[Epoch  5/10 Iter   250/12500]: train_loss = 0.3346\n",
            "[Epoch  5/10 Iter   500/12500]: train_loss = 0.3581\n",
            "[Epoch  5/10 Iter   750/12500]: train_loss = 0.3477\n",
            "[Epoch  5/10 Iter  1000/12500]: train_loss = 0.3427\n",
            "[Epoch  5/10 Iter  1250/12500]: train_loss = 0.2898\n",
            "[Epoch  5/10 Iter  1500/12500]: train_loss = 0.3589\n",
            "[Epoch  5/10 Iter  1750/12500]: train_loss = 0.3715\n",
            "[Epoch  5/10 Iter  2000/12500]: train_loss = 0.3070\n",
            "[Epoch  5/10 Iter  2250/12500]: train_loss = 0.2992\n",
            "[Epoch  5/10 Iter  2500/12500]: train_loss = 0.3481\n",
            "[Epoch  5/10 Iter  2750/12500]: train_loss = 0.3470\n",
            "[Epoch  5/10 Iter  3000/12500]: train_loss = 0.3668\n",
            "[Epoch  5/10 Iter  3250/12500]: train_loss = 0.3267\n",
            "[Epoch  5/10 Iter  3500/12500]: train_loss = 0.3637\n",
            "[Epoch  5/10 Iter  3750/12500]: train_loss = 0.3350\n",
            "[Epoch  5/10 Iter  4000/12500]: train_loss = 0.3808\n",
            "[Epoch  5/10 Iter  4250/12500]: train_loss = 0.3372\n",
            "[Epoch  5/10 Iter  4500/12500]: train_loss = 0.3601\n",
            "[Epoch  5/10 Iter  4750/12500]: train_loss = 0.3352\n",
            "[Epoch  5/10 Iter  5000/12500]: train_loss = 0.3450\n",
            "[Epoch  5/10 Iter  5250/12500]: train_loss = 0.3318\n",
            "[Epoch  5/10 Iter  5500/12500]: train_loss = 0.3322\n",
            "[Epoch  5/10 Iter  5750/12500]: train_loss = 0.3444\n",
            "[Epoch  5/10 Iter  6000/12500]: train_loss = 0.3319\n",
            "[Epoch  5/10 Iter  6250/12500]: train_loss = 0.3939\n",
            "[Epoch  5/10 Iter  6500/12500]: train_loss = 0.3788\n",
            "[Epoch  5/10 Iter  6750/12500]: train_loss = 0.3888\n",
            "[Epoch  5/10 Iter  7000/12500]: train_loss = 0.3233\n",
            "[Epoch  5/10 Iter  7250/12500]: train_loss = 0.3503\n",
            "[Epoch  5/10 Iter  7500/12500]: train_loss = 0.3286\n",
            "[Epoch  5/10 Iter  7750/12500]: train_loss = 0.3420\n",
            "[Epoch  5/10 Iter  8000/12500]: train_loss = 0.3390\n",
            "[Epoch  5/10 Iter  8250/12500]: train_loss = 0.3284\n",
            "[Epoch  5/10 Iter  8500/12500]: train_loss = 0.3319\n",
            "[Epoch  5/10 Iter  8750/12500]: train_loss = 0.3313\n",
            "[Epoch  5/10 Iter  9000/12500]: train_loss = 0.3292\n",
            "[Epoch  5/10 Iter  9250/12500]: train_loss = 0.3513\n",
            "[Epoch  5/10 Iter  9500/12500]: train_loss = 0.3183\n",
            "[Epoch  5/10 Iter  9750/12500]: train_loss = 0.3615\n",
            "[Epoch  5/10 Iter 10000/12500]: train_loss = 0.3146\n",
            "[Epoch  5/10 Iter 10250/12500]: train_loss = 0.3178\n",
            "[Epoch  5/10 Iter 10500/12500]: train_loss = 0.3716\n",
            "[Epoch  5/10 Iter 10750/12500]: train_loss = 0.3695\n",
            "[Epoch  5/10 Iter 11000/12500]: train_loss = 0.2877\n",
            "[Epoch  5/10 Iter 11250/12500]: train_loss = 0.3399\n",
            "[Epoch  5/10 Iter 11500/12500]: train_loss = 0.3137\n",
            "[Epoch  5/10 Iter 11750/12500]: train_loss = 0.3609\n",
            "[Epoch  5/10 Iter 12000/12500]: train_loss = 0.3022\n",
            "[Epoch  5/10 Iter 12250/12500]: train_loss = 0.2988\n",
            "[Epoch  5/10 Iter 12500/12500]: train_loss = 0.3514\n",
            "[Epoch  6/10 Iter   250/12500]: train_loss = 0.3480\n",
            "[Epoch  6/10 Iter   500/12500]: train_loss = 0.3229\n",
            "[Epoch  6/10 Iter   750/12500]: train_loss = 0.3553\n",
            "[Epoch  6/10 Iter  1000/12500]: train_loss = 0.3218\n",
            "[Epoch  6/10 Iter  1250/12500]: train_loss = 0.3185\n",
            "[Epoch  6/10 Iter  1500/12500]: train_loss = 0.2959\n",
            "[Epoch  6/10 Iter  1750/12500]: train_loss = 0.3292\n",
            "[Epoch  6/10 Iter  2000/12500]: train_loss = 0.3343\n",
            "[Epoch  6/10 Iter  2250/12500]: train_loss = 0.3150\n",
            "[Epoch  6/10 Iter  2500/12500]: train_loss = 0.3283\n",
            "[Epoch  6/10 Iter  2750/12500]: train_loss = 0.3052\n",
            "[Epoch  6/10 Iter  3000/12500]: train_loss = 0.3236\n",
            "[Epoch  6/10 Iter  3250/12500]: train_loss = 0.2824\n",
            "[Epoch  6/10 Iter  3500/12500]: train_loss = 0.3890\n",
            "[Epoch  6/10 Iter  3750/12500]: train_loss = 0.3409\n",
            "[Epoch  6/10 Iter  4000/12500]: train_loss = 0.3511\n",
            "[Epoch  6/10 Iter  4250/12500]: train_loss = 0.3212\n",
            "[Epoch  6/10 Iter  4500/12500]: train_loss = 0.3407\n",
            "[Epoch  6/10 Iter  4750/12500]: train_loss = 0.3108\n",
            "[Epoch  6/10 Iter  5000/12500]: train_loss = 0.3358\n",
            "[Epoch  6/10 Iter  5250/12500]: train_loss = 0.3494\n",
            "[Epoch  6/10 Iter  5500/12500]: train_loss = 0.3169\n",
            "[Epoch  6/10 Iter  5750/12500]: train_loss = 0.3357\n",
            "[Epoch  6/10 Iter  6000/12500]: train_loss = 0.3489\n",
            "[Epoch  6/10 Iter  6250/12500]: train_loss = 0.3451\n",
            "[Epoch  6/10 Iter  6500/12500]: train_loss = 0.3295\n",
            "[Epoch  6/10 Iter  6750/12500]: train_loss = 0.3070\n",
            "[Epoch  6/10 Iter  7000/12500]: train_loss = 0.3539\n",
            "[Epoch  6/10 Iter  7250/12500]: train_loss = 0.3088\n",
            "[Epoch  6/10 Iter  7500/12500]: train_loss = 0.3767\n",
            "[Epoch  6/10 Iter  7750/12500]: train_loss = 0.3240\n",
            "[Epoch  6/10 Iter  8000/12500]: train_loss = 0.3151\n",
            "[Epoch  6/10 Iter  8250/12500]: train_loss = 0.3025\n",
            "[Epoch  6/10 Iter  8500/12500]: train_loss = 0.3618\n",
            "[Epoch  6/10 Iter  8750/12500]: train_loss = 0.2688\n",
            "[Epoch  6/10 Iter  9000/12500]: train_loss = 0.3311\n",
            "[Epoch  6/10 Iter  9250/12500]: train_loss = 0.3314\n",
            "[Epoch  6/10 Iter  9500/12500]: train_loss = 0.3039\n",
            "[Epoch  6/10 Iter  9750/12500]: train_loss = 0.3268\n",
            "[Epoch  6/10 Iter 10000/12500]: train_loss = 0.3291\n",
            "[Epoch  6/10 Iter 10250/12500]: train_loss = 0.3292\n",
            "[Epoch  6/10 Iter 10500/12500]: train_loss = 0.3136\n",
            "[Epoch  6/10 Iter 10750/12500]: train_loss = 0.3150\n",
            "[Epoch  6/10 Iter 11000/12500]: train_loss = 0.3125\n",
            "[Epoch  6/10 Iter 11250/12500]: train_loss = 0.2886\n",
            "[Epoch  6/10 Iter 11500/12500]: train_loss = 0.3746\n",
            "[Epoch  6/10 Iter 11750/12500]: train_loss = 0.3382\n",
            "[Epoch  6/10 Iter 12000/12500]: train_loss = 0.2934\n",
            "[Epoch  6/10 Iter 12250/12500]: train_loss = 0.2857\n",
            "[Epoch  6/10 Iter 12500/12500]: train_loss = 0.3376\n",
            "[Epoch  7/10 Iter   250/12500]: train_loss = 0.3244\n",
            "[Epoch  7/10 Iter   500/12500]: train_loss = 0.3044\n",
            "[Epoch  7/10 Iter   750/12500]: train_loss = 0.3114\n",
            "[Epoch  7/10 Iter  1000/12500]: train_loss = 0.3344\n",
            "[Epoch  7/10 Iter  1250/12500]: train_loss = 0.3031\n",
            "[Epoch  7/10 Iter  1500/12500]: train_loss = 0.3265\n",
            "[Epoch  7/10 Iter  1750/12500]: train_loss = 0.3118\n",
            "[Epoch  7/10 Iter  2000/12500]: train_loss = 0.3710\n",
            "[Epoch  7/10 Iter  2250/12500]: train_loss = 0.3493\n",
            "[Epoch  7/10 Iter  2500/12500]: train_loss = 0.3048\n",
            "[Epoch  7/10 Iter  2750/12500]: train_loss = 0.3347\n",
            "[Epoch  7/10 Iter  3000/12500]: train_loss = 0.3188\n",
            "[Epoch  7/10 Iter  3250/12500]: train_loss = 0.3214\n",
            "[Epoch  7/10 Iter  3500/12500]: train_loss = 0.3061\n",
            "[Epoch  7/10 Iter  3750/12500]: train_loss = 0.2864\n",
            "[Epoch  7/10 Iter  4000/12500]: train_loss = 0.3286\n",
            "[Epoch  7/10 Iter  4250/12500]: train_loss = 0.3050\n",
            "[Epoch  7/10 Iter  4500/12500]: train_loss = 0.2794\n",
            "[Epoch  7/10 Iter  4750/12500]: train_loss = 0.3398\n",
            "[Epoch  7/10 Iter  5000/12500]: train_loss = 0.2821\n",
            "[Epoch  7/10 Iter  5250/12500]: train_loss = 0.3203\n",
            "[Epoch  7/10 Iter  5500/12500]: train_loss = 0.2699\n",
            "[Epoch  7/10 Iter  5750/12500]: train_loss = 0.3585\n",
            "[Epoch  7/10 Iter  6000/12500]: train_loss = 0.3567\n",
            "[Epoch  7/10 Iter  6250/12500]: train_loss = 0.3207\n",
            "[Epoch  7/10 Iter  6500/12500]: train_loss = 0.2886\n",
            "[Epoch  7/10 Iter  6750/12500]: train_loss = 0.2898\n",
            "[Epoch  7/10 Iter  7000/12500]: train_loss = 0.2896\n",
            "[Epoch  7/10 Iter  7250/12500]: train_loss = 0.3021\n",
            "[Epoch  7/10 Iter  7500/12500]: train_loss = 0.3287\n",
            "[Epoch  7/10 Iter  7750/12500]: train_loss = 0.3286\n",
            "[Epoch  7/10 Iter  8000/12500]: train_loss = 0.2882\n",
            "[Epoch  7/10 Iter  8250/12500]: train_loss = 0.3096\n",
            "[Epoch  7/10 Iter  8500/12500]: train_loss = 0.2983\n",
            "[Epoch  7/10 Iter  8750/12500]: train_loss = 0.3344\n",
            "[Epoch  7/10 Iter  9000/12500]: train_loss = 0.2656\n",
            "[Epoch  7/10 Iter  9250/12500]: train_loss = 0.2935\n",
            "[Epoch  7/10 Iter  9500/12500]: train_loss = 0.2847\n",
            "[Epoch  7/10 Iter  9750/12500]: train_loss = 0.2822\n",
            "[Epoch  7/10 Iter 10000/12500]: train_loss = 0.2996\n",
            "[Epoch  7/10 Iter 10250/12500]: train_loss = 0.3363\n",
            "[Epoch  7/10 Iter 10500/12500]: train_loss = 0.3108\n",
            "[Epoch  7/10 Iter 10750/12500]: train_loss = 0.3290\n",
            "[Epoch  7/10 Iter 11000/12500]: train_loss = 0.3201\n",
            "[Epoch  7/10 Iter 11250/12500]: train_loss = 0.3299\n",
            "[Epoch  7/10 Iter 11500/12500]: train_loss = 0.3124\n",
            "[Epoch  7/10 Iter 11750/12500]: train_loss = 0.3324\n",
            "[Epoch  7/10 Iter 12000/12500]: train_loss = 0.2754\n",
            "[Epoch  7/10 Iter 12250/12500]: train_loss = 0.3395\n",
            "[Epoch  7/10 Iter 12500/12500]: train_loss = 0.3419\n",
            "[Epoch  8/10 Iter   250/12500]: train_loss = 0.2913\n",
            "[Epoch  8/10 Iter   500/12500]: train_loss = 0.2704\n",
            "[Epoch  8/10 Iter   750/12500]: train_loss = 0.3098\n",
            "[Epoch  8/10 Iter  1000/12500]: train_loss = 0.3017\n",
            "[Epoch  8/10 Iter  1250/12500]: train_loss = 0.2944\n",
            "[Epoch  8/10 Iter  1500/12500]: train_loss = 0.2649\n",
            "[Epoch  8/10 Iter  1750/12500]: train_loss = 0.3134\n",
            "[Epoch  8/10 Iter  2000/12500]: train_loss = 0.3147\n",
            "[Epoch  8/10 Iter  2250/12500]: train_loss = 0.3028\n",
            "[Epoch  8/10 Iter  2500/12500]: train_loss = 0.2838\n",
            "[Epoch  8/10 Iter  2750/12500]: train_loss = 0.3028\n",
            "[Epoch  8/10 Iter  3000/12500]: train_loss = 0.2819\n",
            "[Epoch  8/10 Iter  3250/12500]: train_loss = 0.3191\n",
            "[Epoch  8/10 Iter  3500/12500]: train_loss = 0.2780\n",
            "[Epoch  8/10 Iter  3750/12500]: train_loss = 0.3062\n",
            "[Epoch  8/10 Iter  4000/12500]: train_loss = 0.3257\n",
            "[Epoch  8/10 Iter  4250/12500]: train_loss = 0.3248\n",
            "[Epoch  8/10 Iter  4500/12500]: train_loss = 0.3447\n",
            "[Epoch  8/10 Iter  4750/12500]: train_loss = 0.2957\n",
            "[Epoch  8/10 Iter  5000/12500]: train_loss = 0.3467\n",
            "[Epoch  8/10 Iter  5250/12500]: train_loss = 0.3165\n",
            "[Epoch  8/10 Iter  5500/12500]: train_loss = 0.3360\n",
            "[Epoch  8/10 Iter  5750/12500]: train_loss = 0.3163\n",
            "[Epoch  8/10 Iter  6000/12500]: train_loss = 0.2965\n",
            "[Epoch  8/10 Iter  6250/12500]: train_loss = 0.2977\n",
            "[Epoch  8/10 Iter  6500/12500]: train_loss = 0.3039\n",
            "[Epoch  8/10 Iter  6750/12500]: train_loss = 0.3195\n",
            "[Epoch  8/10 Iter  7000/12500]: train_loss = 0.3230\n",
            "[Epoch  8/10 Iter  7250/12500]: train_loss = 0.2950\n",
            "[Epoch  8/10 Iter  7500/12500]: train_loss = 0.2814\n",
            "[Epoch  8/10 Iter  7750/12500]: train_loss = 0.2753\n",
            "[Epoch  8/10 Iter  8000/12500]: train_loss = 0.3177\n",
            "[Epoch  8/10 Iter  8250/12500]: train_loss = 0.2940\n",
            "[Epoch  8/10 Iter  8500/12500]: train_loss = 0.3393\n",
            "[Epoch  8/10 Iter  8750/12500]: train_loss = 0.3098\n",
            "[Epoch  8/10 Iter  9000/12500]: train_loss = 0.2883\n",
            "[Epoch  8/10 Iter  9250/12500]: train_loss = 0.3077\n",
            "[Epoch  8/10 Iter  9500/12500]: train_loss = 0.2804\n",
            "[Epoch  8/10 Iter  9750/12500]: train_loss = 0.3140\n",
            "[Epoch  8/10 Iter 10000/12500]: train_loss = 0.3183\n",
            "[Epoch  8/10 Iter 10250/12500]: train_loss = 0.3012\n",
            "[Epoch  8/10 Iter 10500/12500]: train_loss = 0.3538\n",
            "[Epoch  8/10 Iter 10750/12500]: train_loss = 0.2996\n",
            "[Epoch  8/10 Iter 11000/12500]: train_loss = 0.2693\n",
            "[Epoch  8/10 Iter 11250/12500]: train_loss = 0.2684\n",
            "[Epoch  8/10 Iter 11500/12500]: train_loss = 0.2369\n",
            "[Epoch  8/10 Iter 11750/12500]: train_loss = 0.2823\n",
            "[Epoch  8/10 Iter 12000/12500]: train_loss = 0.3265\n",
            "[Epoch  8/10 Iter 12250/12500]: train_loss = 0.2945\n",
            "[Epoch  8/10 Iter 12500/12500]: train_loss = 0.3079\n",
            "[Epoch  9/10 Iter   250/12500]: train_loss = 0.2643\n",
            "[Epoch  9/10 Iter   500/12500]: train_loss = 0.2877\n",
            "[Epoch  9/10 Iter   750/12500]: train_loss = 0.3259\n",
            "[Epoch  9/10 Iter  1000/12500]: train_loss = 0.2832\n",
            "[Epoch  9/10 Iter  1250/12500]: train_loss = 0.2809\n",
            "[Epoch  9/10 Iter  1500/12500]: train_loss = 0.2599\n",
            "[Epoch  9/10 Iter  1750/12500]: train_loss = 0.2846\n",
            "[Epoch  9/10 Iter  2000/12500]: train_loss = 0.2554\n",
            "[Epoch  9/10 Iter  2250/12500]: train_loss = 0.2951\n",
            "[Epoch  9/10 Iter  2500/12500]: train_loss = 0.2836\n",
            "[Epoch  9/10 Iter  2750/12500]: train_loss = 0.2928\n",
            "[Epoch  9/10 Iter  3000/12500]: train_loss = 0.3250\n",
            "[Epoch  9/10 Iter  3250/12500]: train_loss = 0.2908\n",
            "[Epoch  9/10 Iter  3500/12500]: train_loss = 0.3051\n",
            "[Epoch  9/10 Iter  3750/12500]: train_loss = 0.2941\n",
            "[Epoch  9/10 Iter  4000/12500]: train_loss = 0.2806\n",
            "[Epoch  9/10 Iter  4250/12500]: train_loss = 0.3141\n",
            "[Epoch  9/10 Iter  4500/12500]: train_loss = 0.3115\n",
            "[Epoch  9/10 Iter  4750/12500]: train_loss = 0.3078\n",
            "[Epoch  9/10 Iter  5000/12500]: train_loss = 0.3351\n",
            "[Epoch  9/10 Iter  5250/12500]: train_loss = 0.3093\n",
            "[Epoch  9/10 Iter  5500/12500]: train_loss = 0.2873\n",
            "[Epoch  9/10 Iter  5750/12500]: train_loss = 0.2982\n",
            "[Epoch  9/10 Iter  6000/12500]: train_loss = 0.2680\n",
            "[Epoch  9/10 Iter  6250/12500]: train_loss = 0.3340\n",
            "[Epoch  9/10 Iter  6500/12500]: train_loss = 0.3084\n",
            "[Epoch  9/10 Iter  6750/12500]: train_loss = 0.3156\n",
            "[Epoch  9/10 Iter  7000/12500]: train_loss = 0.3156\n",
            "[Epoch  9/10 Iter  7250/12500]: train_loss = 0.2948\n",
            "[Epoch  9/10 Iter  7500/12500]: train_loss = 0.2911\n",
            "[Epoch  9/10 Iter  7750/12500]: train_loss = 0.2878\n",
            "[Epoch  9/10 Iter  8000/12500]: train_loss = 0.3176\n",
            "[Epoch  9/10 Iter  8250/12500]: train_loss = 0.2587\n",
            "[Epoch  9/10 Iter  8500/12500]: train_loss = 0.3228\n",
            "[Epoch  9/10 Iter  8750/12500]: train_loss = 0.2747\n",
            "[Epoch  9/10 Iter  9000/12500]: train_loss = 0.2591\n",
            "[Epoch  9/10 Iter  9250/12500]: train_loss = 0.2403\n",
            "[Epoch  9/10 Iter  9500/12500]: train_loss = 0.2924\n",
            "[Epoch  9/10 Iter  9750/12500]: train_loss = 0.3054\n",
            "[Epoch  9/10 Iter 10000/12500]: train_loss = 0.3429\n",
            "[Epoch  9/10 Iter 10250/12500]: train_loss = 0.3163\n",
            "[Epoch  9/10 Iter 10500/12500]: train_loss = 0.3065\n",
            "[Epoch  9/10 Iter 10750/12500]: train_loss = 0.2807\n",
            "[Epoch  9/10 Iter 11000/12500]: train_loss = 0.3003\n",
            "[Epoch  9/10 Iter 11250/12500]: train_loss = 0.2790\n",
            "[Epoch  9/10 Iter 11500/12500]: train_loss = 0.2567\n",
            "[Epoch  9/10 Iter 11750/12500]: train_loss = 0.2974\n",
            "[Epoch  9/10 Iter 12000/12500]: train_loss = 0.2846\n",
            "[Epoch  9/10 Iter 12250/12500]: train_loss = 0.3054\n",
            "[Epoch  9/10 Iter 12500/12500]: train_loss = 0.2783\n",
            "[Epoch 10/10 Iter   250/12500]: train_loss = 0.2598\n",
            "[Epoch 10/10 Iter   500/12500]: train_loss = 0.3201\n",
            "[Epoch 10/10 Iter   750/12500]: train_loss = 0.2504\n",
            "[Epoch 10/10 Iter  1000/12500]: train_loss = 0.2848\n",
            "[Epoch 10/10 Iter  1250/12500]: train_loss = 0.2837\n",
            "[Epoch 10/10 Iter  1500/12500]: train_loss = 0.3214\n",
            "[Epoch 10/10 Iter  1750/12500]: train_loss = 0.2823\n",
            "[Epoch 10/10 Iter  2000/12500]: train_loss = 0.2958\n",
            "[Epoch 10/10 Iter  2250/12500]: train_loss = 0.2569\n",
            "[Epoch 10/10 Iter  2500/12500]: train_loss = 0.3012\n",
            "[Epoch 10/10 Iter  2750/12500]: train_loss = 0.3015\n",
            "[Epoch 10/10 Iter  3000/12500]: train_loss = 0.3024\n",
            "[Epoch 10/10 Iter  3250/12500]: train_loss = 0.2900\n",
            "[Epoch 10/10 Iter  3500/12500]: train_loss = 0.3084\n",
            "[Epoch 10/10 Iter  3750/12500]: train_loss = 0.2922\n",
            "[Epoch 10/10 Iter  4000/12500]: train_loss = 0.2650\n",
            "[Epoch 10/10 Iter  4250/12500]: train_loss = 0.2800\n",
            "[Epoch 10/10 Iter  4500/12500]: train_loss = 0.2956\n",
            "[Epoch 10/10 Iter  4750/12500]: train_loss = 0.2729\n",
            "[Epoch 10/10 Iter  5000/12500]: train_loss = 0.2716\n",
            "[Epoch 10/10 Iter  5250/12500]: train_loss = 0.3143\n",
            "[Epoch 10/10 Iter  5500/12500]: train_loss = 0.2797\n",
            "[Epoch 10/10 Iter  5750/12500]: train_loss = 0.2952\n",
            "[Epoch 10/10 Iter  6000/12500]: train_loss = 0.3139\n",
            "[Epoch 10/10 Iter  6250/12500]: train_loss = 0.3176\n",
            "[Epoch 10/10 Iter  6500/12500]: train_loss = 0.3116\n",
            "[Epoch 10/10 Iter  6750/12500]: train_loss = 0.3050\n",
            "[Epoch 10/10 Iter  7000/12500]: train_loss = 0.2805\n",
            "[Epoch 10/10 Iter  7250/12500]: train_loss = 0.2823\n",
            "[Epoch 10/10 Iter  7500/12500]: train_loss = 0.2941\n",
            "[Epoch 10/10 Iter  7750/12500]: train_loss = 0.3335\n",
            "[Epoch 10/10 Iter  8000/12500]: train_loss = 0.2887\n",
            "[Epoch 10/10 Iter  8250/12500]: train_loss = 0.2866\n",
            "[Epoch 10/10 Iter  8500/12500]: train_loss = 0.2718\n",
            "[Epoch 10/10 Iter  8750/12500]: train_loss = 0.2712\n",
            "[Epoch 10/10 Iter  9000/12500]: train_loss = 0.2874\n",
            "[Epoch 10/10 Iter  9250/12500]: train_loss = 0.3250\n",
            "[Epoch 10/10 Iter  9500/12500]: train_loss = 0.2669\n",
            "[Epoch 10/10 Iter  9750/12500]: train_loss = 0.3118\n",
            "[Epoch 10/10 Iter 10000/12500]: train_loss = 0.2952\n",
            "[Epoch 10/10 Iter 10250/12500]: train_loss = 0.3150\n",
            "[Epoch 10/10 Iter 10500/12500]: train_loss = 0.3054\n",
            "[Epoch 10/10 Iter 10750/12500]: train_loss = 0.2629\n",
            "[Epoch 10/10 Iter 11000/12500]: train_loss = 0.2985\n",
            "[Epoch 10/10 Iter 11250/12500]: train_loss = 0.2709\n",
            "[Epoch 10/10 Iter 11500/12500]: train_loss = 0.2558\n",
            "[Epoch 10/10 Iter 11750/12500]: train_loss = 0.2732\n",
            "[Epoch 10/10 Iter 12000/12500]: train_loss = 0.2629\n",
            "[Epoch 10/10 Iter 12250/12500]: train_loss = 0.2481\n",
            "[Epoch 10/10 Iter 12500/12500]: train_loss = 0.2743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HJhsrPD653A",
        "outputId": "2a8f5a2f-0b25-44c6-9129-406212a511e3"
      },
      "source": [
        "evaluate(model, testloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 88.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VY8z7KIAYwJ"
      },
      "source": [
        "## 5. Convolution Model III\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I-VznUgAcw0"
      },
      "source": [
        "class cnn_model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        channel_size = [3, 32, 32]\n",
        "        layer_size = [32 * 7 * 7, 128, 10]\n",
        "        p = 0.25\n",
        "        \n",
        "        # define blocks\n",
        "        self.conv_block = nn.ModuleList(\n",
        "            build_block_cnn(in_channel, out_channel) for(in_channel, out_channel) in zip(channel_size[0:-1], channel_size[1:])\n",
        "        )\n",
        "        \n",
        "        # define Fully Connected Layers\n",
        "        self.fc = nn.ModuleList(\n",
        "            [nn.Linear(in_channel, out_channel) for(in_channel, out_channel) in zip(layer_size[0:-1], layer_size[1:])]\n",
        "        )\n",
        "\n",
        "        # Define Dropout\n",
        "        self.dropout = nn.ModuleList(\n",
        "            [nn.Dropout(p) for i in range(len(layer_size) - 2)]\n",
        "        )\n",
        "\n",
        "        # Define Batch Normalization\n",
        "        self.bn = nn.ModuleList(\n",
        "            [nn.BatchNorm2d(i) for i in channel_size[:-1]]\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = self.bn[0](x)\n",
        "        tmp = self.conv_block[0](residual)\n",
        "        x = F.max_pool2d(tmp, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        residual = self.bn[1](x)\n",
        "        tmp = self.conv_block[1](residual)\n",
        "        tmp += residual\n",
        "        x = F.max_pool2d(tmp, kernel_size = 2, stride = 2, padding = 0)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        for (dropout, fc) in zip(self.dropout, self.fc[:-1]):\n",
        "          x = dropout(torch.relu(fc(x)))\n",
        "        \n",
        "        x = torch.relu(self.fc[1](x))\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG1i7T3RBjcO",
        "outputId": "a82797e6-f706-4bed-bf73-6f0819978763"
      },
      "source": [
        "model = cnn_model3()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnn_model3(\n",
            "  (conv_block): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=1568, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            "  (dropout): ModuleList(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (bn): ModuleList(\n",
            "    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoKGZvWYBmxS",
        "outputId": "397f9eaf-d933-4a9f-c4b7-77f6a0d63301"
      },
      "source": [
        "summary(model, (3, 28, 28), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 3, 28, 28]               6\n",
            "            Conv2d-2           [-1, 32, 28, 28]             896\n",
            "              ReLU-3           [-1, 32, 28, 28]               0\n",
            "       BatchNorm2d-4           [-1, 32, 14, 14]              64\n",
            "            Conv2d-5           [-1, 32, 14, 14]           9,248\n",
            "              ReLU-6           [-1, 32, 14, 14]               0\n",
            "            Linear-7                  [-1, 128]         200,832\n",
            "           Dropout-8                  [-1, 128]               0\n",
            "            Linear-9                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 212,336\n",
            "Trainable params: 212,336\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.55\n",
            "Params size (MB): 0.81\n",
            "Estimated Total Size (MB): 1.37\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKDMK93FBodx",
        "outputId": "c648255c-b0b3-457f-8e0c-ddd4df9fc44a"
      },
      "source": [
        "history3 = train(model, trainloader, num_epochs = 10, loss_iter = 50, lr = 0.001, momentum = 0.9, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/10 Iter   250/12500]: train_loss = 1.3620\n",
            "[Epoch  1/10 Iter   500/12500]: train_loss = 0.8565\n",
            "[Epoch  1/10 Iter   750/12500]: train_loss = 0.8290\n",
            "[Epoch  1/10 Iter  1000/12500]: train_loss = 0.6938\n",
            "[Epoch  1/10 Iter  1250/12500]: train_loss = 0.6465\n",
            "[Epoch  1/10 Iter  1500/12500]: train_loss = 0.7209\n",
            "[Epoch  1/10 Iter  1750/12500]: train_loss = 0.7283\n",
            "[Epoch  1/10 Iter  2000/12500]: train_loss = 0.6626\n",
            "[Epoch  1/10 Iter  2250/12500]: train_loss = 0.5994\n",
            "[Epoch  1/10 Iter  2500/12500]: train_loss = 0.5994\n",
            "[Epoch  1/10 Iter  2750/12500]: train_loss = 0.5770\n",
            "[Epoch  1/10 Iter  3000/12500]: train_loss = 0.6463\n",
            "[Epoch  1/10 Iter  3250/12500]: train_loss = 0.6060\n",
            "[Epoch  1/10 Iter  3500/12500]: train_loss = 0.6446\n",
            "[Epoch  1/10 Iter  3750/12500]: train_loss = 0.5966\n",
            "[Epoch  1/10 Iter  4000/12500]: train_loss = 0.6522\n",
            "[Epoch  1/10 Iter  4250/12500]: train_loss = 0.5644\n",
            "[Epoch  1/10 Iter  4500/12500]: train_loss = 0.5942\n",
            "[Epoch  1/10 Iter  4750/12500]: train_loss = 0.5700\n",
            "[Epoch  1/10 Iter  5000/12500]: train_loss = 0.5523\n",
            "[Epoch  1/10 Iter  5250/12500]: train_loss = 0.5540\n",
            "[Epoch  1/10 Iter  5500/12500]: train_loss = 0.5685\n",
            "[Epoch  1/10 Iter  5750/12500]: train_loss = 0.5538\n",
            "[Epoch  1/10 Iter  6000/12500]: train_loss = 0.5223\n",
            "[Epoch  1/10 Iter  6250/12500]: train_loss = 0.5195\n",
            "[Epoch  1/10 Iter  6500/12500]: train_loss = 0.5345\n",
            "[Epoch  1/10 Iter  6750/12500]: train_loss = 0.5311\n",
            "[Epoch  1/10 Iter  7000/12500]: train_loss = 0.5054\n",
            "[Epoch  1/10 Iter  7250/12500]: train_loss = 0.5436\n",
            "[Epoch  1/10 Iter  7500/12500]: train_loss = 0.5337\n",
            "[Epoch  1/10 Iter  7750/12500]: train_loss = 0.5596\n",
            "[Epoch  1/10 Iter  8000/12500]: train_loss = 0.5251\n",
            "[Epoch  1/10 Iter  8250/12500]: train_loss = 0.5280\n",
            "[Epoch  1/10 Iter  8500/12500]: train_loss = 0.4895\n",
            "[Epoch  1/10 Iter  8750/12500]: train_loss = 0.5124\n",
            "[Epoch  1/10 Iter  9000/12500]: train_loss = 0.5348\n",
            "[Epoch  1/10 Iter  9250/12500]: train_loss = 0.5322\n",
            "[Epoch  1/10 Iter  9500/12500]: train_loss = 0.4993\n",
            "[Epoch  1/10 Iter  9750/12500]: train_loss = 0.5057\n",
            "[Epoch  1/10 Iter 10000/12500]: train_loss = 0.4880\n",
            "[Epoch  1/10 Iter 10250/12500]: train_loss = 0.4915\n",
            "[Epoch  1/10 Iter 10500/12500]: train_loss = 0.4830\n",
            "[Epoch  1/10 Iter 10750/12500]: train_loss = 0.5246\n",
            "[Epoch  1/10 Iter 11000/12500]: train_loss = 0.4886\n",
            "[Epoch  1/10 Iter 11250/12500]: train_loss = 0.4692\n",
            "[Epoch  1/10 Iter 11500/12500]: train_loss = 0.5037\n",
            "[Epoch  1/10 Iter 11750/12500]: train_loss = 0.5250\n",
            "[Epoch  1/10 Iter 12000/12500]: train_loss = 0.5244\n",
            "[Epoch  1/10 Iter 12250/12500]: train_loss = 0.4491\n",
            "[Epoch  1/10 Iter 12500/12500]: train_loss = 0.5206\n",
            "[Epoch  2/10 Iter   250/12500]: train_loss = 0.5098\n",
            "[Epoch  2/10 Iter   500/12500]: train_loss = 0.4415\n",
            "[Epoch  2/10 Iter   750/12500]: train_loss = 0.4966\n",
            "[Epoch  2/10 Iter  1000/12500]: train_loss = 0.4573\n",
            "[Epoch  2/10 Iter  1250/12500]: train_loss = 0.4737\n",
            "[Epoch  2/10 Iter  1500/12500]: train_loss = 0.4752\n",
            "[Epoch  2/10 Iter  1750/12500]: train_loss = 0.5126\n",
            "[Epoch  2/10 Iter  2000/12500]: train_loss = 0.4632\n",
            "[Epoch  2/10 Iter  2250/12500]: train_loss = 0.4326\n",
            "[Epoch  2/10 Iter  2500/12500]: train_loss = 0.4785\n",
            "[Epoch  2/10 Iter  2750/12500]: train_loss = 0.4690\n",
            "[Epoch  2/10 Iter  3000/12500]: train_loss = 0.4784\n",
            "[Epoch  2/10 Iter  3250/12500]: train_loss = 0.5010\n",
            "[Epoch  2/10 Iter  3500/12500]: train_loss = 0.4224\n",
            "[Epoch  2/10 Iter  3750/12500]: train_loss = 0.4343\n",
            "[Epoch  2/10 Iter  4000/12500]: train_loss = 0.4258\n",
            "[Epoch  2/10 Iter  4250/12500]: train_loss = 0.4737\n",
            "[Epoch  2/10 Iter  4500/12500]: train_loss = 0.4388\n",
            "[Epoch  2/10 Iter  4750/12500]: train_loss = 0.4373\n",
            "[Epoch  2/10 Iter  5000/12500]: train_loss = 0.4480\n",
            "[Epoch  2/10 Iter  5250/12500]: train_loss = 0.4251\n",
            "[Epoch  2/10 Iter  5500/12500]: train_loss = 0.4791\n",
            "[Epoch  2/10 Iter  5750/12500]: train_loss = 0.4261\n",
            "[Epoch  2/10 Iter  6000/12500]: train_loss = 0.4439\n",
            "[Epoch  2/10 Iter  6250/12500]: train_loss = 0.4227\n",
            "[Epoch  2/10 Iter  6500/12500]: train_loss = 0.4232\n",
            "[Epoch  2/10 Iter  6750/12500]: train_loss = 0.4629\n",
            "[Epoch  2/10 Iter  7000/12500]: train_loss = 0.4230\n",
            "[Epoch  2/10 Iter  7250/12500]: train_loss = 0.4295\n",
            "[Epoch  2/10 Iter  7500/12500]: train_loss = 0.4457\n",
            "[Epoch  2/10 Iter  7750/12500]: train_loss = 0.4127\n",
            "[Epoch  2/10 Iter  8000/12500]: train_loss = 0.4639\n",
            "[Epoch  2/10 Iter  8250/12500]: train_loss = 0.4285\n",
            "[Epoch  2/10 Iter  8500/12500]: train_loss = 0.4490\n",
            "[Epoch  2/10 Iter  8750/12500]: train_loss = 0.4258\n",
            "[Epoch  2/10 Iter  9000/12500]: train_loss = 0.4366\n",
            "[Epoch  2/10 Iter  9250/12500]: train_loss = 0.4260\n",
            "[Epoch  2/10 Iter  9500/12500]: train_loss = 0.4179\n",
            "[Epoch  2/10 Iter  9750/12500]: train_loss = 0.4505\n",
            "[Epoch  2/10 Iter 10000/12500]: train_loss = 0.4029\n",
            "[Epoch  2/10 Iter 10250/12500]: train_loss = 0.4435\n",
            "[Epoch  2/10 Iter 10500/12500]: train_loss = 0.4264\n",
            "[Epoch  2/10 Iter 10750/12500]: train_loss = 0.4237\n",
            "[Epoch  2/10 Iter 11000/12500]: train_loss = 0.4900\n",
            "[Epoch  2/10 Iter 11250/12500]: train_loss = 0.4370\n",
            "[Epoch  2/10 Iter 11500/12500]: train_loss = 0.3553\n",
            "[Epoch  2/10 Iter 11750/12500]: train_loss = 0.4387\n",
            "[Epoch  2/10 Iter 12000/12500]: train_loss = 0.4311\n",
            "[Epoch  2/10 Iter 12250/12500]: train_loss = 0.4440\n",
            "[Epoch  2/10 Iter 12500/12500]: train_loss = 0.4351\n",
            "[Epoch  3/10 Iter   250/12500]: train_loss = 0.3972\n",
            "[Epoch  3/10 Iter   500/12500]: train_loss = 0.3859\n",
            "[Epoch  3/10 Iter   750/12500]: train_loss = 0.3643\n",
            "[Epoch  3/10 Iter  1000/12500]: train_loss = 0.4216\n",
            "[Epoch  3/10 Iter  1250/12500]: train_loss = 0.4409\n",
            "[Epoch  3/10 Iter  1500/12500]: train_loss = 0.4562\n",
            "[Epoch  3/10 Iter  1750/12500]: train_loss = 0.4163\n",
            "[Epoch  3/10 Iter  2000/12500]: train_loss = 0.4123\n",
            "[Epoch  3/10 Iter  2250/12500]: train_loss = 0.3785\n",
            "[Epoch  3/10 Iter  2500/12500]: train_loss = 0.4272\n",
            "[Epoch  3/10 Iter  2750/12500]: train_loss = 0.4188\n",
            "[Epoch  3/10 Iter  3000/12500]: train_loss = 0.3940\n",
            "[Epoch  3/10 Iter  3250/12500]: train_loss = 0.4114\n",
            "[Epoch  3/10 Iter  3500/12500]: train_loss = 0.4223\n",
            "[Epoch  3/10 Iter  3750/12500]: train_loss = 0.4606\n",
            "[Epoch  3/10 Iter  4000/12500]: train_loss = 0.4245\n",
            "[Epoch  3/10 Iter  4250/12500]: train_loss = 0.4005\n",
            "[Epoch  3/10 Iter  4500/12500]: train_loss = 0.4297\n",
            "[Epoch  3/10 Iter  4750/12500]: train_loss = 0.3847\n",
            "[Epoch  3/10 Iter  5000/12500]: train_loss = 0.4387\n",
            "[Epoch  3/10 Iter  5250/12500]: train_loss = 0.3750\n",
            "[Epoch  3/10 Iter  5500/12500]: train_loss = 0.3844\n",
            "[Epoch  3/10 Iter  5750/12500]: train_loss = 0.4031\n",
            "[Epoch  3/10 Iter  6000/12500]: train_loss = 0.4442\n",
            "[Epoch  3/10 Iter  6250/12500]: train_loss = 0.4075\n",
            "[Epoch  3/10 Iter  6500/12500]: train_loss = 0.3639\n",
            "[Epoch  3/10 Iter  6750/12500]: train_loss = 0.3756\n",
            "[Epoch  3/10 Iter  7000/12500]: train_loss = 0.3833\n",
            "[Epoch  3/10 Iter  7250/12500]: train_loss = 0.4136\n",
            "[Epoch  3/10 Iter  7500/12500]: train_loss = 0.4062\n",
            "[Epoch  3/10 Iter  7750/12500]: train_loss = 0.4091\n",
            "[Epoch  3/10 Iter  8000/12500]: train_loss = 0.4167\n",
            "[Epoch  3/10 Iter  8250/12500]: train_loss = 0.3616\n",
            "[Epoch  3/10 Iter  8500/12500]: train_loss = 0.4105\n",
            "[Epoch  3/10 Iter  8750/12500]: train_loss = 0.3788\n",
            "[Epoch  3/10 Iter  9000/12500]: train_loss = 0.3820\n",
            "[Epoch  3/10 Iter  9250/12500]: train_loss = 0.3997\n",
            "[Epoch  3/10 Iter  9500/12500]: train_loss = 0.4143\n",
            "[Epoch  3/10 Iter  9750/12500]: train_loss = 0.4038\n",
            "[Epoch  3/10 Iter 10000/12500]: train_loss = 0.3881\n",
            "[Epoch  3/10 Iter 10250/12500]: train_loss = 0.3634\n",
            "[Epoch  3/10 Iter 10500/12500]: train_loss = 0.4235\n",
            "[Epoch  3/10 Iter 10750/12500]: train_loss = 0.3656\n",
            "[Epoch  3/10 Iter 11000/12500]: train_loss = 0.3540\n",
            "[Epoch  3/10 Iter 11250/12500]: train_loss = 0.3546\n",
            "[Epoch  3/10 Iter 11500/12500]: train_loss = 0.4040\n",
            "[Epoch  3/10 Iter 11750/12500]: train_loss = 0.3764\n",
            "[Epoch  3/10 Iter 12000/12500]: train_loss = 0.3571\n",
            "[Epoch  3/10 Iter 12250/12500]: train_loss = 0.4219\n",
            "[Epoch  3/10 Iter 12500/12500]: train_loss = 0.4305\n",
            "[Epoch  4/10 Iter   250/12500]: train_loss = 0.3795\n",
            "[Epoch  4/10 Iter   500/12500]: train_loss = 0.4139\n",
            "[Epoch  4/10 Iter   750/12500]: train_loss = 0.3931\n",
            "[Epoch  4/10 Iter  1000/12500]: train_loss = 0.3882\n",
            "[Epoch  4/10 Iter  1250/12500]: train_loss = 0.3833\n",
            "[Epoch  4/10 Iter  1500/12500]: train_loss = 0.3693\n",
            "[Epoch  4/10 Iter  1750/12500]: train_loss = 0.3287\n",
            "[Epoch  4/10 Iter  2000/12500]: train_loss = 0.3649\n",
            "[Epoch  4/10 Iter  2250/12500]: train_loss = 0.4088\n",
            "[Epoch  4/10 Iter  2500/12500]: train_loss = 0.4546\n",
            "[Epoch  4/10 Iter  2750/12500]: train_loss = 0.3581\n",
            "[Epoch  4/10 Iter  3000/12500]: train_loss = 0.3509\n",
            "[Epoch  4/10 Iter  3250/12500]: train_loss = 0.4070\n",
            "[Epoch  4/10 Iter  3500/12500]: train_loss = 0.3389\n",
            "[Epoch  4/10 Iter  3750/12500]: train_loss = 0.3915\n",
            "[Epoch  4/10 Iter  4000/12500]: train_loss = 0.3359\n",
            "[Epoch  4/10 Iter  4250/12500]: train_loss = 0.3677\n",
            "[Epoch  4/10 Iter  4500/12500]: train_loss = 0.3552\n",
            "[Epoch  4/10 Iter  4750/12500]: train_loss = 0.3682\n",
            "[Epoch  4/10 Iter  5000/12500]: train_loss = 0.3527\n",
            "[Epoch  4/10 Iter  5250/12500]: train_loss = 0.3024\n",
            "[Epoch  4/10 Iter  5500/12500]: train_loss = 0.4047\n",
            "[Epoch  4/10 Iter  5750/12500]: train_loss = 0.4073\n",
            "[Epoch  4/10 Iter  6000/12500]: train_loss = 0.3645\n",
            "[Epoch  4/10 Iter  6250/12500]: train_loss = 0.4270\n",
            "[Epoch  4/10 Iter  6500/12500]: train_loss = 0.3747\n",
            "[Epoch  4/10 Iter  6750/12500]: train_loss = 0.3761\n",
            "[Epoch  4/10 Iter  7000/12500]: train_loss = 0.3809\n",
            "[Epoch  4/10 Iter  7250/12500]: train_loss = 0.3414\n",
            "[Epoch  4/10 Iter  7500/12500]: train_loss = 0.3646\n",
            "[Epoch  4/10 Iter  7750/12500]: train_loss = 0.3905\n",
            "[Epoch  4/10 Iter  8000/12500]: train_loss = 0.3621\n",
            "[Epoch  4/10 Iter  8250/12500]: train_loss = 0.3803\n",
            "[Epoch  4/10 Iter  8500/12500]: train_loss = 0.3407\n",
            "[Epoch  4/10 Iter  8750/12500]: train_loss = 0.4035\n",
            "[Epoch  4/10 Iter  9000/12500]: train_loss = 0.3997\n",
            "[Epoch  4/10 Iter  9250/12500]: train_loss = 0.3440\n",
            "[Epoch  4/10 Iter  9500/12500]: train_loss = 0.3663\n",
            "[Epoch  4/10 Iter  9750/12500]: train_loss = 0.3665\n",
            "[Epoch  4/10 Iter 10000/12500]: train_loss = 0.3554\n",
            "[Epoch  4/10 Iter 10250/12500]: train_loss = 0.3233\n",
            "[Epoch  4/10 Iter 10500/12500]: train_loss = 0.3562\n",
            "[Epoch  4/10 Iter 10750/12500]: train_loss = 0.3344\n",
            "[Epoch  4/10 Iter 11000/12500]: train_loss = 0.3911\n",
            "[Epoch  4/10 Iter 11250/12500]: train_loss = 0.3519\n",
            "[Epoch  4/10 Iter 11500/12500]: train_loss = 0.3410\n",
            "[Epoch  4/10 Iter 11750/12500]: train_loss = 0.3177\n",
            "[Epoch  4/10 Iter 12000/12500]: train_loss = 0.3721\n",
            "[Epoch  4/10 Iter 12250/12500]: train_loss = 0.4291\n",
            "[Epoch  4/10 Iter 12500/12500]: train_loss = 0.3399\n",
            "[Epoch  5/10 Iter   250/12500]: train_loss = 0.3518\n",
            "[Epoch  5/10 Iter   500/12500]: train_loss = 0.3558\n",
            "[Epoch  5/10 Iter   750/12500]: train_loss = 0.3728\n",
            "[Epoch  5/10 Iter  1000/12500]: train_loss = 0.3587\n",
            "[Epoch  5/10 Iter  1250/12500]: train_loss = 0.3624\n",
            "[Epoch  5/10 Iter  1500/12500]: train_loss = 0.3039\n",
            "[Epoch  5/10 Iter  1750/12500]: train_loss = 0.3731\n",
            "[Epoch  5/10 Iter  2000/12500]: train_loss = 0.3312\n",
            "[Epoch  5/10 Iter  2250/12500]: train_loss = 0.3593\n",
            "[Epoch  5/10 Iter  2500/12500]: train_loss = 0.3694\n",
            "[Epoch  5/10 Iter  2750/12500]: train_loss = 0.3563\n",
            "[Epoch  5/10 Iter  3000/12500]: train_loss = 0.3466\n",
            "[Epoch  5/10 Iter  3250/12500]: train_loss = 0.3345\n",
            "[Epoch  5/10 Iter  3500/12500]: train_loss = 0.3920\n",
            "[Epoch  5/10 Iter  3750/12500]: train_loss = 0.3366\n",
            "[Epoch  5/10 Iter  4000/12500]: train_loss = 0.3882\n",
            "[Epoch  5/10 Iter  4250/12500]: train_loss = 0.3514\n",
            "[Epoch  5/10 Iter  4500/12500]: train_loss = 0.3750\n",
            "[Epoch  5/10 Iter  4750/12500]: train_loss = 0.3304\n",
            "[Epoch  5/10 Iter  5000/12500]: train_loss = 0.3711\n",
            "[Epoch  5/10 Iter  5250/12500]: train_loss = 0.3996\n",
            "[Epoch  5/10 Iter  5500/12500]: train_loss = 0.3829\n",
            "[Epoch  5/10 Iter  5750/12500]: train_loss = 0.3831\n",
            "[Epoch  5/10 Iter  6000/12500]: train_loss = 0.3531\n",
            "[Epoch  5/10 Iter  6250/12500]: train_loss = 0.3391\n",
            "[Epoch  5/10 Iter  6500/12500]: train_loss = 0.3662\n",
            "[Epoch  5/10 Iter  6750/12500]: train_loss = 0.3306\n",
            "[Epoch  5/10 Iter  7000/12500]: train_loss = 0.3337\n",
            "[Epoch  5/10 Iter  7250/12500]: train_loss = 0.3828\n",
            "[Epoch  5/10 Iter  7500/12500]: train_loss = 0.3458\n",
            "[Epoch  5/10 Iter  7750/12500]: train_loss = 0.3645\n",
            "[Epoch  5/10 Iter  8000/12500]: train_loss = 0.3674\n",
            "[Epoch  5/10 Iter  8250/12500]: train_loss = 0.4242\n",
            "[Epoch  5/10 Iter  8500/12500]: train_loss = 0.3428\n",
            "[Epoch  5/10 Iter  8750/12500]: train_loss = 0.3596\n",
            "[Epoch  5/10 Iter  9000/12500]: train_loss = 0.3271\n",
            "[Epoch  5/10 Iter  9250/12500]: train_loss = 0.3309\n",
            "[Epoch  5/10 Iter  9500/12500]: train_loss = 0.3460\n",
            "[Epoch  5/10 Iter  9750/12500]: train_loss = 0.3318\n",
            "[Epoch  5/10 Iter 10000/12500]: train_loss = 0.3579\n",
            "[Epoch  5/10 Iter 10250/12500]: train_loss = 0.3283\n",
            "[Epoch  5/10 Iter 10500/12500]: train_loss = 0.4228\n",
            "[Epoch  5/10 Iter 10750/12500]: train_loss = 0.3233\n",
            "[Epoch  5/10 Iter 11000/12500]: train_loss = 0.3836\n",
            "[Epoch  5/10 Iter 11250/12500]: train_loss = 0.4037\n",
            "[Epoch  5/10 Iter 11500/12500]: train_loss = 0.3362\n",
            "[Epoch  5/10 Iter 11750/12500]: train_loss = 0.3295\n",
            "[Epoch  5/10 Iter 12000/12500]: train_loss = 0.3743\n",
            "[Epoch  5/10 Iter 12250/12500]: train_loss = 0.3837\n",
            "[Epoch  5/10 Iter 12500/12500]: train_loss = 0.3669\n",
            "[Epoch  6/10 Iter   250/12500]: train_loss = 0.3314\n",
            "[Epoch  6/10 Iter   500/12500]: train_loss = 0.3443\n",
            "[Epoch  6/10 Iter   750/12500]: train_loss = 0.3330\n",
            "[Epoch  6/10 Iter  1000/12500]: train_loss = 0.3672\n",
            "[Epoch  6/10 Iter  1250/12500]: train_loss = 0.3571\n",
            "[Epoch  6/10 Iter  1500/12500]: train_loss = 0.3346\n",
            "[Epoch  6/10 Iter  1750/12500]: train_loss = 0.3681\n",
            "[Epoch  6/10 Iter  2000/12500]: train_loss = 0.3759\n",
            "[Epoch  6/10 Iter  2250/12500]: train_loss = 0.3323\n",
            "[Epoch  6/10 Iter  2500/12500]: train_loss = 0.3271\n",
            "[Epoch  6/10 Iter  2750/12500]: train_loss = 0.3763\n",
            "[Epoch  6/10 Iter  3000/12500]: train_loss = 0.3037\n",
            "[Epoch  6/10 Iter  3250/12500]: train_loss = 0.3362\n",
            "[Epoch  6/10 Iter  3500/12500]: train_loss = 0.3337\n",
            "[Epoch  6/10 Iter  3750/12500]: train_loss = 0.3755\n",
            "[Epoch  6/10 Iter  4000/12500]: train_loss = 0.3471\n",
            "[Epoch  6/10 Iter  4250/12500]: train_loss = 0.3538\n",
            "[Epoch  6/10 Iter  4500/12500]: train_loss = 0.2934\n",
            "[Epoch  6/10 Iter  4750/12500]: train_loss = 0.3267\n",
            "[Epoch  6/10 Iter  5000/12500]: train_loss = 0.3684\n",
            "[Epoch  6/10 Iter  5250/12500]: train_loss = 0.3542\n",
            "[Epoch  6/10 Iter  5500/12500]: train_loss = 0.3556\n",
            "[Epoch  6/10 Iter  5750/12500]: train_loss = 0.3460\n",
            "[Epoch  6/10 Iter  6000/12500]: train_loss = 0.3727\n",
            "[Epoch  6/10 Iter  6250/12500]: train_loss = 0.3528\n",
            "[Epoch  6/10 Iter  6500/12500]: train_loss = 0.3656\n",
            "[Epoch  6/10 Iter  6750/12500]: train_loss = 0.3369\n",
            "[Epoch  6/10 Iter  7000/12500]: train_loss = 0.3317\n",
            "[Epoch  6/10 Iter  7250/12500]: train_loss = 0.3630\n",
            "[Epoch  6/10 Iter  7500/12500]: train_loss = 0.3844\n",
            "[Epoch  6/10 Iter  7750/12500]: train_loss = 0.3306\n",
            "[Epoch  6/10 Iter  8000/12500]: train_loss = 0.3634\n",
            "[Epoch  6/10 Iter  8250/12500]: train_loss = 0.3340\n",
            "[Epoch  6/10 Iter  8500/12500]: train_loss = 0.3342\n",
            "[Epoch  6/10 Iter  8750/12500]: train_loss = 0.3218\n",
            "[Epoch  6/10 Iter  9000/12500]: train_loss = 0.3379\n",
            "[Epoch  6/10 Iter  9250/12500]: train_loss = 0.3374\n",
            "[Epoch  6/10 Iter  9500/12500]: train_loss = 0.3377\n",
            "[Epoch  6/10 Iter  9750/12500]: train_loss = 0.3786\n",
            "[Epoch  6/10 Iter 10000/12500]: train_loss = 0.2872\n",
            "[Epoch  6/10 Iter 10250/12500]: train_loss = 0.3283\n",
            "[Epoch  6/10 Iter 10500/12500]: train_loss = 0.3388\n",
            "[Epoch  6/10 Iter 10750/12500]: train_loss = 0.3257\n",
            "[Epoch  6/10 Iter 11000/12500]: train_loss = 0.3445\n",
            "[Epoch  6/10 Iter 11250/12500]: train_loss = 0.2912\n",
            "[Epoch  6/10 Iter 11500/12500]: train_loss = 0.3042\n",
            "[Epoch  6/10 Iter 11750/12500]: train_loss = 0.3210\n",
            "[Epoch  6/10 Iter 12000/12500]: train_loss = 0.3670\n",
            "[Epoch  6/10 Iter 12250/12500]: train_loss = 0.3137\n",
            "[Epoch  6/10 Iter 12500/12500]: train_loss = 0.3417\n",
            "[Epoch  7/10 Iter   250/12500]: train_loss = 0.3632\n",
            "[Epoch  7/10 Iter   500/12500]: train_loss = 0.3137\n",
            "[Epoch  7/10 Iter   750/12500]: train_loss = 0.2825\n",
            "[Epoch  7/10 Iter  1000/12500]: train_loss = 0.3257\n",
            "[Epoch  7/10 Iter  1250/12500]: train_loss = 0.3229\n",
            "[Epoch  7/10 Iter  1500/12500]: train_loss = 0.3660\n",
            "[Epoch  7/10 Iter  1750/12500]: train_loss = 0.3304\n",
            "[Epoch  7/10 Iter  2000/12500]: train_loss = 0.3337\n",
            "[Epoch  7/10 Iter  2250/12500]: train_loss = 0.3732\n",
            "[Epoch  7/10 Iter  2500/12500]: train_loss = 0.3765\n",
            "[Epoch  7/10 Iter  2750/12500]: train_loss = 0.3620\n",
            "[Epoch  7/10 Iter  3000/12500]: train_loss = 0.3416\n",
            "[Epoch  7/10 Iter  3250/12500]: train_loss = 0.3198\n",
            "[Epoch  7/10 Iter  3500/12500]: train_loss = 0.3166\n",
            "[Epoch  7/10 Iter  3750/12500]: train_loss = 0.3754\n",
            "[Epoch  7/10 Iter  4000/12500]: train_loss = 0.3094\n",
            "[Epoch  7/10 Iter  4250/12500]: train_loss = 0.3794\n",
            "[Epoch  7/10 Iter  4500/12500]: train_loss = 0.2843\n",
            "[Epoch  7/10 Iter  4750/12500]: train_loss = 0.3258\n",
            "[Epoch  7/10 Iter  5000/12500]: train_loss = 0.3721\n",
            "[Epoch  7/10 Iter  5250/12500]: train_loss = 0.4066\n",
            "[Epoch  7/10 Iter  5500/12500]: train_loss = 0.3402\n",
            "[Epoch  7/10 Iter  5750/12500]: train_loss = 0.3478\n",
            "[Epoch  7/10 Iter  6000/12500]: train_loss = 0.3192\n",
            "[Epoch  7/10 Iter  6250/12500]: train_loss = 0.3056\n",
            "[Epoch  7/10 Iter  6500/12500]: train_loss = 0.3136\n",
            "[Epoch  7/10 Iter  6750/12500]: train_loss = 0.3124\n",
            "[Epoch  7/10 Iter  7000/12500]: train_loss = 0.3148\n",
            "[Epoch  7/10 Iter  7250/12500]: train_loss = 0.3347\n",
            "[Epoch  7/10 Iter  7500/12500]: train_loss = 0.2863\n",
            "[Epoch  7/10 Iter  7750/12500]: train_loss = 0.3199\n",
            "[Epoch  7/10 Iter  8000/12500]: train_loss = 0.3172\n",
            "[Epoch  7/10 Iter  8250/12500]: train_loss = 0.3286\n",
            "[Epoch  7/10 Iter  8500/12500]: train_loss = 0.3331\n",
            "[Epoch  7/10 Iter  8750/12500]: train_loss = 0.3309\n",
            "[Epoch  7/10 Iter  9000/12500]: train_loss = 0.3372\n",
            "[Epoch  7/10 Iter  9250/12500]: train_loss = 0.3044\n",
            "[Epoch  7/10 Iter  9500/12500]: train_loss = 0.3257\n",
            "[Epoch  7/10 Iter  9750/12500]: train_loss = 0.3615\n",
            "[Epoch  7/10 Iter 10000/12500]: train_loss = 0.3530\n",
            "[Epoch  7/10 Iter 10250/12500]: train_loss = 0.3405\n",
            "[Epoch  7/10 Iter 10500/12500]: train_loss = 0.3034\n",
            "[Epoch  7/10 Iter 10750/12500]: train_loss = 0.3396\n",
            "[Epoch  7/10 Iter 11000/12500]: train_loss = 0.3140\n",
            "[Epoch  7/10 Iter 11250/12500]: train_loss = 0.2850\n",
            "[Epoch  7/10 Iter 11500/12500]: train_loss = 0.3401\n",
            "[Epoch  7/10 Iter 11750/12500]: train_loss = 0.3296\n",
            "[Epoch  7/10 Iter 12000/12500]: train_loss = 0.3516\n",
            "[Epoch  7/10 Iter 12250/12500]: train_loss = 0.3263\n",
            "[Epoch  7/10 Iter 12500/12500]: train_loss = 0.3311\n",
            "[Epoch  8/10 Iter   250/12500]: train_loss = 0.3508\n",
            "[Epoch  8/10 Iter   500/12500]: train_loss = 0.3442\n",
            "[Epoch  8/10 Iter   750/12500]: train_loss = 0.3532\n",
            "[Epoch  8/10 Iter  1000/12500]: train_loss = 0.3539\n",
            "[Epoch  8/10 Iter  1250/12500]: train_loss = 0.2984\n",
            "[Epoch  8/10 Iter  1500/12500]: train_loss = 0.3268\n",
            "[Epoch  8/10 Iter  1750/12500]: train_loss = 0.3235\n",
            "[Epoch  8/10 Iter  2000/12500]: train_loss = 0.3438\n",
            "[Epoch  8/10 Iter  2250/12500]: train_loss = 0.3471\n",
            "[Epoch  8/10 Iter  2500/12500]: train_loss = 0.3275\n",
            "[Epoch  8/10 Iter  2750/12500]: train_loss = 0.2867\n",
            "[Epoch  8/10 Iter  3000/12500]: train_loss = 0.3108\n",
            "[Epoch  8/10 Iter  3250/12500]: train_loss = 0.3504\n",
            "[Epoch  8/10 Iter  3500/12500]: train_loss = 0.3273\n",
            "[Epoch  8/10 Iter  3750/12500]: train_loss = 0.3626\n",
            "[Epoch  8/10 Iter  4000/12500]: train_loss = 0.2855\n",
            "[Epoch  8/10 Iter  4250/12500]: train_loss = 0.3552\n",
            "[Epoch  8/10 Iter  4500/12500]: train_loss = 0.3248\n",
            "[Epoch  8/10 Iter  4750/12500]: train_loss = 0.3344\n",
            "[Epoch  8/10 Iter  5000/12500]: train_loss = 0.3363\n",
            "[Epoch  8/10 Iter  5250/12500]: train_loss = 0.3371\n",
            "[Epoch  8/10 Iter  5500/12500]: train_loss = 0.2828\n",
            "[Epoch  8/10 Iter  5750/12500]: train_loss = 0.3068\n",
            "[Epoch  8/10 Iter  6000/12500]: train_loss = 0.3312\n",
            "[Epoch  8/10 Iter  6250/12500]: train_loss = 0.3098\n",
            "[Epoch  8/10 Iter  6500/12500]: train_loss = 0.2877\n",
            "[Epoch  8/10 Iter  6750/12500]: train_loss = 0.3343\n",
            "[Epoch  8/10 Iter  7000/12500]: train_loss = 0.2885\n",
            "[Epoch  8/10 Iter  7250/12500]: train_loss = 0.3203\n",
            "[Epoch  8/10 Iter  7500/12500]: train_loss = 0.3206\n",
            "[Epoch  8/10 Iter  7750/12500]: train_loss = 0.3209\n",
            "[Epoch  8/10 Iter  8000/12500]: train_loss = 0.3117\n",
            "[Epoch  8/10 Iter  8250/12500]: train_loss = 0.3510\n",
            "[Epoch  8/10 Iter  8500/12500]: train_loss = 0.3685\n",
            "[Epoch  8/10 Iter  8750/12500]: train_loss = 0.3278\n",
            "[Epoch  8/10 Iter  9000/12500]: train_loss = 0.3158\n",
            "[Epoch  8/10 Iter  9250/12500]: train_loss = 0.3221\n",
            "[Epoch  8/10 Iter  9500/12500]: train_loss = 0.3740\n",
            "[Epoch  8/10 Iter  9750/12500]: train_loss = 0.3379\n",
            "[Epoch  8/10 Iter 10000/12500]: train_loss = 0.3217\n",
            "[Epoch  8/10 Iter 10250/12500]: train_loss = 0.3168\n",
            "[Epoch  8/10 Iter 10500/12500]: train_loss = 0.2931\n",
            "[Epoch  8/10 Iter 10750/12500]: train_loss = 0.3380\n",
            "[Epoch  8/10 Iter 11000/12500]: train_loss = 0.3033\n",
            "[Epoch  8/10 Iter 11250/12500]: train_loss = 0.3379\n",
            "[Epoch  8/10 Iter 11500/12500]: train_loss = 0.3155\n",
            "[Epoch  8/10 Iter 11750/12500]: train_loss = 0.3075\n",
            "[Epoch  8/10 Iter 12000/12500]: train_loss = 0.2915\n",
            "[Epoch  8/10 Iter 12250/12500]: train_loss = 0.3398\n",
            "[Epoch  8/10 Iter 12500/12500]: train_loss = 0.3312\n",
            "[Epoch  9/10 Iter   250/12500]: train_loss = 0.2863\n",
            "[Epoch  9/10 Iter   500/12500]: train_loss = 0.3069\n",
            "[Epoch  9/10 Iter   750/12500]: train_loss = 0.2943\n",
            "[Epoch  9/10 Iter  1000/12500]: train_loss = 0.3173\n",
            "[Epoch  9/10 Iter  1250/12500]: train_loss = 0.3301\n",
            "[Epoch  9/10 Iter  1500/12500]: train_loss = 0.3173\n",
            "[Epoch  9/10 Iter  1750/12500]: train_loss = 0.3115\n",
            "[Epoch  9/10 Iter  2000/12500]: train_loss = 0.2943\n",
            "[Epoch  9/10 Iter  2250/12500]: train_loss = 0.2780\n",
            "[Epoch  9/10 Iter  2500/12500]: train_loss = 0.3077\n",
            "[Epoch  9/10 Iter  2750/12500]: train_loss = 0.3571\n",
            "[Epoch  9/10 Iter  3000/12500]: train_loss = 0.2928\n",
            "[Epoch  9/10 Iter  3250/12500]: train_loss = 0.3674\n",
            "[Epoch  9/10 Iter  3500/12500]: train_loss = 0.3277\n",
            "[Epoch  9/10 Iter  3750/12500]: train_loss = 0.2985\n",
            "[Epoch  9/10 Iter  4000/12500]: train_loss = 0.3685\n",
            "[Epoch  9/10 Iter  4250/12500]: train_loss = 0.3442\n",
            "[Epoch  9/10 Iter  4500/12500]: train_loss = 0.3086\n",
            "[Epoch  9/10 Iter  4750/12500]: train_loss = 0.3363\n",
            "[Epoch  9/10 Iter  5000/12500]: train_loss = 0.3035\n",
            "[Epoch  9/10 Iter  5250/12500]: train_loss = 0.3313\n",
            "[Epoch  9/10 Iter  5500/12500]: train_loss = 0.3295\n",
            "[Epoch  9/10 Iter  5750/12500]: train_loss = 0.3058\n",
            "[Epoch  9/10 Iter  6000/12500]: train_loss = 0.3082\n",
            "[Epoch  9/10 Iter  6250/12500]: train_loss = 0.2996\n",
            "[Epoch  9/10 Iter  6500/12500]: train_loss = 0.3070\n",
            "[Epoch  9/10 Iter  6750/12500]: train_loss = 0.3649\n",
            "[Epoch  9/10 Iter  7000/12500]: train_loss = 0.2898\n",
            "[Epoch  9/10 Iter  7250/12500]: train_loss = 0.2811\n",
            "[Epoch  9/10 Iter  7500/12500]: train_loss = 0.2828\n",
            "[Epoch  9/10 Iter  7750/12500]: train_loss = 0.3129\n",
            "[Epoch  9/10 Iter  8000/12500]: train_loss = 0.3008\n",
            "[Epoch  9/10 Iter  8250/12500]: train_loss = 0.3550\n",
            "[Epoch  9/10 Iter  8500/12500]: train_loss = 0.2861\n",
            "[Epoch  9/10 Iter  8750/12500]: train_loss = 0.3863\n",
            "[Epoch  9/10 Iter  9000/12500]: train_loss = 0.2689\n",
            "[Epoch  9/10 Iter  9250/12500]: train_loss = 0.3424\n",
            "[Epoch  9/10 Iter  9500/12500]: train_loss = 0.3594\n",
            "[Epoch  9/10 Iter  9750/12500]: train_loss = 0.2878\n",
            "[Epoch  9/10 Iter 10000/12500]: train_loss = 0.3482\n",
            "[Epoch  9/10 Iter 10250/12500]: train_loss = 0.2869\n",
            "[Epoch  9/10 Iter 10500/12500]: train_loss = 0.2881\n",
            "[Epoch  9/10 Iter 10750/12500]: train_loss = 0.3140\n",
            "[Epoch  9/10 Iter 11000/12500]: train_loss = 0.3185\n",
            "[Epoch  9/10 Iter 11250/12500]: train_loss = 0.3128\n",
            "[Epoch  9/10 Iter 11500/12500]: train_loss = 0.3035\n",
            "[Epoch  9/10 Iter 11750/12500]: train_loss = 0.3463\n",
            "[Epoch  9/10 Iter 12000/12500]: train_loss = 0.3033\n",
            "[Epoch  9/10 Iter 12250/12500]: train_loss = 0.2962\n",
            "[Epoch  9/10 Iter 12500/12500]: train_loss = 0.2628\n",
            "[Epoch 10/10 Iter   250/12500]: train_loss = 0.3198\n",
            "[Epoch 10/10 Iter   500/12500]: train_loss = 0.2686\n",
            "[Epoch 10/10 Iter   750/12500]: train_loss = 0.2942\n",
            "[Epoch 10/10 Iter  1000/12500]: train_loss = 0.3099\n",
            "[Epoch 10/10 Iter  1250/12500]: train_loss = 0.2966\n",
            "[Epoch 10/10 Iter  1500/12500]: train_loss = 0.2947\n",
            "[Epoch 10/10 Iter  1750/12500]: train_loss = 0.3378\n",
            "[Epoch 10/10 Iter  2000/12500]: train_loss = 0.3052\n",
            "[Epoch 10/10 Iter  2250/12500]: train_loss = 0.3125\n",
            "[Epoch 10/10 Iter  2500/12500]: train_loss = 0.3307\n",
            "[Epoch 10/10 Iter  2750/12500]: train_loss = 0.2976\n",
            "[Epoch 10/10 Iter  3000/12500]: train_loss = 0.3404\n",
            "[Epoch 10/10 Iter  3250/12500]: train_loss = 0.3063\n",
            "[Epoch 10/10 Iter  3500/12500]: train_loss = 0.3096\n",
            "[Epoch 10/10 Iter  3750/12500]: train_loss = 0.3359\n",
            "[Epoch 10/10 Iter  4000/12500]: train_loss = 0.3173\n",
            "[Epoch 10/10 Iter  4250/12500]: train_loss = 0.3061\n",
            "[Epoch 10/10 Iter  4500/12500]: train_loss = 0.3332\n",
            "[Epoch 10/10 Iter  4750/12500]: train_loss = 0.3407\n",
            "[Epoch 10/10 Iter  5000/12500]: train_loss = 0.3395\n",
            "[Epoch 10/10 Iter  5250/12500]: train_loss = 0.3115\n",
            "[Epoch 10/10 Iter  5500/12500]: train_loss = 0.2841\n",
            "[Epoch 10/10 Iter  5750/12500]: train_loss = 0.3138\n",
            "[Epoch 10/10 Iter  6000/12500]: train_loss = 0.2965\n",
            "[Epoch 10/10 Iter  6250/12500]: train_loss = 0.3108\n",
            "[Epoch 10/10 Iter  6500/12500]: train_loss = 0.2948\n",
            "[Epoch 10/10 Iter  6750/12500]: train_loss = 0.2808\n",
            "[Epoch 10/10 Iter  7000/12500]: train_loss = 0.2845\n",
            "[Epoch 10/10 Iter  7250/12500]: train_loss = 0.3510\n",
            "[Epoch 10/10 Iter  7500/12500]: train_loss = 0.3181\n",
            "[Epoch 10/10 Iter  7750/12500]: train_loss = 0.3000\n",
            "[Epoch 10/10 Iter  8000/12500]: train_loss = 0.2844\n",
            "[Epoch 10/10 Iter  8250/12500]: train_loss = 0.2599\n",
            "[Epoch 10/10 Iter  8500/12500]: train_loss = 0.3174\n",
            "[Epoch 10/10 Iter  8750/12500]: train_loss = 0.3232\n",
            "[Epoch 10/10 Iter  9000/12500]: train_loss = 0.2905\n",
            "[Epoch 10/10 Iter  9250/12500]: train_loss = 0.3043\n",
            "[Epoch 10/10 Iter  9500/12500]: train_loss = 0.3093\n",
            "[Epoch 10/10 Iter  9750/12500]: train_loss = 0.2895\n",
            "[Epoch 10/10 Iter 10000/12500]: train_loss = 0.2878\n",
            "[Epoch 10/10 Iter 10250/12500]: train_loss = 0.3147\n",
            "[Epoch 10/10 Iter 10500/12500]: train_loss = 0.3122\n",
            "[Epoch 10/10 Iter 10750/12500]: train_loss = 0.2914\n",
            "[Epoch 10/10 Iter 11000/12500]: train_loss = 0.3345\n",
            "[Epoch 10/10 Iter 11250/12500]: train_loss = 0.3185\n",
            "[Epoch 10/10 Iter 11500/12500]: train_loss = 0.2796\n",
            "[Epoch 10/10 Iter 11750/12500]: train_loss = 0.3134\n",
            "[Epoch 10/10 Iter 12000/12500]: train_loss = 0.2968\n",
            "[Epoch 10/10 Iter 12250/12500]: train_loss = 0.2861\n",
            "[Epoch 10/10 Iter 12500/12500]: train_loss = 0.2682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "6x_IW_FbBp6o",
        "outputId": "6d050023-148b-412a-9825-347b28fcc20a"
      },
      "source": [
        "evaluate(model, testloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-25aa4d2ddc41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-331498f627f8>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, testloader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# # disable gradient computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-d8403c6e0105>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2282\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument weight in method wrapper_cudnn_batch_norm)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_OiHk1wW2ov"
      },
      "source": [
        "## 6. Performance of Each Model\n",
        "*Might use Tensorboard for Visualization*\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 Score\n",
        "* Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjm0EEtNW7o6"
      },
      "source": [
        "## 7. Visualize CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b7Y2nViXHu4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}