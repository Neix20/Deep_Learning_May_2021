{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Test Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eBIr8b5aqwG"
      },
      "source": [
        "## Mount Notebook to Google Colab Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VraP7JTaqBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978418ec-1114-4634-d4e9-a46f93a74d30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVuKkMuaufv"
      },
      "source": [
        "## Change Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBcBdPdjapwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6edd4f8e-dd1d-41d4-f73c-1da75122c473"
      },
      "source": [
        "cd /content/drive/MyDrive/Deep_Learning_Assignment_May_2021/Deep_Learning_Assignment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Deep_Learning_Assignment_May_2021/Deep_Learning_Assignment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABULw_tbIZx"
      },
      "source": [
        "## Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a379duUmbME2"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import torch, torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdTNzG70VbSF"
      },
      "source": [
        "## Create Dataset Classes\n",
        "This is to transform any dataset we import into a class that can be processed by our model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhw6GdLjb9Xl"
      },
      "source": [
        "### 1. Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCthiKoIcDFk"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "class FashionMNIST(Dataset):\n",
        "    \n",
        "    def __init__(self, root = \".\", train = True, download=False, transform=None, num_samples = None):\n",
        "        self.classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "        self.num_classes = len(self.classes)            \n",
        "        self.__dataset = torchvision.datasets.FashionMNIST(root=root, train=train, download=download, transform=transform)\n",
        "        self.__num_samples = min(num_samples, len(self.__dataset)) if num_samples is not None else len(self.__dataset)\n",
        "                       \n",
        "    def __len__(self):\n",
        "        return self.__num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):            \n",
        "        return self.__dataset[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAfwYegQuK4w"
      },
      "source": [
        "### 2. Fashion Product Small Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SSDMJ90uNLT"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "# We try to work on other fashion dataset but\n",
        "# it take too long for collab to process the dataset\n",
        "\n",
        "class FashionProductSmallDataset(Dataset):\n",
        "    def __init__(self, csv_name, image_path, transform=None):\n",
        "        df = pd.read_csv(csv_name)\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted(df.articleType.unique())\n",
        "        self.transform = transform\n",
        "\n",
        "        for class_id, cls in enumerate(self.classes):\n",
        "            cls_folder = os.path.join(image_path, cls)\n",
        "            for img_name in os.listdir(cls_folder):\n",
        "                self.data.append(os.path.join(cls_folder, img_name))\n",
        "                self.labels.append(class_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.data[idx])\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJV8EjTkumus"
      },
      "source": [
        "## Fetch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRoFERXDui8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020eef1b-1f41-4955-f8d7-d456032868f4"
      },
      "source": [
        "dataset = FashionMNIST(root=\".\", download= True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CONb7SLFxboa"
      },
      "source": [
        "## Original Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDAinoxbxgUQ"
      },
      "source": [
        "def show_image(img):\n",
        "    plt.imshow (img, cmap = matplotlib.cm.gray, interpolation = 'nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztr9vgdtxjbR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ddd505e2-0788-4ad0-9f14-d13430236403"
      },
      "source": [
        "idx = 10\n",
        "img, label = dataset[idx]\n",
        "\n",
        "show_image(img)\n",
        "print(img.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KL0PIYWwcc_"
      },
      "source": [
        "## Transformation Pipeline\n",
        "* Resize image from (28,28) to Standard Image Size (32, 32)\n",
        "* Convert Tensor Pil Image to Tensor\n",
        "* Normalize Image to Tensor Float\n",
        "* _Data Augmentation (Extra): Random Crop_\n",
        "* _Data Augmentation (Extra): Random Horizontal Flip_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQLqrvQ8HjHI"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "fashion_MNIST_transform= transforms.Compose([ \n",
        "    transforms.Resize(32),       \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (1.0,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeOM569TvPSx"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "fashion_product_small_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda img: ImageOps.grayscale(img)),                                              \n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomCrop(28),                            \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (1.0,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLml6MTUkM6p"
      },
      "source": [
        "## Load Dataset, Split Dataset, Batch Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eZ97TD0vxh8"
      },
      "source": [
        "### Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmGhPPBw4YSk"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "## Split dataset into trainset and test set\n",
        "\n",
        "trainset = FashionMNIST(root=\".\",train=True, download= True, transform = fashion_MNIST_transform)\n",
        "testset = FashionMNIST(root=\".\",train=False, download= True, transform = fashion_MNIST_transform)\n",
        "\n",
        "## BatchLoader\n",
        "trainloader = DataLoader(trainset, batch_size = 20, shuffle = True, num_workers = 2)\n",
        "testloader = DataLoader(testset, batch_size = 20, shuffle = True, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnzNDOqcmlk0"
      },
      "source": [
        "### Fashion Product Small Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1nLPjbomoki"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "# csv_name = \"./fashion_product_small_img/fashion_image_small.csv\"\n",
        "# image_path = \"./fashion_product_small_img/kaggle_fashion_small/\"\n",
        "# dataset = FashionProductSmallDataset(csv_name, image_path, transform = fashion_product_small_transform)\n",
        "\n",
        "# ## Split Dataset into Train Set and Testing Set\n",
        "# x = math.ceil(len(dataset) * 0.8)\n",
        "# y = len(dataset) - x\n",
        "# trainset, testset = torch.utils.data.random_split(dataset, [x, y])\n",
        "\n",
        "# ## BatchLoader\n",
        "# trainloader = DataLoader(trainset, batch_size = 4, shuffle = True, num_workers = 2)\n",
        "# testloader = DataLoader(testset, batch_size = 4, shuffle = True, num_workers = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwj4Ef6uyEyf"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zWByYmbZHN2"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "file_dir = \"Models\"\n",
        "num_of_class = len(trainset.classes)\n",
        "num_epoch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlrUXLWV7xST"
      },
      "source": [
        "# Train-Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQXNJJFJ5cNn"
      },
      "source": [
        "## Train Function\n",
        "\n",
        "#### Train Pipeline:\n",
        "\n",
        "* Optimizer => Used to update parameters after forward-backward Propagation\n",
        "* Loss Function => Used to update the direction of the gradient\n",
        "* The following train Model uses Gradient Descent algorithm. The gradient descent algorithm has the following pipeline.\n",
        "  * Forward Propagation\n",
        "  * Update Gradient to backwards\n",
        "  * Backward Propagation\n",
        "* For each epoch, the training loss and the test accuracy is recorded and stored for analysis purpose.\n",
        "\n",
        "The save Function Records:\n",
        "* Model Parameters\n",
        "* Optimizer Parameters\n",
        "* Last Epoch\n",
        "* Train Loss for Each Epoch\n",
        "* Test Accuracy for Each Epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5GcNXrD8SOI"
      },
      "source": [
        "##################################################\n",
        "# code by Lee Hao Jie and Tan Xi En  #\n",
        "##################################################\n",
        "# This train function will save model based on file_dir and file_name\n",
        "# determine by user and save the train loss and test accuracy in .pt file \n",
        "\n",
        "def train(model, trainloader, testloader, optimizer, file_dir, file_name, device, num_epoch=25, loss_iter=250, verbose=True):\n",
        "\n",
        "    history_train_loss, history_train_accuracy = [], []\n",
        "    loss_iterations = int(np.ceil(len(trainloader)/loss_iter))\n",
        "\n",
        "    # set to training mode\n",
        "    model.train()\n",
        "\n",
        "    # train the network\n",
        "    for e in range(num_epoch):\n",
        "        running_loss, running_count, train_loss = 0.0, 0.0, 0\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outs = model(inputs)\n",
        "            loss = F.cross_entropy(outs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_count += 1\n",
        "            train_loss = 0\n",
        "\n",
        "            if i % loss_iterations == loss_iterations - 1 or i == len(trainloader) - 1:\n",
        "                train_loss = running_loss / running_count\n",
        "                running_loss, running_count = 0.0, 0.0\n",
        "                if verbose:\n",
        "                    print(f'[Epoch {e+1:2d}/{num_epoch:d} Iter {i+1:5d}/{len(trainloader)}]: train_loss = {train_loss:.4f}')\n",
        "\n",
        "        # Train Loss for Each Epoch\n",
        "        history_train_loss.append(train_loss)\n",
        "\n",
        "        # Train Accuracy for Each Epoch\n",
        "        running_corrects = 0\n",
        "        for inputs, targets in testloader:\n",
        "\n",
        "            # transfer to the GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # disable gradient computation\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                running_corrects += (targets == predicted).double().sum()\n",
        "\n",
        "        history_train_accuracy.append(\n",
        "            running_corrects/len(testloader.dataset)*100.0)\n",
        "\n",
        "        # save the model(done by Tan Xi En)\n",
        "        checkpoint_file = f'./{file_dir}/{file_name}.pt'\n",
        "        torch.save({\n",
        "          \"epoch\": num_epoch,\n",
        "          \"model_state_dict\": model.state_dict(),\n",
        "          \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "          \"history_train_loss\": history_train_loss,\n",
        "          \"history_train_accuracy\": history_train_accuracy\n",
        "        }, checkpoint_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PI0kirDHbfm"
      },
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXk44bLkHfjf"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "def evaluate(model, testloader, device):\n",
        "\n",
        "    # set to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # running_correct\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Repeat for all batch data in the test set\n",
        "    for inputs, targets in testloader:\n",
        "\n",
        "        # transfer to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # # disable gradient computation\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            running_corrects += (targets == predicted).double().sum()\n",
        "\n",
        "    return 100*running_corrects/len(testloader.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K0oscT0Wz7E"
      },
      "source": [
        "## 3. LeNet Model (Done by Yikanin Yit)\n",
        "\n",
        "![LeNet Model](https://drive.google.com/uc?id=19rKcKXQ2S6k7xbMWFumWCYnYyRbcGskH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB7rW31AsjA1"
      },
      "source": [
        "###############################\n",
        "# code by Yikanin Yit  #\n",
        "###############################\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # define layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5,stride=1)\n",
        "    self.pool1 = nn.AvgPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5,stride=1)\n",
        "    self.pool2 = nn.AvgPool2d(2,2)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=400, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.out = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "  # define forward function\n",
        "  def forward(self, t):\n",
        "    # conv 1\n",
        "    t = torch.tanh(self.conv1(t))\n",
        "    t = torch.tanh(self.pool1(t))\n",
        "\n",
        "    # conv 2\n",
        "    t = torch.tanh(self.conv2(t))\n",
        "    t = torch.tanh(self.pool2(t))\n",
        "\n",
        "    # fc1\n",
        "    t = t.view(-1,400)\n",
        "    t = torch.tanh(self.fc1(t))\n",
        "\n",
        "    # fc2\n",
        "    t = torch.tanh(self.fc2(t))\n",
        "    \n",
        "    # output\n",
        "    t = self.out(t)\n",
        "    # don't need softmax here since we'll use cross-entropy as activation.\n",
        "\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxZYVJKyiLHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27693fd3-68a2-4033-fe8c-0f83590fc56b"
      },
      "source": [
        "###############################\n",
        "# code by Yikanin Yit  #\n",
        "###############################\n",
        "\n",
        "model1 = LeNet5()\n",
        "print(model1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (out): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymIvdIk9yIWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2859e5a2-e0b1-4427-aea0-9bf582b7e8a9"
      },
      "source": [
        "###############################\n",
        "# code by Yikanin Yit  #\n",
        "###############################\n",
        "\n",
        "summary(model1, (1, 32, 32), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "         AvgPool2d-2            [-1, 6, 14, 14]               0\n",
            "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
            "         AvgPool2d-4             [-1, 16, 5, 5]               0\n",
            "            Linear-5                  [-1, 120]          48,120\n",
            "            Linear-6                   [-1, 84]          10,164\n",
            "            Linear-7                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l3FqfB1pqos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a851e931-0afc-479f-e539-4e438498776d"
      },
      "source": [
        "###############################\n",
        "# code by Yikanin Yit  #\n",
        "###############################\n",
        "\n",
        "#train this take around an half hour\n",
        "model1 = model1.to(device)\n",
        "optimizer1 = optim.SGD(model1.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "file_name = \"Model_1\"\n",
        "train(model1, trainloader, testloader, optimizer1, file_dir, file_name, device, num_epoch = 25, loss_iter = 10, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/25 Iter   300/3000]: train_loss = 2.2242\n",
            "[Epoch  1/25 Iter   600/3000]: train_loss = 1.6008\n",
            "[Epoch  1/25 Iter   900/3000]: train_loss = 1.1960\n",
            "[Epoch  1/25 Iter  1200/3000]: train_loss = 1.0166\n",
            "[Epoch  1/25 Iter  1500/3000]: train_loss = 0.8946\n",
            "[Epoch  1/25 Iter  1800/3000]: train_loss = 0.8049\n",
            "[Epoch  1/25 Iter  2100/3000]: train_loss = 0.7632\n",
            "[Epoch  1/25 Iter  2400/3000]: train_loss = 0.7348\n",
            "[Epoch  1/25 Iter  2700/3000]: train_loss = 0.7028\n",
            "[Epoch  1/25 Iter  3000/3000]: train_loss = 0.6709\n",
            "[Epoch  2/25 Iter   300/3000]: train_loss = 0.6327\n",
            "[Epoch  2/25 Iter   600/3000]: train_loss = 0.6597\n",
            "[Epoch  2/25 Iter   900/3000]: train_loss = 0.6239\n",
            "[Epoch  2/25 Iter  1200/3000]: train_loss = 0.6059\n",
            "[Epoch  2/25 Iter  1500/3000]: train_loss = 0.5914\n",
            "[Epoch  2/25 Iter  1800/3000]: train_loss = 0.5708\n",
            "[Epoch  2/25 Iter  2100/3000]: train_loss = 0.5627\n",
            "[Epoch  2/25 Iter  2400/3000]: train_loss = 0.5730\n",
            "[Epoch  2/25 Iter  2700/3000]: train_loss = 0.5704\n",
            "[Epoch  2/25 Iter  3000/3000]: train_loss = 0.5499\n",
            "[Epoch  3/25 Iter   300/3000]: train_loss = 0.5400\n",
            "[Epoch  3/25 Iter   600/3000]: train_loss = 0.5272\n",
            "[Epoch  3/25 Iter   900/3000]: train_loss = 0.5134\n",
            "[Epoch  3/25 Iter  1200/3000]: train_loss = 0.5194\n",
            "[Epoch  3/25 Iter  1500/3000]: train_loss = 0.5076\n",
            "[Epoch  3/25 Iter  1800/3000]: train_loss = 0.5178\n",
            "[Epoch  3/25 Iter  2100/3000]: train_loss = 0.4894\n",
            "[Epoch  3/25 Iter  2400/3000]: train_loss = 0.4956\n",
            "[Epoch  3/25 Iter  2700/3000]: train_loss = 0.4846\n",
            "[Epoch  3/25 Iter  3000/3000]: train_loss = 0.4991\n",
            "[Epoch  4/25 Iter   300/3000]: train_loss = 0.4864\n",
            "[Epoch  4/25 Iter   600/3000]: train_loss = 0.4835\n",
            "[Epoch  4/25 Iter   900/3000]: train_loss = 0.4623\n",
            "[Epoch  4/25 Iter  1200/3000]: train_loss = 0.4775\n",
            "[Epoch  4/25 Iter  1500/3000]: train_loss = 0.4648\n",
            "[Epoch  4/25 Iter  1800/3000]: train_loss = 0.4722\n",
            "[Epoch  4/25 Iter  2100/3000]: train_loss = 0.4553\n",
            "[Epoch  4/25 Iter  2400/3000]: train_loss = 0.4575\n",
            "[Epoch  4/25 Iter  2700/3000]: train_loss = 0.4343\n",
            "[Epoch  4/25 Iter  3000/3000]: train_loss = 0.4271\n",
            "[Epoch  5/25 Iter   300/3000]: train_loss = 0.4457\n",
            "[Epoch  5/25 Iter   600/3000]: train_loss = 0.4488\n",
            "[Epoch  5/25 Iter   900/3000]: train_loss = 0.4547\n",
            "[Epoch  5/25 Iter  1200/3000]: train_loss = 0.4276\n",
            "[Epoch  5/25 Iter  1500/3000]: train_loss = 0.4358\n",
            "[Epoch  5/25 Iter  1800/3000]: train_loss = 0.4150\n",
            "[Epoch  5/25 Iter  2100/3000]: train_loss = 0.4255\n",
            "[Epoch  5/25 Iter  2400/3000]: train_loss = 0.4192\n",
            "[Epoch  5/25 Iter  2700/3000]: train_loss = 0.4110\n",
            "[Epoch  5/25 Iter  3000/3000]: train_loss = 0.4201\n",
            "[Epoch  6/25 Iter   300/3000]: train_loss = 0.4203\n",
            "[Epoch  6/25 Iter   600/3000]: train_loss = 0.4270\n",
            "[Epoch  6/25 Iter   900/3000]: train_loss = 0.4086\n",
            "[Epoch  6/25 Iter  1200/3000]: train_loss = 0.4022\n",
            "[Epoch  6/25 Iter  1500/3000]: train_loss = 0.4214\n",
            "[Epoch  6/25 Iter  1800/3000]: train_loss = 0.4133\n",
            "[Epoch  6/25 Iter  2100/3000]: train_loss = 0.3971\n",
            "[Epoch  6/25 Iter  2400/3000]: train_loss = 0.3974\n",
            "[Epoch  6/25 Iter  2700/3000]: train_loss = 0.3909\n",
            "[Epoch  6/25 Iter  3000/3000]: train_loss = 0.3965\n",
            "[Epoch  7/25 Iter   300/3000]: train_loss = 0.4028\n",
            "[Epoch  7/25 Iter   600/3000]: train_loss = 0.3896\n",
            "[Epoch  7/25 Iter   900/3000]: train_loss = 0.4078\n",
            "[Epoch  7/25 Iter  1200/3000]: train_loss = 0.3895\n",
            "[Epoch  7/25 Iter  1500/3000]: train_loss = 0.3803\n",
            "[Epoch  7/25 Iter  1800/3000]: train_loss = 0.3865\n",
            "[Epoch  7/25 Iter  2100/3000]: train_loss = 0.3842\n",
            "[Epoch  7/25 Iter  2400/3000]: train_loss = 0.3783\n",
            "[Epoch  7/25 Iter  2700/3000]: train_loss = 0.3899\n",
            "[Epoch  7/25 Iter  3000/3000]: train_loss = 0.3716\n",
            "[Epoch  8/25 Iter   300/3000]: train_loss = 0.3620\n",
            "[Epoch  8/25 Iter   600/3000]: train_loss = 0.3920\n",
            "[Epoch  8/25 Iter   900/3000]: train_loss = 0.3821\n",
            "[Epoch  8/25 Iter  1200/3000]: train_loss = 0.3704\n",
            "[Epoch  8/25 Iter  1500/3000]: train_loss = 0.3669\n",
            "[Epoch  8/25 Iter  1800/3000]: train_loss = 0.3670\n",
            "[Epoch  8/25 Iter  2100/3000]: train_loss = 0.3643\n",
            "[Epoch  8/25 Iter  2400/3000]: train_loss = 0.3720\n",
            "[Epoch  8/25 Iter  2700/3000]: train_loss = 0.3722\n",
            "[Epoch  8/25 Iter  3000/3000]: train_loss = 0.3776\n",
            "[Epoch  9/25 Iter   300/3000]: train_loss = 0.3802\n",
            "[Epoch  9/25 Iter   600/3000]: train_loss = 0.3578\n",
            "[Epoch  9/25 Iter   900/3000]: train_loss = 0.3529\n",
            "[Epoch  9/25 Iter  1200/3000]: train_loss = 0.3657\n",
            "[Epoch  9/25 Iter  1500/3000]: train_loss = 0.3518\n",
            "[Epoch  9/25 Iter  1800/3000]: train_loss = 0.3602\n",
            "[Epoch  9/25 Iter  2100/3000]: train_loss = 0.3654\n",
            "[Epoch  9/25 Iter  2400/3000]: train_loss = 0.3680\n",
            "[Epoch  9/25 Iter  2700/3000]: train_loss = 0.3495\n",
            "[Epoch  9/25 Iter  3000/3000]: train_loss = 0.3426\n",
            "[Epoch 10/25 Iter   300/3000]: train_loss = 0.3554\n",
            "[Epoch 10/25 Iter   600/3000]: train_loss = 0.3466\n",
            "[Epoch 10/25 Iter   900/3000]: train_loss = 0.3455\n",
            "[Epoch 10/25 Iter  1200/3000]: train_loss = 0.3580\n",
            "[Epoch 10/25 Iter  1500/3000]: train_loss = 0.3502\n",
            "[Epoch 10/25 Iter  1800/3000]: train_loss = 0.3424\n",
            "[Epoch 10/25 Iter  2100/3000]: train_loss = 0.3274\n",
            "[Epoch 10/25 Iter  2400/3000]: train_loss = 0.3483\n",
            "[Epoch 10/25 Iter  2700/3000]: train_loss = 0.3578\n",
            "[Epoch 10/25 Iter  3000/3000]: train_loss = 0.3539\n",
            "[Epoch 11/25 Iter   300/3000]: train_loss = 0.3348\n",
            "[Epoch 11/25 Iter   600/3000]: train_loss = 0.3439\n",
            "[Epoch 11/25 Iter   900/3000]: train_loss = 0.3538\n",
            "[Epoch 11/25 Iter  1200/3000]: train_loss = 0.3395\n",
            "[Epoch 11/25 Iter  1500/3000]: train_loss = 0.3113\n",
            "[Epoch 11/25 Iter  1800/3000]: train_loss = 0.3566\n",
            "[Epoch 11/25 Iter  2100/3000]: train_loss = 0.3293\n",
            "[Epoch 11/25 Iter  2400/3000]: train_loss = 0.3408\n",
            "[Epoch 11/25 Iter  2700/3000]: train_loss = 0.3263\n",
            "[Epoch 11/25 Iter  3000/3000]: train_loss = 0.3501\n",
            "[Epoch 12/25 Iter   300/3000]: train_loss = 0.3308\n",
            "[Epoch 12/25 Iter   600/3000]: train_loss = 0.3450\n",
            "[Epoch 12/25 Iter   900/3000]: train_loss = 0.3396\n",
            "[Epoch 12/25 Iter  1200/3000]: train_loss = 0.3149\n",
            "[Epoch 12/25 Iter  1500/3000]: train_loss = 0.3359\n",
            "[Epoch 12/25 Iter  1800/3000]: train_loss = 0.3348\n",
            "[Epoch 12/25 Iter  2100/3000]: train_loss = 0.3204\n",
            "[Epoch 12/25 Iter  2400/3000]: train_loss = 0.3096\n",
            "[Epoch 12/25 Iter  2700/3000]: train_loss = 0.3324\n",
            "[Epoch 12/25 Iter  3000/3000]: train_loss = 0.3318\n",
            "[Epoch 13/25 Iter   300/3000]: train_loss = 0.3039\n",
            "[Epoch 13/25 Iter   600/3000]: train_loss = 0.3274\n",
            "[Epoch 13/25 Iter   900/3000]: train_loss = 0.3314\n",
            "[Epoch 13/25 Iter  1200/3000]: train_loss = 0.3319\n",
            "[Epoch 13/25 Iter  1500/3000]: train_loss = 0.3368\n",
            "[Epoch 13/25 Iter  1800/3000]: train_loss = 0.3074\n",
            "[Epoch 13/25 Iter  2100/3000]: train_loss = 0.2968\n",
            "[Epoch 13/25 Iter  2400/3000]: train_loss = 0.3174\n",
            "[Epoch 13/25 Iter  2700/3000]: train_loss = 0.3361\n",
            "[Epoch 13/25 Iter  3000/3000]: train_loss = 0.3308\n",
            "[Epoch 14/25 Iter   300/3000]: train_loss = 0.3173\n",
            "[Epoch 14/25 Iter   600/3000]: train_loss = 0.3153\n",
            "[Epoch 14/25 Iter   900/3000]: train_loss = 0.2983\n",
            "[Epoch 14/25 Iter  1200/3000]: train_loss = 0.3145\n",
            "[Epoch 14/25 Iter  1500/3000]: train_loss = 0.3122\n",
            "[Epoch 14/25 Iter  1800/3000]: train_loss = 0.2984\n",
            "[Epoch 14/25 Iter  2100/3000]: train_loss = 0.3297\n",
            "[Epoch 14/25 Iter  2400/3000]: train_loss = 0.3264\n",
            "[Epoch 14/25 Iter  2700/3000]: train_loss = 0.3196\n",
            "[Epoch 14/25 Iter  3000/3000]: train_loss = 0.3261\n",
            "[Epoch 15/25 Iter   300/3000]: train_loss = 0.3129\n",
            "[Epoch 15/25 Iter   600/3000]: train_loss = 0.3113\n",
            "[Epoch 15/25 Iter   900/3000]: train_loss = 0.3158\n",
            "[Epoch 15/25 Iter  1200/3000]: train_loss = 0.3036\n",
            "[Epoch 15/25 Iter  1500/3000]: train_loss = 0.3079\n",
            "[Epoch 15/25 Iter  1800/3000]: train_loss = 0.3185\n",
            "[Epoch 15/25 Iter  2100/3000]: train_loss = 0.3097\n",
            "[Epoch 15/25 Iter  2400/3000]: train_loss = 0.3045\n",
            "[Epoch 15/25 Iter  2700/3000]: train_loss = 0.2910\n",
            "[Epoch 15/25 Iter  3000/3000]: train_loss = 0.3074\n",
            "[Epoch 16/25 Iter   300/3000]: train_loss = 0.3075\n",
            "[Epoch 16/25 Iter   600/3000]: train_loss = 0.3211\n",
            "[Epoch 16/25 Iter   900/3000]: train_loss = 0.3074\n",
            "[Epoch 16/25 Iter  1200/3000]: train_loss = 0.2892\n",
            "[Epoch 16/25 Iter  1500/3000]: train_loss = 0.3104\n",
            "[Epoch 16/25 Iter  1800/3000]: train_loss = 0.2943\n",
            "[Epoch 16/25 Iter  2100/3000]: train_loss = 0.2937\n",
            "[Epoch 16/25 Iter  2400/3000]: train_loss = 0.2945\n",
            "[Epoch 16/25 Iter  2700/3000]: train_loss = 0.3056\n",
            "[Epoch 16/25 Iter  3000/3000]: train_loss = 0.2949\n",
            "[Epoch 17/25 Iter   300/3000]: train_loss = 0.2938\n",
            "[Epoch 17/25 Iter   600/3000]: train_loss = 0.2925\n",
            "[Epoch 17/25 Iter   900/3000]: train_loss = 0.3004\n",
            "[Epoch 17/25 Iter  1200/3000]: train_loss = 0.2808\n",
            "[Epoch 17/25 Iter  1500/3000]: train_loss = 0.2930\n",
            "[Epoch 17/25 Iter  1800/3000]: train_loss = 0.2857\n",
            "[Epoch 17/25 Iter  2100/3000]: train_loss = 0.3063\n",
            "[Epoch 17/25 Iter  2400/3000]: train_loss = 0.3041\n",
            "[Epoch 17/25 Iter  2700/3000]: train_loss = 0.3131\n",
            "[Epoch 17/25 Iter  3000/3000]: train_loss = 0.3080\n",
            "[Epoch 18/25 Iter   300/3000]: train_loss = 0.2982\n",
            "[Epoch 18/25 Iter   600/3000]: train_loss = 0.2925\n",
            "[Epoch 18/25 Iter   900/3000]: train_loss = 0.2870\n",
            "[Epoch 18/25 Iter  1200/3000]: train_loss = 0.2788\n",
            "[Epoch 18/25 Iter  1500/3000]: train_loss = 0.2856\n",
            "[Epoch 18/25 Iter  1800/3000]: train_loss = 0.3010\n",
            "[Epoch 18/25 Iter  2100/3000]: train_loss = 0.2939\n",
            "[Epoch 18/25 Iter  2400/3000]: train_loss = 0.2964\n",
            "[Epoch 18/25 Iter  2700/3000]: train_loss = 0.2934\n",
            "[Epoch 18/25 Iter  3000/3000]: train_loss = 0.2849\n",
            "[Epoch 19/25 Iter   300/3000]: train_loss = 0.2739\n",
            "[Epoch 19/25 Iter   600/3000]: train_loss = 0.2804\n",
            "[Epoch 19/25 Iter   900/3000]: train_loss = 0.2920\n",
            "[Epoch 19/25 Iter  1200/3000]: train_loss = 0.3014\n",
            "[Epoch 19/25 Iter  1500/3000]: train_loss = 0.2713\n",
            "[Epoch 19/25 Iter  1800/3000]: train_loss = 0.2878\n",
            "[Epoch 19/25 Iter  2100/3000]: train_loss = 0.2860\n",
            "[Epoch 19/25 Iter  2400/3000]: train_loss = 0.2932\n",
            "[Epoch 19/25 Iter  2700/3000]: train_loss = 0.2888\n",
            "[Epoch 19/25 Iter  3000/3000]: train_loss = 0.2831\n",
            "[Epoch 20/25 Iter   300/3000]: train_loss = 0.2766\n",
            "[Epoch 20/25 Iter   600/3000]: train_loss = 0.2646\n",
            "[Epoch 20/25 Iter   900/3000]: train_loss = 0.2827\n",
            "[Epoch 20/25 Iter  1200/3000]: train_loss = 0.2821\n",
            "[Epoch 20/25 Iter  1500/3000]: train_loss = 0.2880\n",
            "[Epoch 20/25 Iter  1800/3000]: train_loss = 0.2836\n",
            "[Epoch 20/25 Iter  2100/3000]: train_loss = 0.2836\n",
            "[Epoch 20/25 Iter  2400/3000]: train_loss = 0.2955\n",
            "[Epoch 20/25 Iter  2700/3000]: train_loss = 0.2765\n",
            "[Epoch 20/25 Iter  3000/3000]: train_loss = 0.2772\n",
            "[Epoch 21/25 Iter   300/3000]: train_loss = 0.2793\n",
            "[Epoch 21/25 Iter   600/3000]: train_loss = 0.2893\n",
            "[Epoch 21/25 Iter   900/3000]: train_loss = 0.2707\n",
            "[Epoch 21/25 Iter  1200/3000]: train_loss = 0.2758\n",
            "[Epoch 21/25 Iter  1500/3000]: train_loss = 0.2884\n",
            "[Epoch 21/25 Iter  1800/3000]: train_loss = 0.2696\n",
            "[Epoch 21/25 Iter  2100/3000]: train_loss = 0.2738\n",
            "[Epoch 21/25 Iter  2400/3000]: train_loss = 0.2749\n",
            "[Epoch 21/25 Iter  2700/3000]: train_loss = 0.2789\n",
            "[Epoch 21/25 Iter  3000/3000]: train_loss = 0.2718\n",
            "[Epoch 22/25 Iter   300/3000]: train_loss = 0.2689\n",
            "[Epoch 22/25 Iter   600/3000]: train_loss = 0.2568\n",
            "[Epoch 22/25 Iter   900/3000]: train_loss = 0.2772\n",
            "[Epoch 22/25 Iter  1200/3000]: train_loss = 0.2680\n",
            "[Epoch 22/25 Iter  1500/3000]: train_loss = 0.2698\n",
            "[Epoch 22/25 Iter  1800/3000]: train_loss = 0.2852\n",
            "[Epoch 22/25 Iter  2100/3000]: train_loss = 0.2720\n",
            "[Epoch 22/25 Iter  2400/3000]: train_loss = 0.2754\n",
            "[Epoch 22/25 Iter  2700/3000]: train_loss = 0.2774\n",
            "[Epoch 22/25 Iter  3000/3000]: train_loss = 0.2750\n",
            "[Epoch 23/25 Iter   300/3000]: train_loss = 0.2713\n",
            "[Epoch 23/25 Iter   600/3000]: train_loss = 0.2708\n",
            "[Epoch 23/25 Iter   900/3000]: train_loss = 0.2666\n",
            "[Epoch 23/25 Iter  1200/3000]: train_loss = 0.2615\n",
            "[Epoch 23/25 Iter  1500/3000]: train_loss = 0.2710\n",
            "[Epoch 23/25 Iter  1800/3000]: train_loss = 0.2802\n",
            "[Epoch 23/25 Iter  2100/3000]: train_loss = 0.2669\n",
            "[Epoch 23/25 Iter  2400/3000]: train_loss = 0.2711\n",
            "[Epoch 23/25 Iter  2700/3000]: train_loss = 0.2570\n",
            "[Epoch 23/25 Iter  3000/3000]: train_loss = 0.2629\n",
            "[Epoch 24/25 Iter   300/3000]: train_loss = 0.2536\n",
            "[Epoch 24/25 Iter   600/3000]: train_loss = 0.2624\n",
            "[Epoch 24/25 Iter   900/3000]: train_loss = 0.2722\n",
            "[Epoch 24/25 Iter  1200/3000]: train_loss = 0.2582\n",
            "[Epoch 24/25 Iter  1500/3000]: train_loss = 0.2579\n",
            "[Epoch 24/25 Iter  1800/3000]: train_loss = 0.2663\n",
            "[Epoch 24/25 Iter  2100/3000]: train_loss = 0.2674\n",
            "[Epoch 24/25 Iter  2400/3000]: train_loss = 0.2620\n",
            "[Epoch 24/25 Iter  2700/3000]: train_loss = 0.2657\n",
            "[Epoch 24/25 Iter  3000/3000]: train_loss = 0.2683\n",
            "[Epoch 25/25 Iter   300/3000]: train_loss = 0.2510\n",
            "[Epoch 25/25 Iter   600/3000]: train_loss = 0.2633\n",
            "[Epoch 25/25 Iter   900/3000]: train_loss = 0.2756\n",
            "[Epoch 25/25 Iter  1200/3000]: train_loss = 0.2628\n",
            "[Epoch 25/25 Iter  1500/3000]: train_loss = 0.2473\n",
            "[Epoch 25/25 Iter  1800/3000]: train_loss = 0.2590\n",
            "[Epoch 25/25 Iter  2100/3000]: train_loss = 0.2709\n",
            "[Epoch 25/25 Iter  2400/3000]: train_loss = 0.2570\n",
            "[Epoch 25/25 Iter  2700/3000]: train_loss = 0.2625\n",
            "[Epoch 25/25 Iter  3000/3000]: train_loss = 0.2421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5SCKLgwprty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83ecbef-82ef-4d9e-81ca-79276e4f6229"
      },
      "source": [
        "###############################\n",
        "# code by Yikanin Yit  #\n",
        "###############################\n",
        "\n",
        "acc_score = evaluate(model1, testloader, device)\n",
        "print(f'Accuracy = {acc_score:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 88.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSDM0lXg5r-Y"
      },
      "source": [
        "## 4. Squeeze Excitation (SE) ResNet Model (Done by Lee Hao Jie)\n",
        "\n",
        "![Squeeze Excitation (SE) ResNet Model](https://drive.google.com/uc?id=1jKqW1JX9ErqomwZkFSIMNhbZoshL25On)\n",
        "\n",
        "Source Link: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQTZam41qHSH"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "def conv_block(in_c,out_c,k=3,s=1):\n",
        "    #3x3 use stride 1 or 7x7 use stride 3\n",
        "    if k==3:\n",
        "      p=1\n",
        "    elif k==5:\n",
        "      p=2\n",
        "    else:\n",
        "      p=0\n",
        "\n",
        "    block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = in_c, out_channels = out_c, kernel_size = k, stride = s, padding = p),\n",
        "        nn.BatchNorm2d(out_c)\n",
        "    )\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObWwcP5M5hgL"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie. Adapted From https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py #\n",
        "###############################\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.adaptive_avg_pool2d(x,1).view(x.size()[0], x.size()[1])\n",
        "        y = self.fc(y).view(x.size()[0], x.size()[1], 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp0GpAtUzCNP"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_c, out_c, downsample=None):\n",
        "        super(Block, self).__init__()\n",
        "        self.downsmaple = downsample\n",
        "        if downsample is None:\n",
        "          kernels=[3,3]\n",
        "          self.conv1 = nn.ModuleList(conv_block(in_c, out_c, k) for k in kernels)\n",
        "          self.se1 = SELayer(out_c,16)\n",
        "          self.conv2 = nn.ModuleList(conv_block(out_c, out_c, k) for k in kernels)\n",
        "          self.se2 = SELayer(out_c,16)\n",
        "        else:\n",
        "          kernels=[3,3]\n",
        "          self.convs2_1=conv_block(in_c, out_c,1,2)\n",
        "          self.convs2_2=conv_block(in_c, out_c, kernels[0],2)\n",
        "          self.conv1 = nn.ModuleList(conv_block(out_c, out_c, k) for k in kernels[1:])\n",
        "          self.se1 = SELayer(out_c,16)\n",
        "          self.conv2 = nn.ModuleList(conv_block(out_c, out_c, k) for k in kernels)\n",
        "          self.se2 = SELayer(out_c,16)\n",
        "       \n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsmaple is None:\n",
        "          residual = x\n",
        "        else:\n",
        "          residual = self.convs2_1(x)\n",
        "          x=self.convs2_2(x)\n",
        "        \n",
        "        for conv in self.conv1 :\n",
        "          x = F.relu(conv(x))\n",
        "        x = self.se1(x)\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "\n",
        "        residual = x\n",
        "        for conv in self.conv2 :\n",
        "          x = F.relu(conv(x))\n",
        "        x = self.se2(x)\n",
        "        x += residual\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQjDibzm5962"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "class SEResnet(nn.Module):\n",
        "    def __init__(self, num_of_class=10):\n",
        "        super().__init__()\n",
        "        self.stem=conv_block(1,64,5,2)\n",
        "        self.block1=Block(64,64)\n",
        "        self.block2=Block(64,128,downsample=True)\n",
        "        self.block3=Block(128,256,downsample=True)\n",
        "        self.fc=nn.Linear(256,num_of_class)\n",
        "    def forward(self, x):\n",
        "        x=F.relu(self.stem(x))\n",
        "        x=self.block1(x)\n",
        "        x=self.block2(x)\n",
        "        x=self.block3(x)\n",
        "        x=F.adaptive_avg_pool2d(x,1)\n",
        "        x=x.view(x.size(0),-1)\n",
        "        x=self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW6_Lz9Q6xO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893df622-85ac-41f8-be7a-c17fd0cd48cc"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "model2 = SEResnet()\n",
        "x=torch.rand(4,1, 32, 32)\n",
        "out=model2(x)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk7jkYSa2tbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73d4bb8e-4837-4c9d-9c47-f6244928b761"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "print(model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SEResnet(\n",
            "  (stem): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block1): Block(\n",
            "    (conv1): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=4, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=4, out_features=64, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (conv2): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se2): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=4, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=4, out_features=64, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (block2): Block(\n",
            "    (convs2_1): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (convs2_2): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=8, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=8, out_features=128, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (conv2): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se2): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=8, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=8, out_features=128, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (block3): Block(\n",
            "    (convs2_1): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (convs2_2): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv1): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se1): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=16, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=256, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "    (conv2): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (se2): SELayer(\n",
            "      (fc): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=16, bias=False)\n",
            "        (1): ReLU(inplace=True)\n",
            "        (2): Linear(in_features=16, out_features=256, bias=False)\n",
            "        (3): Sigmoid()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG7uYDnR6zwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ee1eae-3601-40d4-b18d-f3844327c55f"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "summary(model2, (1, 32, 32), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           1,664\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "            Conv2d-3           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-4           [-1, 64, 16, 16]             128\n",
            "            Conv2d-5           [-1, 64, 16, 16]          36,928\n",
            "       BatchNorm2d-6           [-1, 64, 16, 16]             128\n",
            "            Linear-7                    [-1, 4]             256\n",
            "              ReLU-8                    [-1, 4]               0\n",
            "            Linear-9                   [-1, 64]             256\n",
            "          Sigmoid-10                   [-1, 64]               0\n",
            "          SELayer-11           [-1, 64, 16, 16]               0\n",
            "           Conv2d-12           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-13           [-1, 64, 16, 16]             128\n",
            "           Conv2d-14           [-1, 64, 16, 16]          36,928\n",
            "      BatchNorm2d-15           [-1, 64, 16, 16]             128\n",
            "           Linear-16                    [-1, 4]             256\n",
            "             ReLU-17                    [-1, 4]               0\n",
            "           Linear-18                   [-1, 64]             256\n",
            "          Sigmoid-19                   [-1, 64]               0\n",
            "          SELayer-20           [-1, 64, 16, 16]               0\n",
            "            Block-21           [-1, 64, 16, 16]               0\n",
            "           Conv2d-22            [-1, 128, 8, 8]           8,320\n",
            "      BatchNorm2d-23            [-1, 128, 8, 8]             256\n",
            "           Conv2d-24            [-1, 128, 8, 8]          73,856\n",
            "      BatchNorm2d-25            [-1, 128, 8, 8]             256\n",
            "           Conv2d-26            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-27            [-1, 128, 8, 8]             256\n",
            "           Linear-28                    [-1, 8]           1,024\n",
            "             ReLU-29                    [-1, 8]               0\n",
            "           Linear-30                  [-1, 128]           1,024\n",
            "          Sigmoid-31                  [-1, 128]               0\n",
            "          SELayer-32            [-1, 128, 8, 8]               0\n",
            "           Conv2d-33            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-34            [-1, 128, 8, 8]             256\n",
            "           Conv2d-35            [-1, 128, 8, 8]         147,584\n",
            "      BatchNorm2d-36            [-1, 128, 8, 8]             256\n",
            "           Linear-37                    [-1, 8]           1,024\n",
            "             ReLU-38                    [-1, 8]               0\n",
            "           Linear-39                  [-1, 128]           1,024\n",
            "          Sigmoid-40                  [-1, 128]               0\n",
            "          SELayer-41            [-1, 128, 8, 8]               0\n",
            "            Block-42            [-1, 128, 8, 8]               0\n",
            "           Conv2d-43            [-1, 256, 4, 4]          33,024\n",
            "      BatchNorm2d-44            [-1, 256, 4, 4]             512\n",
            "           Conv2d-45            [-1, 256, 4, 4]         295,168\n",
            "      BatchNorm2d-46            [-1, 256, 4, 4]             512\n",
            "           Conv2d-47            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-48            [-1, 256, 4, 4]             512\n",
            "           Linear-49                   [-1, 16]           4,096\n",
            "             ReLU-50                   [-1, 16]               0\n",
            "           Linear-51                  [-1, 256]           4,096\n",
            "          Sigmoid-52                  [-1, 256]               0\n",
            "          SELayer-53            [-1, 256, 4, 4]               0\n",
            "           Conv2d-54            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-55            [-1, 256, 4, 4]             512\n",
            "           Conv2d-56            [-1, 256, 4, 4]         590,080\n",
            "      BatchNorm2d-57            [-1, 256, 4, 4]             512\n",
            "           Linear-58                   [-1, 16]           4,096\n",
            "             ReLU-59                   [-1, 16]               0\n",
            "           Linear-60                  [-1, 256]           4,096\n",
            "          Sigmoid-61                  [-1, 256]               0\n",
            "          SELayer-62            [-1, 256, 4, 4]               0\n",
            "            Block-63            [-1, 256, 4, 4]               0\n",
            "           Linear-64                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 2,801,290\n",
            "Trainable params: 2,801,290\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.86\n",
            "Params size (MB): 10.69\n",
            "Estimated Total Size (MB): 13.55\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y5Epdgd62Im",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74bbc0b4-574b-423a-f8c1-df046f5cce20"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "#train this take around one an half hour\n",
        "model2 = SEResnet(num_of_class)\n",
        "model2 = model2.to(device)\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "file_name = \"Model_2\"\n",
        "train(model2, trainloader, testloader, optimizer2, file_dir, file_name, device, num_epoch = 25, loss_iter = 10, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/25 Iter   300/3000]: train_loss = 1.1385\n",
            "[Epoch  1/25 Iter   600/3000]: train_loss = 0.6253\n",
            "[Epoch  1/25 Iter   900/3000]: train_loss = 0.5144\n",
            "[Epoch  1/25 Iter  1200/3000]: train_loss = 0.4759\n",
            "[Epoch  1/25 Iter  1500/3000]: train_loss = 0.4466\n",
            "[Epoch  1/25 Iter  1800/3000]: train_loss = 0.4179\n",
            "[Epoch  1/25 Iter  2100/3000]: train_loss = 0.3863\n",
            "[Epoch  1/25 Iter  2400/3000]: train_loss = 0.3872\n",
            "[Epoch  1/25 Iter  2700/3000]: train_loss = 0.3848\n",
            "[Epoch  1/25 Iter  3000/3000]: train_loss = 0.3501\n",
            "[Epoch  2/25 Iter   300/3000]: train_loss = 0.3314\n",
            "[Epoch  2/25 Iter   600/3000]: train_loss = 0.3088\n",
            "[Epoch  2/25 Iter   900/3000]: train_loss = 0.3236\n",
            "[Epoch  2/25 Iter  1200/3000]: train_loss = 0.3335\n",
            "[Epoch  2/25 Iter  1500/3000]: train_loss = 0.3185\n",
            "[Epoch  2/25 Iter  1800/3000]: train_loss = 0.2928\n",
            "[Epoch  2/25 Iter  2100/3000]: train_loss = 0.3057\n",
            "[Epoch  2/25 Iter  2400/3000]: train_loss = 0.2856\n",
            "[Epoch  2/25 Iter  2700/3000]: train_loss = 0.3019\n",
            "[Epoch  2/25 Iter  3000/3000]: train_loss = 0.2890\n",
            "[Epoch  3/25 Iter   300/3000]: train_loss = 0.2525\n",
            "[Epoch  3/25 Iter   600/3000]: train_loss = 0.2582\n",
            "[Epoch  3/25 Iter   900/3000]: train_loss = 0.2485\n",
            "[Epoch  3/25 Iter  1200/3000]: train_loss = 0.2588\n",
            "[Epoch  3/25 Iter  1500/3000]: train_loss = 0.2581\n",
            "[Epoch  3/25 Iter  1800/3000]: train_loss = 0.2491\n",
            "[Epoch  3/25 Iter  2100/3000]: train_loss = 0.2433\n",
            "[Epoch  3/25 Iter  2400/3000]: train_loss = 0.2640\n",
            "[Epoch  3/25 Iter  2700/3000]: train_loss = 0.2486\n",
            "[Epoch  3/25 Iter  3000/3000]: train_loss = 0.2580\n",
            "[Epoch  4/25 Iter   300/3000]: train_loss = 0.2206\n",
            "[Epoch  4/25 Iter   600/3000]: train_loss = 0.2305\n",
            "[Epoch  4/25 Iter   900/3000]: train_loss = 0.2201\n",
            "[Epoch  4/25 Iter  1200/3000]: train_loss = 0.2104\n",
            "[Epoch  4/25 Iter  1500/3000]: train_loss = 0.2168\n",
            "[Epoch  4/25 Iter  1800/3000]: train_loss = 0.2112\n",
            "[Epoch  4/25 Iter  2100/3000]: train_loss = 0.2164\n",
            "[Epoch  4/25 Iter  2400/3000]: train_loss = 0.2036\n",
            "[Epoch  4/25 Iter  2700/3000]: train_loss = 0.2195\n",
            "[Epoch  4/25 Iter  3000/3000]: train_loss = 0.2261\n",
            "[Epoch  5/25 Iter   300/3000]: train_loss = 0.1764\n",
            "[Epoch  5/25 Iter   600/3000]: train_loss = 0.1772\n",
            "[Epoch  5/25 Iter   900/3000]: train_loss = 0.1968\n",
            "[Epoch  5/25 Iter  1200/3000]: train_loss = 0.1795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-3719cffe1854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Model_2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-5d0497af3ea9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader, testloader, optimizer, file_dir, file_name, device, num_epoch, loss_iter, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HJhsrPD653A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfd980e-29cd-4b6e-cd61-fbb6486d2633"
      },
      "source": [
        "###############################\n",
        "# code by Lee Hao Jie  #\n",
        "###############################\n",
        "\n",
        "acc_score = evaluate(model2, testloader, device)\n",
        "print(f'Accuracy = {acc_score:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 91.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VY8z7KIAYwJ"
      },
      "source": [
        "## 5. CNN + BN + Skip Connection Model (Done by Tan Xi En)\n",
        "\n",
        "![CNN + BN + Residual Skip Connection Model](https://drive.google.com/uc?id=1gWl9-rUJlzAcGFaq1nndNRr3X7yLto52)\n",
        "\n",
        "Notes: `k`: number of filters, `f`: filter or kernel size, `s`: stride, `p`: padding, `o`: output shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8H46ixIovk7"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "def build_block_cnn(in_channels, out_channels):\n",
        "  block = nn.Sequential(\n",
        "    nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "    nn.ReLU()\n",
        "  )\n",
        "  return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I-VznUgAcw0"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "class cnn_model3(nn.Module):\n",
        "    def __init__(self, num_of_class):\n",
        "        super().__init__()\n",
        "\n",
        "        channel_size = [1, 32, 32]\n",
        "        layer_size = [32 * 8 * 8, 128, num_of_class]\n",
        "        p = 0.25\n",
        "\n",
        "        # define blocks\n",
        "        self.conv_block = nn.ModuleList(\n",
        "            build_block_cnn(in_channel, out_channel) for(in_channel, out_channel) in zip(channel_size[0:-1], channel_size[1:])\n",
        "        )\n",
        "\n",
        "        # define Fully Connected Layers\n",
        "        self.fc = nn.ModuleList(\n",
        "            nn.Linear(in_channel, out_channel) for(in_channel, out_channel) in zip(layer_size[0:-1], layer_size[1:])\n",
        "        )\n",
        "\n",
        "        # Define Dropout\n",
        "        self.dropout = nn.ModuleList(\n",
        "            nn.Dropout(p) for i in range(len(layer_size) - 2)\n",
        "        )\n",
        "\n",
        "        # Define Batch Normalization\n",
        "        self.bn = nn.ModuleList(\n",
        "            nn.BatchNorm2d(i) for i in channel_size[:-1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.bn[0](x)\n",
        "        tmp = self.conv_block[0](residual)\n",
        "        x = F.max_pool2d(tmp, kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        residual = self.bn[1](x)\n",
        "        tmp = self.conv_block[1](residual)\n",
        "        x = F.max_pool2d(tmp + residual, kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        for (dropout, fc) in zip(self.dropout, self.fc[:-1]):\n",
        "            x = dropout(torch.relu(fc(x)))\n",
        "\n",
        "        x = torch.relu(self.fc[1](x))\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG1i7T3RBjcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e324bfa-87c5-47ee-9c8a-c1aca6758a61"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "model3 = cnn_model3(num_of_class)\n",
        "print(model3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cnn_model3(\n",
            "  (conv_block): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (fc): ModuleList(\n",
            "    (0): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            "  (dropout): ModuleList(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "  )\n",
            "  (bn): ModuleList(\n",
            "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoKGZvWYBmxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c5f80d-ae3d-4c80-b6fb-c50642157084"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "summary(model3, (1, 32, 32), device = \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "       BatchNorm2d-1            [-1, 1, 32, 32]               2\n",
            "            Conv2d-2           [-1, 32, 32, 32]             320\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 32, 16, 16]              64\n",
            "            Conv2d-5           [-1, 32, 16, 16]           9,248\n",
            "              ReLU-6           [-1, 32, 16, 16]               0\n",
            "            Linear-7                  [-1, 128]         262,272\n",
            "           Dropout-8                  [-1, 128]               0\n",
            "            Linear-9                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 273,196\n",
            "Trainable params: 273,196\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.70\n",
            "Params size (MB): 1.04\n",
            "Estimated Total Size (MB): 1.74\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKDMK93FBodx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9da1ad-4b52-4c91-d076-39e8c11f5efc"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "#train this take around an half hour\n",
        "model3 = cnn_model3(num_of_class)\n",
        "model3 = model3.to(device)\n",
        "optimizer3 = optim.SGD(model3.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "file_name = \"Model_3\"\n",
        "train(model3, trainloader, testloader, optimizer3, file_dir, file_name, device, num_epoch = 25, loss_iter = 10, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch  1/25 Iter   300/3000]: train_loss = 0.8996\n",
            "[Epoch  1/25 Iter   600/3000]: train_loss = 0.5048\n",
            "[Epoch  1/25 Iter   900/3000]: train_loss = 0.4383\n",
            "[Epoch  1/25 Iter  1200/3000]: train_loss = 0.4238\n",
            "[Epoch  1/25 Iter  1500/3000]: train_loss = 0.3938\n",
            "[Epoch  1/25 Iter  1800/3000]: train_loss = 0.4004\n",
            "[Epoch  1/25 Iter  2100/3000]: train_loss = 0.3695\n",
            "[Epoch  1/25 Iter  2400/3000]: train_loss = 0.3715\n",
            "[Epoch  1/25 Iter  2700/3000]: train_loss = 0.3527\n",
            "[Epoch  1/25 Iter  3000/3000]: train_loss = 0.3509\n",
            "[Epoch  2/25 Iter   300/3000]: train_loss = 0.3246\n",
            "[Epoch  2/25 Iter   600/3000]: train_loss = 0.3240\n",
            "[Epoch  2/25 Iter   900/3000]: train_loss = 0.3169\n",
            "[Epoch  2/25 Iter  1200/3000]: train_loss = 0.3043\n",
            "[Epoch  2/25 Iter  1500/3000]: train_loss = 0.3020\n",
            "[Epoch  2/25 Iter  1800/3000]: train_loss = 0.3017\n",
            "[Epoch  2/25 Iter  2100/3000]: train_loss = 0.3076\n",
            "[Epoch  2/25 Iter  2400/3000]: train_loss = 0.3110\n",
            "[Epoch  2/25 Iter  2700/3000]: train_loss = 0.2989\n",
            "[Epoch  2/25 Iter  3000/3000]: train_loss = 0.3056\n",
            "[Epoch  3/25 Iter   300/3000]: train_loss = 0.2843\n",
            "[Epoch  3/25 Iter   600/3000]: train_loss = 0.2846\n",
            "[Epoch  3/25 Iter   900/3000]: train_loss = 0.2806\n",
            "[Epoch  3/25 Iter  1200/3000]: train_loss = 0.2736\n",
            "[Epoch  3/25 Iter  1500/3000]: train_loss = 0.2780\n",
            "[Epoch  3/25 Iter  1800/3000]: train_loss = 0.2655\n",
            "[Epoch  3/25 Iter  2100/3000]: train_loss = 0.2653\n",
            "[Epoch  3/25 Iter  2400/3000]: train_loss = 0.2756\n",
            "[Epoch  3/25 Iter  2700/3000]: train_loss = 0.2749\n",
            "[Epoch  3/25 Iter  3000/3000]: train_loss = 0.2642\n",
            "[Epoch  4/25 Iter   300/3000]: train_loss = 0.2503\n",
            "[Epoch  4/25 Iter   600/3000]: train_loss = 0.2654\n",
            "[Epoch  4/25 Iter   900/3000]: train_loss = 0.2537\n",
            "[Epoch  4/25 Iter  1200/3000]: train_loss = 0.2541\n",
            "[Epoch  4/25 Iter  1500/3000]: train_loss = 0.2566\n",
            "[Epoch  4/25 Iter  1800/3000]: train_loss = 0.2452\n",
            "[Epoch  4/25 Iter  2100/3000]: train_loss = 0.2382\n",
            "[Epoch  4/25 Iter  2400/3000]: train_loss = 0.2624\n",
            "[Epoch  4/25 Iter  2700/3000]: train_loss = 0.2407\n",
            "[Epoch  4/25 Iter  3000/3000]: train_loss = 0.2553\n",
            "[Epoch  5/25 Iter   300/3000]: train_loss = 0.2314\n",
            "[Epoch  5/25 Iter   600/3000]: train_loss = 0.2383\n",
            "[Epoch  5/25 Iter   900/3000]: train_loss = 0.2390\n",
            "[Epoch  5/25 Iter  1200/3000]: train_loss = 0.2431\n",
            "[Epoch  5/25 Iter  1500/3000]: train_loss = 0.2350\n",
            "[Epoch  5/25 Iter  1800/3000]: train_loss = 0.2211\n",
            "[Epoch  5/25 Iter  2100/3000]: train_loss = 0.2323\n",
            "[Epoch  5/25 Iter  2400/3000]: train_loss = 0.2520\n",
            "[Epoch  5/25 Iter  2700/3000]: train_loss = 0.2343\n",
            "[Epoch  5/25 Iter  3000/3000]: train_loss = 0.2296\n",
            "[Epoch  6/25 Iter   300/3000]: train_loss = 0.2089\n",
            "[Epoch  6/25 Iter   600/3000]: train_loss = 0.2013\n",
            "[Epoch  6/25 Iter   900/3000]: train_loss = 0.2256\n",
            "[Epoch  6/25 Iter  1200/3000]: train_loss = 0.2036\n",
            "[Epoch  6/25 Iter  1500/3000]: train_loss = 0.2216\n",
            "[Epoch  6/25 Iter  1800/3000]: train_loss = 0.2331\n",
            "[Epoch  6/25 Iter  2100/3000]: train_loss = 0.2284\n",
            "[Epoch  6/25 Iter  2400/3000]: train_loss = 0.2264\n",
            "[Epoch  6/25 Iter  2700/3000]: train_loss = 0.2347\n",
            "[Epoch  6/25 Iter  3000/3000]: train_loss = 0.2326\n",
            "[Epoch  7/25 Iter   300/3000]: train_loss = 0.2071\n",
            "[Epoch  7/25 Iter   600/3000]: train_loss = 0.2031\n",
            "[Epoch  7/25 Iter   900/3000]: train_loss = 0.2072\n",
            "[Epoch  7/25 Iter  1200/3000]: train_loss = 0.2206\n",
            "[Epoch  7/25 Iter  1500/3000]: train_loss = 0.1878\n",
            "[Epoch  7/25 Iter  1800/3000]: train_loss = 0.2132\n",
            "[Epoch  7/25 Iter  2100/3000]: train_loss = 0.1988\n",
            "[Epoch  7/25 Iter  2400/3000]: train_loss = 0.2220\n",
            "[Epoch  7/25 Iter  2700/3000]: train_loss = 0.2158\n",
            "[Epoch  7/25 Iter  3000/3000]: train_loss = 0.2103\n",
            "[Epoch  8/25 Iter   300/3000]: train_loss = 0.1972\n",
            "[Epoch  8/25 Iter   600/3000]: train_loss = 0.1982\n",
            "[Epoch  8/25 Iter   900/3000]: train_loss = 0.1874\n",
            "[Epoch  8/25 Iter  1200/3000]: train_loss = 0.1971\n",
            "[Epoch  8/25 Iter  1500/3000]: train_loss = 0.1948\n",
            "[Epoch  8/25 Iter  1800/3000]: train_loss = 0.2026\n",
            "[Epoch  8/25 Iter  2100/3000]: train_loss = 0.1944\n",
            "[Epoch  8/25 Iter  2400/3000]: train_loss = 0.2061\n",
            "[Epoch  8/25 Iter  2700/3000]: train_loss = 0.2023\n",
            "[Epoch  8/25 Iter  3000/3000]: train_loss = 0.2025\n",
            "[Epoch  9/25 Iter   300/3000]: train_loss = 0.1925\n",
            "[Epoch  9/25 Iter   600/3000]: train_loss = 0.1840\n",
            "[Epoch  9/25 Iter   900/3000]: train_loss = 0.1921\n",
            "[Epoch  9/25 Iter  1200/3000]: train_loss = 0.2006\n",
            "[Epoch  9/25 Iter  1500/3000]: train_loss = 0.1870\n",
            "[Epoch  9/25 Iter  1800/3000]: train_loss = 0.1823\n",
            "[Epoch  9/25 Iter  2100/3000]: train_loss = 0.1871\n",
            "[Epoch  9/25 Iter  2400/3000]: train_loss = 0.1902\n",
            "[Epoch  9/25 Iter  2700/3000]: train_loss = 0.1844\n",
            "[Epoch  9/25 Iter  3000/3000]: train_loss = 0.1894\n",
            "[Epoch 10/25 Iter   300/3000]: train_loss = 0.1816\n",
            "[Epoch 10/25 Iter   600/3000]: train_loss = 0.1723\n",
            "[Epoch 10/25 Iter   900/3000]: train_loss = 0.1846\n",
            "[Epoch 10/25 Iter  1200/3000]: train_loss = 0.1819\n",
            "[Epoch 10/25 Iter  1500/3000]: train_loss = 0.1759\n",
            "[Epoch 10/25 Iter  1800/3000]: train_loss = 0.1681\n",
            "[Epoch 10/25 Iter  2100/3000]: train_loss = 0.1836\n",
            "[Epoch 10/25 Iter  2400/3000]: train_loss = 0.1916\n",
            "[Epoch 10/25 Iter  2700/3000]: train_loss = 0.1847\n",
            "[Epoch 10/25 Iter  3000/3000]: train_loss = 0.1826\n",
            "[Epoch 11/25 Iter   300/3000]: train_loss = 0.1705\n",
            "[Epoch 11/25 Iter   600/3000]: train_loss = 0.1705\n",
            "[Epoch 11/25 Iter   900/3000]: train_loss = 0.1755\n",
            "[Epoch 11/25 Iter  1200/3000]: train_loss = 0.1701\n",
            "[Epoch 11/25 Iter  1500/3000]: train_loss = 0.1712\n",
            "[Epoch 11/25 Iter  1800/3000]: train_loss = 0.1753\n",
            "[Epoch 11/25 Iter  2100/3000]: train_loss = 0.1769\n",
            "[Epoch 11/25 Iter  2400/3000]: train_loss = 0.1634\n",
            "[Epoch 11/25 Iter  2700/3000]: train_loss = 0.1666\n",
            "[Epoch 11/25 Iter  3000/3000]: train_loss = 0.1843\n",
            "[Epoch 12/25 Iter   300/3000]: train_loss = 0.1641\n",
            "[Epoch 12/25 Iter   600/3000]: train_loss = 0.1601\n",
            "[Epoch 12/25 Iter   900/3000]: train_loss = 0.1539\n",
            "[Epoch 12/25 Iter  1200/3000]: train_loss = 0.1626\n",
            "[Epoch 12/25 Iter  1500/3000]: train_loss = 0.1587\n",
            "[Epoch 12/25 Iter  1800/3000]: train_loss = 0.1737\n",
            "[Epoch 12/25 Iter  2100/3000]: train_loss = 0.1690\n",
            "[Epoch 12/25 Iter  2400/3000]: train_loss = 0.1671\n",
            "[Epoch 12/25 Iter  2700/3000]: train_loss = 0.1774\n",
            "[Epoch 12/25 Iter  3000/3000]: train_loss = 0.1752\n",
            "[Epoch 13/25 Iter   300/3000]: train_loss = 0.1533\n",
            "[Epoch 13/25 Iter   600/3000]: train_loss = 0.1544\n",
            "[Epoch 13/25 Iter   900/3000]: train_loss = 0.1507\n",
            "[Epoch 13/25 Iter  1200/3000]: train_loss = 0.1570\n",
            "[Epoch 13/25 Iter  1500/3000]: train_loss = 0.1406\n",
            "[Epoch 13/25 Iter  1800/3000]: train_loss = 0.1667\n",
            "[Epoch 13/25 Iter  2100/3000]: train_loss = 0.1657\n",
            "[Epoch 13/25 Iter  2400/3000]: train_loss = 0.1604\n",
            "[Epoch 13/25 Iter  2700/3000]: train_loss = 0.1530\n",
            "[Epoch 13/25 Iter  3000/3000]: train_loss = 0.1670\n",
            "[Epoch 14/25 Iter   300/3000]: train_loss = 0.1368\n",
            "[Epoch 14/25 Iter   600/3000]: train_loss = 0.1559\n",
            "[Epoch 14/25 Iter   900/3000]: train_loss = 0.1549\n",
            "[Epoch 14/25 Iter  1200/3000]: train_loss = 0.1453\n",
            "[Epoch 14/25 Iter  1500/3000]: train_loss = 0.1467\n",
            "[Epoch 14/25 Iter  1800/3000]: train_loss = 0.1416\n",
            "[Epoch 14/25 Iter  2100/3000]: train_loss = 0.1628\n",
            "[Epoch 14/25 Iter  2400/3000]: train_loss = 0.1607\n",
            "[Epoch 14/25 Iter  2700/3000]: train_loss = 0.1565\n",
            "[Epoch 14/25 Iter  3000/3000]: train_loss = 0.1436\n",
            "[Epoch 15/25 Iter   300/3000]: train_loss = 0.1308\n",
            "[Epoch 15/25 Iter   600/3000]: train_loss = 0.1421\n",
            "[Epoch 15/25 Iter   900/3000]: train_loss = 0.1351\n",
            "[Epoch 15/25 Iter  1200/3000]: train_loss = 0.1508\n",
            "[Epoch 15/25 Iter  1500/3000]: train_loss = 0.1364\n",
            "[Epoch 15/25 Iter  1800/3000]: train_loss = 0.1432\n",
            "[Epoch 15/25 Iter  2100/3000]: train_loss = 0.1475\n",
            "[Epoch 15/25 Iter  2400/3000]: train_loss = 0.1508\n",
            "[Epoch 15/25 Iter  2700/3000]: train_loss = 0.1556\n",
            "[Epoch 15/25 Iter  3000/3000]: train_loss = 0.1513\n",
            "[Epoch 16/25 Iter   300/3000]: train_loss = 0.1323\n",
            "[Epoch 16/25 Iter   600/3000]: train_loss = 0.1275\n",
            "[Epoch 16/25 Iter   900/3000]: train_loss = 0.1296\n",
            "[Epoch 16/25 Iter  1200/3000]: train_loss = 0.1317\n",
            "[Epoch 16/25 Iter  1500/3000]: train_loss = 0.1445\n",
            "[Epoch 16/25 Iter  1800/3000]: train_loss = 0.1361\n",
            "[Epoch 16/25 Iter  2100/3000]: train_loss = 0.1355\n",
            "[Epoch 16/25 Iter  2400/3000]: train_loss = 0.1426\n",
            "[Epoch 16/25 Iter  2700/3000]: train_loss = 0.1395\n",
            "[Epoch 16/25 Iter  3000/3000]: train_loss = 0.1490\n",
            "[Epoch 17/25 Iter   300/3000]: train_loss = 0.1273\n",
            "[Epoch 17/25 Iter   600/3000]: train_loss = 0.1288\n",
            "[Epoch 17/25 Iter   900/3000]: train_loss = 0.1349\n",
            "[Epoch 17/25 Iter  1200/3000]: train_loss = 0.1263\n",
            "[Epoch 17/25 Iter  1500/3000]: train_loss = 0.1298\n",
            "[Epoch 17/25 Iter  1800/3000]: train_loss = 0.1280\n",
            "[Epoch 17/25 Iter  2100/3000]: train_loss = 0.1273\n",
            "[Epoch 17/25 Iter  2400/3000]: train_loss = 0.1238\n",
            "[Epoch 17/25 Iter  2700/3000]: train_loss = 0.1372\n",
            "[Epoch 17/25 Iter  3000/3000]: train_loss = 0.1388\n",
            "[Epoch 18/25 Iter   300/3000]: train_loss = 0.1233\n",
            "[Epoch 18/25 Iter   600/3000]: train_loss = 0.1227\n",
            "[Epoch 18/25 Iter   900/3000]: train_loss = 0.1253\n",
            "[Epoch 18/25 Iter  1200/3000]: train_loss = 0.1246\n",
            "[Epoch 18/25 Iter  1500/3000]: train_loss = 0.1200\n",
            "[Epoch 18/25 Iter  1800/3000]: train_loss = 0.1233\n",
            "[Epoch 18/25 Iter  2100/3000]: train_loss = 0.1151\n",
            "[Epoch 18/25 Iter  2400/3000]: train_loss = 0.1331\n",
            "[Epoch 18/25 Iter  2700/3000]: train_loss = 0.1250\n",
            "[Epoch 18/25 Iter  3000/3000]: train_loss = 0.1356\n",
            "[Epoch 19/25 Iter   300/3000]: train_loss = 0.1157\n",
            "[Epoch 19/25 Iter   600/3000]: train_loss = 0.1129\n",
            "[Epoch 19/25 Iter   900/3000]: train_loss = 0.1160\n",
            "[Epoch 19/25 Iter  1200/3000]: train_loss = 0.1176\n",
            "[Epoch 19/25 Iter  1500/3000]: train_loss = 0.1145\n",
            "[Epoch 19/25 Iter  1800/3000]: train_loss = 0.1190\n",
            "[Epoch 19/25 Iter  2100/3000]: train_loss = 0.1237\n",
            "[Epoch 19/25 Iter  2400/3000]: train_loss = 0.1320\n",
            "[Epoch 19/25 Iter  2700/3000]: train_loss = 0.1219\n",
            "[Epoch 19/25 Iter  3000/3000]: train_loss = 0.1104\n",
            "[Epoch 20/25 Iter   300/3000]: train_loss = 0.1054\n",
            "[Epoch 20/25 Iter   600/3000]: train_loss = 0.1198\n",
            "[Epoch 20/25 Iter   900/3000]: train_loss = 0.1115\n",
            "[Epoch 20/25 Iter  1200/3000]: train_loss = 0.1074\n",
            "[Epoch 20/25 Iter  1500/3000]: train_loss = 0.1129\n",
            "[Epoch 20/25 Iter  1800/3000]: train_loss = 0.1105\n",
            "[Epoch 20/25 Iter  2100/3000]: train_loss = 0.1120\n",
            "[Epoch 20/25 Iter  2400/3000]: train_loss = 0.1188\n",
            "[Epoch 20/25 Iter  2700/3000]: train_loss = 0.1269\n",
            "[Epoch 20/25 Iter  3000/3000]: train_loss = 0.1217\n",
            "[Epoch 21/25 Iter   300/3000]: train_loss = 0.1113\n",
            "[Epoch 21/25 Iter   600/3000]: train_loss = 0.1087\n",
            "[Epoch 21/25 Iter   900/3000]: train_loss = 0.1079\n",
            "[Epoch 21/25 Iter  1200/3000]: train_loss = 0.1117\n",
            "[Epoch 21/25 Iter  1500/3000]: train_loss = 0.1117\n",
            "[Epoch 21/25 Iter  1800/3000]: train_loss = 0.1090\n",
            "[Epoch 21/25 Iter  2100/3000]: train_loss = 0.1133\n",
            "[Epoch 21/25 Iter  2400/3000]: train_loss = 0.1180\n",
            "[Epoch 21/25 Iter  2700/3000]: train_loss = 0.1106\n",
            "[Epoch 21/25 Iter  3000/3000]: train_loss = 0.1135\n",
            "[Epoch 22/25 Iter   300/3000]: train_loss = 0.0953\n",
            "[Epoch 22/25 Iter   600/3000]: train_loss = 0.0955\n",
            "[Epoch 22/25 Iter   900/3000]: train_loss = 0.0917\n",
            "[Epoch 22/25 Iter  1200/3000]: train_loss = 0.1122\n",
            "[Epoch 22/25 Iter  1500/3000]: train_loss = 0.1032\n",
            "[Epoch 22/25 Iter  1800/3000]: train_loss = 0.1061\n",
            "[Epoch 22/25 Iter  2100/3000]: train_loss = 0.1205\n",
            "[Epoch 22/25 Iter  2400/3000]: train_loss = 0.1089\n",
            "[Epoch 22/25 Iter  2700/3000]: train_loss = 0.1127\n",
            "[Epoch 22/25 Iter  3000/3000]: train_loss = 0.1155\n",
            "[Epoch 23/25 Iter   300/3000]: train_loss = 0.0964\n",
            "[Epoch 23/25 Iter   600/3000]: train_loss = 0.0958\n",
            "[Epoch 23/25 Iter   900/3000]: train_loss = 0.1006\n",
            "[Epoch 23/25 Iter  1200/3000]: train_loss = 0.0997\n",
            "[Epoch 23/25 Iter  1500/3000]: train_loss = 0.1024\n",
            "[Epoch 23/25 Iter  1800/3000]: train_loss = 0.1011\n",
            "[Epoch 23/25 Iter  2100/3000]: train_loss = 0.1017\n",
            "[Epoch 23/25 Iter  2400/3000]: train_loss = 0.1103\n",
            "[Epoch 23/25 Iter  2700/3000]: train_loss = 0.0998\n",
            "[Epoch 23/25 Iter  3000/3000]: train_loss = 0.1064\n",
            "[Epoch 24/25 Iter   300/3000]: train_loss = 0.0916\n",
            "[Epoch 24/25 Iter   600/3000]: train_loss = 0.1010\n",
            "[Epoch 24/25 Iter   900/3000]: train_loss = 0.0976\n",
            "[Epoch 24/25 Iter  1200/3000]: train_loss = 0.0960\n",
            "[Epoch 24/25 Iter  1500/3000]: train_loss = 0.1002\n",
            "[Epoch 24/25 Iter  1800/3000]: train_loss = 0.0872\n",
            "[Epoch 24/25 Iter  2100/3000]: train_loss = 0.1014\n",
            "[Epoch 24/25 Iter  2400/3000]: train_loss = 0.0878\n",
            "[Epoch 24/25 Iter  2700/3000]: train_loss = 0.0971\n",
            "[Epoch 24/25 Iter  3000/3000]: train_loss = 0.0971\n",
            "[Epoch 25/25 Iter   300/3000]: train_loss = 0.0977\n",
            "[Epoch 25/25 Iter   600/3000]: train_loss = 0.0926\n",
            "[Epoch 25/25 Iter   900/3000]: train_loss = 0.0908\n",
            "[Epoch 25/25 Iter  1200/3000]: train_loss = 0.0910\n",
            "[Epoch 25/25 Iter  1500/3000]: train_loss = 0.0878\n",
            "[Epoch 25/25 Iter  1800/3000]: train_loss = 0.0951\n",
            "[Epoch 25/25 Iter  2100/3000]: train_loss = 0.0943\n",
            "[Epoch 25/25 Iter  2400/3000]: train_loss = 0.0963\n",
            "[Epoch 25/25 Iter  2700/3000]: train_loss = 0.0967\n",
            "[Epoch 25/25 Iter  3000/3000]: train_loss = 0.0990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x_IW_FbBp6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7308bed4-ff01-42fb-d0c9-9d76f283dc60"
      },
      "source": [
        "###############################\n",
        "# code by Tan Xi En  #\n",
        "###############################\n",
        "\n",
        "acc_score = evaluate(model3, testloader, device)\n",
        "print(f'Accuracy = {acc_score:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 92.07%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}